\documentclass[aspectratio=169]{beamer}

% The 'handout' option collapses all overlays into single slides
%\documentclass[aspectratio=169, handout]{beamer}

\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{algorithm, algorithmic}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc,shapes}

% --- Standard Beamer Theme ---
\usetheme{Madrid}
\usecolortheme{default}

\include{macros} % Assuming you have this file



\title{RL with Linear Features: When Does It Work \\ \& When Doesn't It Work?}
\subtitle{Part 3: Lower Bounds\\
CS 2284: Foundations of Reinforcement Learning}
\author{Kiant\'{e} Brantley \& Sham Kakade}
\date{}

\begin{document}

% =============================================================================
% TITLE
% =============================================================================

\begin{frame}[plain]
    \titlepage
\end{frame}
\addtocounter{framenumber}{-1}

% =============================================================================
% Part I: The Setup (Recap)
% =============================================================================

\section{The Split Universe: Completeness vs. Realizability}

\begin{frame}{1. The Critical Decomposition}
    To analyze error propagation, we introduce the \textbf{Infinite-Sample Target}.
    
    \vspace{0.2cm}
    
    \textbf{Definition ($f^\star_h$):} The function LSVI would learn with infinite data.
    \[
    f^\star_h \;:=\; \argmin_{f \in \Fcal} \E_{\rho} \left[ (f(s,a) - \Tcal_h \widehat{Q}_{h+1}(s,a))^2 \right] \;=\; \Pi_{\Fcal, \rho} ( \Tcal_h \widehat{Q}_{h+1} )
    \]
    \emph{(Ideally, we want $f^\star_h$ to track the optimal value $Q^\star_h$)}.
    
    \vspace{0.3cm}
    
    \textbf{The Error Triangle:}
    We split the total error into Statistical Variance and Recursive Stability.
    \[
    \|\widehat{Q}_h - Q^\star_h\|_\infty \le \underbrace{\|\widehat{Q}_h - f^\star_h\|_\infty}_{\text{Statistical Error}} + \underbrace{\|f^\star_h - Q^\star_h\|_\infty}_{\text{Recursive Stability}}
    \]
   \begin{itemize}
        \item \textbf{Statistical Error:} Controlled by $N$ and D-Optimal Design ($\approx \sqrt{d/N}$).
        \item \textbf{Recursive Stability:} This is where the universe splits.
    \end{itemize}
\end{frame}



\begin{frame}{2. The Split Universe}
    How does the Recursive Error $\|f^\star_h - Q^\star_h\|_\infty$ behave?\\
    Recall $f^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h \widehat{Q}_{h+1})$
    and $Q^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h Q^\star_{h+1})$ (by realizability).
    
    \vspace{0.2cm}
    
    \begin{columns}[t]
        \column{0.48\textwidth}

        \begin{block}{Universe A: Completeness}
        \textbf{Assumption:} $\Tcal_h$ preserves linearity.
        
        \vspace{0.2cm}
         Since $\widehat{Q}_{h+1} \in \Fcal$, we have:
        $f^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h \widehat{Q}_{h+1}) =
        \Tcal_h \widehat{Q}_{h+1}$, and thus:

        \vspace*{-0.3cm}
        \begin{align*}
        \|f^\star_h - Q^\star_h\|_\infty &=\|\Tcal_h \widehat{Q}_{h+1}
        - \Tcal_h Q_{h+1}^\star \|_\infty\\
          &\leq \| \widehat{Q}_{h+1} - Q_{h+1}^\star \|_\infty
        \end{align*}
        \textbf{Result:} Error is stable (contraction).
        \end{block}

        \column{0.48\textwidth}

        \begin{block}{Universe B: Realizability Only}
        \textbf{Assumption:} Only $Q^\star \in \Fcal$.
        
        \vspace{0.2cm}
        The Bellman backup $\Tcal_h \widehat{Q}_{h+1}$ may be \textbf{non-linear} (off-manifold).
        
        \vspace{0.2cm}
        We must project it back to $\Fcal$:
        \[
        f^\star_h = \Pi_{\Fcal,\rho} (\text{Non-Linear Target})
        \]
        \textbf{Result:} We pay for the stability of the projection operator $\Pi_{\Fcal,\rho}$.
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}{3. The Amplification Mechanism (Universe B)}
    Without Completeness, we must bound the stability of the projection $\Pi_{\Fcal,\rho}$.
    Let $\Delta = \Tcal_h \widehat{Q}_{h+1} - \Tcal_h Q^\star_{h+1}$.
    
    \vspace{0.2cm}
    
    \textbf{The Chain of Inequalities:}
    \begin{align*}
        \|f^\star_h - Q^\star_h\|_\infty &= \|\Pi_{\Fcal,\rho} \Delta\|_\infty \\
        &\le \sqrt{d} \cdot \|\Pi_{\Fcal,\rho} \Delta\|_{L_2(\rho)} \quad \text{(\textbf{Step 1:} Norm Equivalence)} \\
        &\le \sqrt{d} \cdot \|\Delta\|_{L_2(\rho)} \quad \quad \quad \text{(\textbf{Step 2:} $L_2$ Stability of LS)} \\
        &\le \sqrt{d} \cdot \|\Delta\|_\infty \quad \quad \quad \quad \quad \text{(\textbf{Step 3:} Norm Monotonicity)} \\
        &\le \sqrt{d} \cdot \|\widehat{Q}_{h+1} - Q^\star_{h+1}\|_\infty
    \end{align*}

    \vspace{0.1cm}
    
    \textbf{The Verdict:}
    The "price" of converting the $L_2$ guarantee (regression) to the $L_\infty$ guarantee (DP) is exactly $\sqrt{d}$.
    
    \centering
    \alert{Total Amplification over $H$ steps $\approx (\sqrt{d})^H$.}
\end{frame}


\iffalse
\begin{frame}{3. The Amplification Mechanism (Universe B): projection is not stable}
\small

The OLS population projection is:
\[
\Pi_{\Fcal,\rho}(g)\;:=\;\argmin_{f\in\Fcal}\ \E_{(s,a)\sim\rho}\big[(f(s,a)-g(s,a))^2\big].
\]
Then (using realizability for $Q_h^\star$):
\[
f_h^\star \;=\; \Pi_{\Fcal,\rho}(\Tcal_h \widehat Q_{h+1}) \;=\; \theta_h^\top\phi,
\qquad
Q_h^\star \;=\; \Pi_{\Fcal,\rho}(\Tcal_h Q_{h+1}^\star) \;=\; (\theta_h^\star)^\top\phi.
\]

\vspace{0.05cm}
\textbf{Recursive error bound (Universe B):} Assuming we use the
$D$-opt design $\Sigma_\rho :=
\E_{\rho}[\phi\phi^\top]$ in our regression.
\begin{align*}
\|f_h^\star - Q_h^\star\|_\infty
&= \sup_{s,a}\Big|(\theta_h-\theta_h^\star)^\top \phi(s,a)\Big|\\
&\le \sup_{s,a}\ \|\theta_h-\theta_h^\star\|_{\Sigma_\rho}\,
\sqrt{\phi(s,a)^\top \Sigma_\rho^{-1}\phi(s,a)} \\
&\le \sqrt d\ \|\theta_h-\theta_h^\star\|_{\Sigma_\rho}
\qquad\text{(D-opt leverage: $\sup \phi^\top\Sigma_\rho^{-1}\phi \le d$)}.\\
&\le \sqrt d\ \|\theta_h-\theta_h^\star\|_{\Sigma_\rho}
\qquad\text{(D-opt leverage: $\sup \phi^\top\Sigma_\rho^{-1}\phi \le d$)}.\\
  \|\widehat{Q}_{h+1} - Q^\star_{h+1}\|_\infty
\end{align*}

\vspace{-0.05cm}
\centering
\alert{Total amplification over $H$ steps $\approx (\sqrt{d})^H$.}

\end{frame}
\fi

% =============================================================================
% Part II: The Offline Lower Bound
% =============================================================================

\begin{frame}[plain]
  \vfill
  \begin{center}
    {\Huge \bfseries Offline Lower Bounds with Realizatiblity+Coverage}
  \end{center}
  \vfill
\end{frame}
\addtocounter{framenumber}{-1}

\section{The Offline Lower Bound}

\begin{frame}{The Offline Setting: Strong Assumptions}
    Let's try to "break" the lower bound by making extremely strong assumptions.
    
    \textbf{The Setup:}
    \begin{itemize}
        \item \textbf{Offline Data:} Fixed datasets $D_0, \dots, D_{H-1}$.
        \item \textbf{Assumption 1 (All-Policies Realizability):} 
        For \emph{every} policy $\pi$, $Q^\pi_h$ is linear in $\phi$.
        \item \textbf{Assumption 2 (Perfect Coverage):} 
        The offline data covariance is "isotropic" (perfectly conditioned):
        \[
        \Sigma_{D_h} = \frac{1}{d} I \quad \text{for all } h.
        \]
    \end{itemize}

    \vspace{0.3cm}
    \textbf{The Question:} Under these ideal conditions (Realizability + Perfect Coverage), can we learn $V^\pi$ with $\poly(d, H)$ samples?
\end{frame}

\begin{frame}{Theorem: Offline Hardness}
  \begin{block}{Theorem 3.2 (Offline Policy Evaluation Hardness)}
    There exists a class of MDPs with dimension $d$ and horizon $H$ satisfying \textbf{All-Policies Realizability} and \textbf{Perfect Coverage}, such that any algorithm requires
    \[
    N \ge \Omega\left( (d/2)^H \right)
    \]
    samples to estimate $Q^\pi(s_0,a)$ to constant accuracy.
    \end{block}

    \vspace{0.3cm}
    \textbf{Interpretation:}
    Even with infinite data in the "covered" directions, variance accumulates exponentially in the "hidden" directions.
\end{frame}



% =============================================================================
% Part II: The Construction
% =============================================================================


\section{The Construction}

\begin{frame}{The High-Level Idea: Hiding a Secret}
    \textbf{The Goal:} Construct an MDP where the optimal value depends on a tiny parameter $r_\infty$, but we can only learn it by solving a "hard" regression problem.
    
    \vspace{0.3cm}
    \textbf{The Mechanism (Two Chains):}
    \begin{enumerate}
        \item \textbf{The Hidden Chain ($g$):} 
        Carries the true "signal" ($V \approx \text{Large}$).
        \item \textbf{The Observed Chain ($s$):} 
        Carries a "noisy version" of the signal ($V \approx \text{Small}$).
    \end{enumerate}
    
    \vspace{0.3cm}
    \textbf{The "Trap":}
    Linearly Realizable implies that $V(g)$ is coupled to $V(s)$ by a factor of $\sqrt{d}$.
    \begin{itemize}
        \item To know the large value at $g$, we must estimate the small value at $s$.
        \item Estimating the small value requires cancelling out huge noise.
        \item This requires exponentially many samples.
    \end{itemize}
\end{frame}


% --- Slide 1: Schematic and Definitions ---
\begin{frame}{Offline PE hard instance (schematic)}
\begin{columns}[T,onlytextwidth]
  % ---------------- Left: dynamics ----------------
  \begin{column}{0.7\textwidth}
    \centering
    % Using the macro defined above
    %\resizebox{0.95\textwidth}{!}{ \HardInstanceFig }
    \resizebox{0.95\textwidth}{!}{\input{tikzpic_lb_lecture} }
  \end{column}

  % ---------------- Right: Legend ----------------
  \begin{column}{0.3\textwidth}
  \footnotesize
  
  % Actions Legend
  \noindent
  \begin{tikzpicture}
    \node[draw,inner sep=3.5pt] (leg) {%
      \begin{tikzpicture}[>=Latex]
        \draw[thick] (0,0) -- (1.2,0) node[midway,above] {$a_1$};
        \draw[thick,dashed] (0,-0.5) -- (1.2,-0.5) node[midway,above] {$a_2$};
      \end{tikzpicture}
    };
  \end{tikzpicture}
  
  \vspace{0.5em}
  
  \noindent\textbf{Features.}\\%[-0.25em]
  $\phi(s_h^c,a_1)=e_c$\\
  $\phi(s_h^c,a_2)=e_{m+c}$\\
  $\phi(g_h,a)=\frac{1}{\sqrt m}(e_1+\cdots+e_m)$
  
  \vspace{0.5em}
  
  \noindent\textbf{Reward parameters.}\\%[-0.25em]
  $r_\infty\in\{0,\ m^{-H/2}\}$\\
  $\alpha_h := r_\infty\, m^{(H-h)/2}$
  
  \vspace{0.5em}
  
  \noindent\textbf{Aggregator rewards.}\\%[-0.25em]
  $r_h(g_h,\cdot)=\alpha_h-\alpha_{h+1}$.\\
  Last layer: $r_{H-1}(g_{H-1})=\alpha_{H-1}$.
  
  \end{column}
\end{columns}
\end{frame}



% --- Slide 2: Dynamics ---
\begin{frame}{State space and transitions (two chains)}
\small
Fix $m\ge 1$, set $d=2m$, $|\Scal_h|=m+1$, $\Acal=\{a_1,a_2\}$, horizon $H$.

\vspace{0.5em}
\textbf{State space (layered).}
\[
\Scal_h := \{s_h^1,\dots,s_h^m\}\cup\{g_h\},\qquad h=0,\dots,H-1,
\quad\text{and } s_0=g_0.
\]

\vspace{0.75em}
\textbf{Deterministic transitions for $h\le H-2$:}
\begin{align*}
&\text{From any state, action $a_1$ jumps to aggregators:}
&&P(g_{h+1}\mid s,a_1)=1 \quad \forall s\in\Scal_h.\\
&\text{Action $a_2$ keeps index on observed chain:}
&&P(s_{h+1}^i\mid s_h^i,a_2)=1 \quad \forall i\in[m].\\
&\text{Aggregator chain stays on aggregators:}
&&P(g_{h+1}\mid g_h,a_2)=1.
\end{align*}

\vspace{0.5em}
\textbf{Interpretation:} $a_2$ keeps you in the observed world; $a_1$ (or being at $g_h$) takes you off it.
\end{frame}

% --- Slide 3: Features & Coverage ---
\begin{frame}{Features and Verifying Perfect Coverage}
    \textbf{The Features:}
    We use the standard basis for the observed states, and a dense "mean" vector for the aggregators.
    \begin{itemize}
        \item $\phi(s^i, a_1) = e_i$ \quad and \quad $\phi(s^i, a_2) = e_{m+i}$
        \item $\phi(g, \cdot) = \frac{1}{\sqrt{m}} \sum_{i=1}^m e_i$
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Observed Data Distribution ($\mu_h$):}
    Uniform distribution over the $m$ observed states $\{s_h^i\}_{i=1}^m$ and actions.
    
    \vspace{0.3cm}
    \textbf{Coverage Check:}
    Calculate the covariance matrix of the data:
    \[
    \Sigma_{D} \;=\; \frac{1}{2m} \sum_{i=1}^m (e_i e_i^\top + e_{m+i} e_{m+i}^\top) \;=\; \frac{1}{2m} I \;=\; \frac{1}{d} I
    \]
    
    \alert{Perfect isotropic coverage!} The offline data explores every dimension of the feature space equally.
\end{frame}

% --- Slide 4: Realizability Step 1 ---
\begin{frame}{Verifying Realizability. Step 1: The Constraint}
    Linearity $Q(s,a) = \theta^\top \phi(s,a)$ imposes rigid
    structure. (Fix level $h$ below. Let $\pi$ be arbitrary.)
    
    \vspace{0.1cm}
    
    \textbf{1. The Observed States are Tabular:}
    Since $\phi(s^i, a_1) = e_i$, the weights $\theta$ just memorize the values:
    \[
    \theta_i = Q(s^i, a_1) \quad \text{and} \quad \theta_{m+i} = Q(s^i, a_2)
    \]
    We can represent \emph{any} function on the observed states perfectly.
    
    \vspace{0.1cm}
    
    \textbf{2. The Aggregator is Constrained:}
    However, the value at $g$ is fixed by the weights $\theta$:
    \[
    Q(g, \cdot) \;=\; \theta^\top \phi(g) \;=\; \theta^\top \left( \frac{1}{\sqrt{m}} \sum_{i=1}^m e_i \right)
    \]
    Substituting $\theta_i = Q(s^i, a_1)$, we get the \textbf{Realizability Constraint}:
    \[
    \boxed{ Q(g, \cdot) \;=\; \frac{1}{\sqrt{m}} \sum_{i=1}^m Q(s^i, a_1) }
    \]
    \small
    \emph{To be realizable, the value at $g$ must be the scaled sum of values at $s$.}
\end{frame}

% --- Slide 5: Realizability Step 2 ---
\begin{frame}{Verifying Realizability. Step 2: Checking the Values}
    Does our specific MDP satisfy this constraint?
    \[
    \text{Constraint: } Q_h(g_h, \cdot) \overset{?}{=} \frac{1}{\sqrt{m}} \sum_{i=1}^m Q_h(s_h^i, a_1)
    \]

    \textbf{First:}
    By construction, $\alpha_h = \sqrt{m} \alpha_{h+1}$.
    
    \textbf{LHS (Actual Value at $g$):}
    By construction, rewards telescope so $V_h(g_h) = \alpha_h=\sqrt{m}V_{h+1}(g_{h+1})$.
    
    \vspace{0.3cm}
    
    \textbf{RHS (Sum of Values at $s$):}
    Action $a_1$ at $s_h^i$ jumps to $g_{h+1}$.
    \[
    Q_h(s_h^i, a_1) = 0 + V_{h+1}(g_{h+1}) %= \alpha_{h+1}
    \]
    So the sum is:
    \[
    \frac{1}{\sqrt{m}} \sum_{i=1}^m V_{h+1}(g_{h+1}) \;=\; \sqrt{m} V_{h+1}(g_{h+1})
    \]

    \textcolor{green!60!black}{\textbf{Constraint Verified.}}
\end{frame}

% --- Slide 6: Impossibility ---
\begin{frame}{Step 3: The Impossibility Result}
    \textbf{Putting it together:}
    \begin{itemize}
        \item We verified Realizability + Opt. Coverage hold for all
          $h$.
        \item Problem choice: $r_\infty\in\{0,\ m^{-H/2}\}$
        \item Value at start: $V^\pi(g_0) = \alpha_0 = m^{H/2}
          r_\infty\in\{0,1\}$.
    \end{itemize}

    \vspace{0.3cm}
    \textbf{The Hypothesis Test:}
    To decide if $V^\pi(g_0)$ is 0 or 1, we must distinguish:
    \[
    r_\infty = 0 \quad \text{vs} \quad r_\infty = m^{-H/2}
    \]
    
    \textbf{The Bottleneck:}
    We never see $g_h$. We only see the noisy leaves at $H-1$.
    \begin{itemize}
        \item Signal Gap: $\Delta = m^{-H/2}$.
        \item Samples Needed: $N \approx 1/\Delta^2 = m^H = (d/2)^H$.
    \end{itemize}
    
    \centering
    \alert{We need exponential samples to track the value of the unseen state.}
\end{frame}

% --- Slide 7: LSPE Discussion ---
\begin{frame}{Discussion 1: Why LSPE Fails (Variance Explosion)}
    \textbf{The Paradox:}
    LSPE is an unbiased estimator. Why does it fail?
    
    \vspace{0.2cm}
    
    \textbf{1. Tabular Learning on Observed States (Base Case)}
    On the observed states, the features are standard basis vectors $e_i$. Thus, LSPE estimates the parameters $\widehat{Q}_{H-1}(s^i, a_1)$ purely locally:
    \[
    \widehat{Q}_{H-1}(s^i, a_1) \;\approx\; Q^\star_{H-1}(s^i, a_1) \pm \frac{\sigma}{\sqrt{N}}
    \]
    
    \vspace{0.1cm}
    
    \textbf{2. The Recursive Update.}
    At subsequent layers, the value of jumping $Q_h(s, a_1)$ is determined by the value at the (unseen) next state $g_{h+1}$. Working out the LSPE update shows:
    \[
    \widehat{Q}_h(s, a_1) \;=\; \frac{1}{\sqrt{m}} \sum_{i=1}^m \widehat{Q}_{h+1}(s^i, a_1) 
    \;=\; \sqrt{m} \underbrace{\left(\frac{1}{m} \sum_{i=1}^m \widehat{Q}_{h+1}(s^i, a_1)\right)}_{\text{Average Estimate}}
    \]
    
    \textbf{Variance Explosion:}
    The estimate at $h$ is inflated by $\sqrt{m}$ times the average estimate at $h+1$.
    
    \centering
    \small
    \emph{We are extrapolating $\sqrt{m}$ further out than our data support, $H$ times in a row.}
\end{frame}

\begin{frame}{Discussion 2: Can Online RL Save Us?}
  \begin{itemize}
  \item The features are not \textbf{complete} (even though every
    policy is linear).

    \vspace{0.2cm}
    
  \item \textbf{A Trivial Observation:} Our offline hard instance has very few states-action pairs. \\
    $|\Scal|\cdot|\Acal|=H \cdot (d+2)$

    \vspace{0.2cm}
    
  \item In an \textbf{Online} (episodic) or \textbf{Generative
      model} setting, we could simply visit every state and
    solve it tabularly! 
        
        \vspace{0.2cm}
        
   
      \item \textbf{The Question:} Is there a hard instance --- where
        the state space is huge --- where even active exploration
        fails because we can't ``find'' the hidden $\theta^\star$ direction?
      \end{itemize}

      \vspace{0.5cm}
      \centering      
\emph{Can we construct a "Needle in a Haystack" where we can't find the features?}
\end{frame}
  

% =============================================================================
% Part III: The Online Lower Bound (Linear Q*)
% =============================================================================

\section{The Online Lower Bound}

\begin{frame}[plain]
  \vfill
  \begin{center}
    {\Huge \bfseries Online Lower Bounds\\ with Realizability}
  \end{center}
  \vfill
\end{frame}
\addtocounter{framenumber}{-1}


\begin{frame}{What if we can explore? (Generative Model)}
    \textbf{The Question:}
    We saw Offline RL fails even with coverage and all policies linear. 
    What if we have a \textbf{Generative Model} (can sample any $(s,a)$)?
    
    \vspace{0.2cm}
    
    \textbf{The Assumption (Weakening):}
    Suppose \textbf{only} that $Q^\star$ is linear.
    \[
    Q^\star_h(s,a) = \langle \theta^\star_h, \phi(s,a) \rangle
    \]
    (So no linear BC, i.e. we do \emph{not} assume $\Tcal Q \in \Fcal$
    for all $Q$).

    \vspace{0.3cm}
    \begin{block}{Theorem (Hardness of Linear $Q^\star$)}
    Even with a Generative Model, any algorithm requires
    \[
    N \ge \min \{ 2^{\Omega(d)}, 2^{\Omega(H)} \}
    \]
    samples to find an $\epsilon$-optimal policy (hard instance:
    $|\Acal| \approx \text{poly}(d)$, $|\Scal| \approx \text{exp}(d)$).
    \end{block}
\end{frame}

\begin{frame}{Proof Comments}
    
    \textbf{1. The Construction:}
    
    \begin{itemize}
        \item Construct a tree where only one specific path (the "needle") has high value.
        \item Again there is this ``amplfication'' effect.
        \item But the feature vector construction is very subtle.
    \end{itemize}

    \textbf{2. The Failure of Gradient Signal:}
    \begin{itemize}
        \item Without \textbf{Completeness}, the Bellman updates on the "distractor" paths do not propagate useful gradient information about $\theta^\star$.
        \item The value function looks "flat" everywhere except exactly on the optimal path.
    \end{itemize}
    
\end{frame}


% =============================================================================
% Part IV: Summary & Implications
% =============================================================================

\section{Summary}

\begin{frame}{Summary: The Linearity Ladder}
    We can classify RL problems based on their structural assumptions.

    \vspace{0.3cm}

    \begin{enumerate}
        \item \textbf{Linear Bellman Completeness:}
        \begin{itemize}
            \item \textbf{Result:} $\text{poly}(d, H)$.
            \item \textbf{Status:} \textcolor{green!60!black}{Efficient} (LSVI).
        \end{itemize}
        \vspace{0.2cm}

        \item \textbf{Linear $Q^\star$ Realizability:}
        \begin{itemize}
            \item \textbf{Result:} Hard with a Generative Model.
            \item \textbf{Status:} \textcolor{red}{Exponential Lower Bounds}.
        \end{itemize}
        \vspace{0.2cm}

        \item \textbf{All-Policy Realizability:}
        \begin{itemize}
            \item \textbf{Offline (+ Coverage):} \textcolor{red}{Hard} (Variance Explosion).
            \item \textbf{Generative Model:} \textcolor{green!60!black}{Easy} (Matrix Estimation).
            \item \textbf{Online RL:} \textbf{Unknown/Open Question}.
        \end{itemize}
    \end{enumerate}
        
    \vspace{0.4cm}
    \textbf{Implication:}
    To get polynomial sample complexity in RL, we need strong \textbf{Structural Assumptions} (like Completeness, Block MDPs, or Low Rank) that ensure values can be propagated globally.
\end{frame}

\end{document}

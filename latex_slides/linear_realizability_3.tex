\documentclass[aspectratio=169]{beamer}

% The 'handout' option collapses all overlays into single slides
%\documentclass[aspectratio=169, handout]{beamer}

\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{algorithm, algorithmic}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc,shapes}

% --- Standard Beamer Theme ---
\usetheme{Madrid}
\usecolortheme{default}

\include{macros} % Assuming you have this file


% =============================================================================
% PREAMBLE / MACROS (Put this before \begin{document} if not already there)
% =============================================================================
\usepackage{tikz}
\usetikzlibrary{arrows.meta,fit,positioning,calc}

% Define the Figure as a macro to reuse it cleanly
\newcommand{\HardInstanceFig}{
\begin{tikzpicture}[
  >=Latex,
  font=\small,
  state/.style={circle,draw,minimum size=9mm,inner sep=1pt},
  % Make boxes visually distinct from a2 dashed edges:
  obsbox/.style={draw,densely dotted,rounded corners,inner sep=3.0mm},
  % If you want subtle arrowheads, use these instead:
  % aone/.style={thick,-{Latex[length=1.9mm]}},
  % atwo/.style={thick,dashed,-{Latex[length=1.9mm]}},
  aone/.style={thick},
  atwo/.style={thick,dashed}
]

% -------------------------
% Geometry (as before)
% -------------------------
\def\xOne{0}
\def\xTwo{2.1}
\def\xThree{4.2}
\def\xDots{6.2}
\def\xLast{8.4}
\def\xAgg{11.2}

\def\yRowZero{0}
\def\yRowOne{-2.0}
\def\yRowMid{-4.2}
\def\yRowHm{-6.4}
\def\yRowH{-8.4}

% How much extra room to the LEFT for the D_h boxes
\def\boxLeftExtra{2.1cm}

% Shift the whole picture right to create left margin
\begin{scope}[xshift=1.6cm]

% -------------------------
% Top text: features + rewards legend + action legend
% -------------------------
% Use text widths to make the top feel "more justified"/blocky.
\node[anchor=north west,align=left,text width=6.2cm] at (1,2.9) {%
\textbf{features:}\\
$\phi(s_h^c,a_1)=e_c$\\
$\phi(s_h^c,a_2)=e_{m+c}$\\
$\phi(g_h,a)=\frac{1}{\sqrt m}(e_1+e_2+\cdots+e_m)$};

\node[anchor=north west,align=left,text width=5.4cm] at (-3,2.9) {%
\textbf{reward params:}\\
$r_\infty\in\{0,\ m^{-H/2}\}$\\
$\alpha_h := r_\infty\, m^{(H-h)/2}$};

% Legend box (higher)
\node[draw,minimum width=2.7cm,minimum height=1.4cm,anchor=north east] (leg)
  at (\xAgg+3.2,2.8) {};
\draw[aone] ([xshift=-2.3cm,yshift=-0.45cm]leg.north east) -- ++(1.2,0)
  node[midway,above] {$a_1$};
\draw[atwo] ([xshift=-2.3cm,yshift=-1.05cm]leg.north east) -- ++(1.2,0)
  node[midway,above] {$a_2$};

  
% -------------------------
% Nodes
% -------------------------
\node[state] (s0-1) at (\xOne,\yRowZero) {$s_0^{1}$};
\node[state] (s0-2) at (\xTwo,\yRowZero) {$s_0^{2}$};
\node[state] (s0-3) at (\xThree,\yRowZero) {$s_0^{3}$};
\node        (s0-d) at (\xDots,\yRowZero) {$\cdots$};
\node[state] (s0-m) at (\xLast,\yRowZero) {$s_0^{m}$};
\node[state] (g0)   at (\xAgg,\yRowZero) {$g_0$};

\node[state] (s1-1) at (\xOne,\yRowOne) {$s_1^{1}$};
\node[state] (s1-2) at (\xTwo,\yRowOne) {$s_1^{2}$};
\node[state] (s1-3) at (\xThree,\yRowOne) {$s_1^{3}$};
\node        (s1-d) at (\xDots,\yRowOne) {$\cdots$};
\node[state] (s1-m) at (\xLast,\yRowOne) {$s_1^{m}$};
\node[state] (g1)   at (\xAgg,\yRowOne) {$g_1$};

\node[state] (sh-1) at (\xOne,\yRowMid) {$s_h^{1}$};
\node[state] (sh-2) at (\xTwo,\yRowMid) {$s_h^{2}$};
\node[state] (sh-3) at (\xThree,\yRowMid) {$s_h^{3}$};
\node        (sh-d) at (\xDots,\yRowMid) {$\cdots$};
\node[state] (sh-m) at (\xLast,\yRowMid) {$s_h^{m}$};
\node[state] (gh)   at (\xAgg,\yRowMid) {$g_h$};

\node[state] (sHm-1) at (\xOne,\yRowHm) {$s_{H-2}^{1}$};
\node[state] (sHm-2) at (\xTwo,\yRowHm) {$s_{H-2}^{2}$};
\node[state] (sHm-3) at (\xThree,\yRowHm) {$s_{H-2}^{3}$};
\node        (sHm-d) at (\xDots,\yRowHm) {$\cdots$};
\node[state] (sHm-m) at (\xLast,\yRowHm) {$s_{H-2}^{m}$};
\node[state] (gHm)   at (\xAgg,\yRowHm) {$g_{H-2}$};

\node[state] (sH-1) at (\xOne,\yRowH) {$s_{H-1}^{1}$};
\node[state] (sH-2) at (\xTwo,\yRowH) {$s_{H-1}^{2}$};
\node[state] (sH-3) at (\xThree,\yRowH) {$s_{H-1}^{3}$};
\node        (sH-d) at (\xDots,\yRowH) {$\cdots$};
\node[state] (sH-m) at (\xLast,\yRowH) {$s_{H-1}^{m}$};
\node[state] (gH)   at (\xAgg,\yRowH) {$g_{H-1}$};

% -------------------------
% Data support boxes: extend LEFT to create writing room
% -------------------------
\coordinate (L0)  at ([xshift=-\boxLeftExtra]s0-1.west);
\coordinate (L1)  at ([xshift=-\boxLeftExtra]s1-1.west);
\coordinate (Lh)  at ([xshift=-\boxLeftExtra]sh-1.west);
\coordinate (LHm) at ([xshift=-\boxLeftExtra]sHm-1.west);
\coordinate (LH)  at ([xshift=-\boxLeftExtra]sH-1.west);

\node[obsbox,fit=(L0)(s0-m)] (box0) {};
\node[obsbox,fit=(L1)(s1-m)] (box1) {};
\node[obsbox,fit=(Lh)(sh-m)] (boxh) {};
\node[obsbox,fit=(LHm)(sHm-m)] (boxHm) {};
\node[obsbox,fit=(LH)(sH-m)] (boxH) {};

% Labels INSIDE boxes (shifted down a bit more to avoid top-border collision)
\def\labx{2.5mm}
\def\laby{-3.2mm}
\node[anchor=north west] at ([xshift=\labx,yshift=\laby]box0.north west) {$D_0$};
\node[anchor=north west] at ([xshift=\labx,yshift=\laby]box1.north west) {$D_1$};
\node[anchor=north west] at ([xshift=\labx,yshift=\laby]boxh.north west) {$D_h$};
\node[anchor=north west] at ([xshift=\labx,yshift=\laby]boxHm.north west) {$D_{H-2}$};
\node[anchor=north west] at ([xshift=\labx,yshift=\laby]boxH.north west) {$D_{H-1}$};

% Reward notes INSIDE boxes (left margin)
\node[anchor=north west,align=left] at ([xshift=\labx,yshift=-8.2mm]box0.north west)
  {\scriptsize $r_0=0$};
\node[anchor=north west,align=left] at ([xshift=\labx,yshift=-8.2mm]box1.north west)
  {\scriptsize $r_1=0$};
\node[anchor=north west,align=left] at ([xshift=\labx,yshift=-8.2mm]boxh.north west)
  {\scriptsize $r_h=0$};
\node[anchor=north west,align=left] at ([xshift=\labx,yshift=-8.2mm]boxHm.north west)
  {\scriptsize $r_{H-2}=0$};

\node[anchor=north west,align=left] at ([xshift=\labx,yshift=-6.5mm]boxH.north west)
  {\scriptsize $r_{H-1}\in\{\pm1\}$\\[-0.5mm]\scriptsize $\E[r_{H-1}]=r_\infty$};

% -------------------------
% Right-chain reward labels (stage-indexed, matches text)
% -------------------------
\node[anchor=west,align=left] at ([xshift=3mm]g0.east)
  {\scriptsize $r_0=\alpha_0-\alpha_1$};
\node[anchor=west,align=left] at ([xshift=3mm]g1.east)
  {\scriptsize $r_1=\alpha_1-\alpha_2$};
\node[anchor=west,align=left] at ([xshift=3mm]gh.east)
  {\scriptsize $r_h=\alpha_h-\alpha_{h+1}$};
\node[anchor=west,align=left] at ([xshift=3mm]gHm.east)
  {\scriptsize $r_{H-2}=\alpha_{H-2}-\alpha_{H-1}$};
\node[anchor=west,align=left] at ([xshift=3mm]gH.east)
  {\scriptsize $r_{H-1}=\alpha_{H-1}$};

% -------------------------
% Transitions
% -------------------------
% a2 edges (vertical, observed chain)
\foreach \u/\v in {s0-1/s1-1,s0-2/s1-2,s0-3/s1-3,s0-m/s1-m,
                  s1-1/sh-1,s1-2/sh-2,s1-3/sh-3,s1-m/sh-m,
                  sh-1/sHm-1,sh-2/sHm-2,sh-3/sHm-3,sh-m/sHm-m,
                  sHm-1/sH-1,sHm-2/sH-2,sHm-3/sH-3,sHm-m/sH-m} {
  \draw[atwo] (\u) -- (\v);
}

% a1 edges (to aggregator at next layer)
\foreach \u/\v in {s0-1/g1,s0-2/g1,s0-3/g1,s0-m/g1,
                  s1-1/gh,s1-2/gh,s1-3/gh,s1-m/gh,
                  sh-1/gHm,sh-2/gHm,sh-3/gHm,sh-m/gHm,
                  sHm-1/gH,sHm-2/gH,sHm-3/gH,sHm-m/gH} {
  \draw[aone] (\u) -- (\v);
}

% aggregator chain: show BOTH actions with slight separation
\foreach \u/\v in {g0/g1,g1/gh,gh/gHm,gHm/gH} {
  \draw[aone] (\u) to[bend left=10] (\v);
  \draw[atwo] (\u) to[bend right=10] (\v);
}

% dots
\node at (\xAgg,{(\yRowMid+\yRowHm)/2}) {$\vdots$};
\node at (\xOne,{(\yRowMid+\yRowHm)/2}) {$\vdots$};

\end{scope}
\end{tikzpicture}

}

\title{RL with Linear Features: When Does It Work \\ \& When Doesn't It Work?}
\subtitle{Part 3: Lower Bounds\\
CS 2284: Foundations of Reinforcement Learning}
\author{Kiant\'{e} Brantley \& Sham Kakade}
\date{}

\begin{document}

% =============================================================================
% TITLE
% =============================================================================

\begin{frame}[plain]
    \titlepage
\end{frame}
\addtocounter{framenumber}{-1}

% =============================================================================
% Part I: The Setup (Recap)
% =============================================================================

\section{Recap: The Split Universe}

\begin{frame}{1. The Critical Decomposition}
    To analyze error propagation, we introduce the \textbf{Infinite-Sample Target}.
    
    \vspace{0.2cm}
    
    \textbf{Definition ($f^\star_h$):} The function LSVI would learn with infinite data.
    \[
    f^\star_h \;:=\; \argmin_{f \in \Fcal} \E_{\rho} \left[ (f(s,a) - \Tcal_h \widehat{Q}_{h+1}(s,a))^2 \right] \;=\; \Pi_{\Fcal, \rho} ( \Tcal_h \widehat{Q}_{h+1} )
    \]
    \emph{(Ideally, we want $f^\star_h$ to track the optimal value $Q^\star_h$)}.
    
    \vspace{0.3cm}
    
    \textbf{The Error Triangle:}
    We split the total error into Statistical Variance and Recursive Stability.
    \[
    \|\widehat{Q}_h - Q^\star_h\|_\infty \le \underbrace{\|\widehat{Q}_h - f^\star_h\|_\infty}_{\text{Statistical Error}} + \underbrace{\|f^\star_h - Q^\star_h\|_\infty}_{\text{Recursive Stability}}
    \]
    \begin{itemize}
        \item \textbf{Statistical Error:} Controlled by $N$ and D-Optimal Design ($\approx \sqrt{d/N}$).
        \item \textbf{Recursive Stability:} This is where the universe splits.
    \end{itemize}
\end{frame}

\begin{frame}{2. The Split Universe}
    How does the Recursive Error $\|f^\star_h - Q^\star_h\|_\infty$ behave?\\
    Recall $f^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h \widehat{Q}_{h+1})$
    and $Q^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h Q^\star_{h+1})$ (by realizability).
    
    \vspace{0.2cm}
    
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Universe A: Completeness}
        \textbf{Assumption:} $\Tcal_h$ preserves linearity.
        
        \vspace{0.2cm}
         Since $\widehat{Q}_{h+1} \in \Fcal$, we have:
        $f^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h \widehat{Q}_{h+1}) =
        \Tcal_h \widehat{Q}_{h+1}$, and thus:

        \vspace*{-0.3cm}
        \begin{align*}
        \|f^\star_h - Q^\star_h\|_\infty &=\|\Tcal_h \widehat{Q}_{h+1}
        - \Tcal_h Q_{h+1}^\star \|_\infty\\
          &\leq \| \widehat{Q}_{h+1} - Q_{h+1}^\star \|_\infty
        \end{align*}
        \textbf{Result:} Error is stable (contraction).
        \end{block}

        \column{0.48\textwidth}
        \begin{block}{Universe B: Realizability Only}
        \textbf{Assumption:} Only $Q^\star \in \Fcal$.
        
        \vspace{0.2cm}
        The Bellman backup $\Tcal_h \widehat{Q}_{h+1}$ may be \textbf{non-linear} (off-manifold).
        
        \vspace{0.2cm}
        We must project it back to $\Fcal$:
        \[
        f^\star_h = \Pi_{\Fcal,\rho} (\text{Non-Linear Target})
        \]
        \textbf{Result:} We pay for the stability of the projection operator $\Pi_{\Fcal,\rho}$.
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}{3. The Amplification Mechanism (Universe B)}
    Without Completeness, we must bound the stability of the projection $\Pi_{\Fcal,\rho}$.
    Let $\Delta = \Tcal_h \widehat{Q}_{h+1} - \Tcal_h Q^\star_{h+1}$.
    
    \vspace{0.2cm}
    
    \textbf{The Chain of Inequalities:}
    \begin{align*}
        \|f^\star_h - Q^\star_h\|_\infty &= \|\Pi_{\Fcal,\rho} \Delta\|_\infty \\
        &\le \sqrt{d} \cdot \|\Pi_{\Fcal,\rho} \Delta\|_{L_2(\rho)} \quad \text{(\textbf{Step 1:} Norm Equivalence)} \\
        &\le \sqrt{d} \cdot \|\Delta\|_{L_2(\rho)} \quad \quad \quad \text{(\textbf{Step 2:} $L_2$ Stability of LS)} \\
        &\le \sqrt{d} \cdot \|\Delta\|_\infty \quad \quad \quad \quad \quad \text{(\textbf{Step 3:} Norm Monotonicity)} \\
        &\le \sqrt{d} \cdot \|\widehat{Q}_{h+1} - Q^\star_{h+1}\|_\infty
    \end{align*}

    \vspace{0.1cm}
    
    \textbf{The Question for Today:}
    Is this $\sqrt{d}$ amplification an artifact of our loose analysis? Or is it a fundamental barrier?
    
    \centering
    \alert{Today: The amplification is real, information theoretically.}
\end{frame}

% =============================================================================
% Part II: The Offline Lower Bound
% =============================================================================

\section{The Offline Lower Bound}

\begin{frame}{The Offline Setting: Strong Assumptions}
    Let's try to "break" the lower bound by making extremely strong assumptions.
    
    \textbf{The Setup:}
    \begin{itemize}
        \item \textbf{Offline Data:} Fixed datasets $D_0, \dots, D_{H-1}$.
        \item \textbf{Assumption 1 (All-Policies Realizability):} 
        For \emph{every} policy $\pi$, $Q^\pi_h$ is linear in $\phi$.
        \item \textbf{Assumption 2 (Perfect Coverage):} 
        The offline data covariance is "isotropic" (perfectly conditioned):
        \[
        \Sigma_{D_h} = \frac{1}{d} I \quad \text{for all } h.
        \]
    \end{itemize}

    \vspace{0.3cm}
    \textbf{The Question:} Under these ideal conditions (Realizability + Perfect Coverage), can we learn $V^\pi$ with $\poly(d, H)$ samples?
\end{frame}

\begin{frame}{Theorem: Offline Hardness}
  \begin{block}{Theorem 3.2 (Offline Policy Evaluation Hardness)}
    There exists a class of MDPs with dimension $d$ and horizon $H$ satisfying \textbf{All-Policies Realizability} and \textbf{Perfect Coverage}, such that any algorithm requires
    \[
    N \ge \Omega\left( (d/2)^H \right)
    \]
    samples to estimate $V^\pi(s_0)$ to constant accuracy.
    \end{block}

    \vspace{0.3cm}
    \textbf{Interpretation:}
    Even with infinite data in the "covered" directions, variance accumulates exponentially in the "hidden" directions.
\end{frame}



% =============================================================================
% Part II: The Construction
% =============================================================================


\section{The Construction}

% --- Slide 1: Schematic and Definitions ---
\begin{frame}{Offline PE hard instance (schematic)}
\begin{columns}[T,onlytextwidth]
  % ---------------- Left: dynamics ----------------
  \begin{column}{0.7\textwidth}
    \centering
    % Using the macro defined above
    %\resizebox{0.95\textwidth}{!}{ \HardInstanceFig }
    \resizebox{0.95\textwidth}{!}{\input{tikzpic_lb_lecture} }
  \end{column}

  % ---------------- Right: Legend ----------------
  \begin{column}{0.3\textwidth}
  \footnotesize
  
  % Actions Legend
  \noindent
  \begin{tikzpicture}
    \node[draw,inner sep=3.5pt] (leg) {%
      \begin{tikzpicture}[>=Latex]
        \draw[thick] (0,0) -- (1.2,0) node[midway,above] {$a_1$};
        \draw[thick,dashed] (0,-0.5) -- (1.2,-0.5) node[midway,above] {$a_2$};
      \end{tikzpicture}
    };
  \end{tikzpicture}
  
  \vspace{0.5em}
  
  \noindent\textbf{Features.}\\%[-0.25em]
  $\phi(s_h^c,a_1)=e_c$\\
  $\phi(s_h^c,a_2)=e_{m+c}$\\
  $\phi(g_h,a)=\frac{1}{\sqrt m}(e_1+\cdots+e_m)$
  
  \vspace{0.5em}
  
  \noindent\textbf{Reward parameters.}\\%[-0.25em]
  $r_\infty\in\{0,\ m^{-H/2}\}$\\
  $\alpha_h := r_\infty\, m^{(H-h)/2}$
  
  \vspace{0.5em}
  
  \noindent\textbf{Aggregator rewards.}\\%[-0.25em]
  $r_h(g_h,\cdot)=\alpha_h-\alpha_{h+1}$.\\
  Last layer: $r_{H-1}(g_{H-1})=\alpha_{H-1}$.
  
  \end{column}
\end{columns}
\end{frame}

% --- Slide 0: High Level Intuition ---
\begin{frame}{The High-Level Idea: Hiding a Secret}
    \textbf{The Goal:} Construct an MDP where the optimal value depends on a tiny parameter $r_\infty$, but we can only learn it by solving a "hard" regression problem.
    
    \vspace{0.3cm}
    \textbf{The Mechanism (Two Chains):}
    \begin{enumerate}
        \item \textbf{The Hidden Chain ($g$):} 
        Carries the true "signal" ($V \approx \text{Large}$).
        \item \textbf{The Observed Chain ($s$):} 
        Carries a "noisy version" of the signal ($V \approx \text{Small}$).
    \end{enumerate}
    
    \vspace{0.3cm}
    \textbf{The "Trap":}
    Linearly Realizable means $V(g) \approx \sqrt{d} \times V(s)$.
    \begin{itemize}
        \item To know the large value at $g$, we must estimate the small value at $s$.
        \item Estimating the small value requires cancelling out huge noise.
        \item This requires exponentially many samples.
    \end{itemize}
\end{frame}

% --- Slide 2: Dynamics ---
\begin{frame}{State space and transitions (two chains)}
\small
Fix $m\ge 1$, set $d=2m$, $\Acal=\{a_1,a_2\}$, horizon $H$.

\vspace{0.5em}
\textbf{State space (layered).}
\[
\Scal_h := \{s_h^1,\dots,s_h^m\}\cup\{g_h\},\qquad h=0,\dots,H-1,
\quad\text{and } s_0=g_0.
\]

\vspace{0.75em}
\textbf{Deterministic transitions for $h\le H-2$:}
\begin{align*}
&\text{From any state, action $a_1$ jumps to aggregators:}
&&P(g_{h+1}\mid s,a_1)=1 \quad \forall s\in\Scal_h.\\
&\text{Action $a_2$ keeps index on observed chain:}
&&P(s_{h+1}^i\mid s_h^i,a_2)=1 \quad \forall i\in[m].\\
&\text{Aggregator chain stays on aggregators:}
&&P(g_{h+1}\mid g_h,a_2)=1.
\end{align*}

\vspace{0.5em}
\textbf{Interpretation:} $a_2$ keeps you in the observed world; $a_1$ (or being at $g_h$) takes you off it.
\end{frame}

% --- Slide 3: Features & Coverage ---
\begin{frame}{Features and Coverage}
    \textbf{The Features:}
    We use the standard basis for the observed states, and a dense "mean" vector for the aggregators.
    \begin{itemize}
        \item $\phi(s^i, a_1) = e_i$ \quad and \quad $\phi(s^i, a_2) = e_{m+i}$
        \item $\phi(g, \cdot) = \frac{1}{\sqrt{m}} \sum_{i=1}^m e_i$
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Observed Data Distribution ($\mu_h$):}
    Uniform distribution over the $m$ observed states $\{s_h^i\}_{i=1}^m$ and actions.
    
    \vspace{0.3cm}
    \textbf{Coverage Check:}
    Calculate the covariance matrix of the data:
    \[
    \Sigma_{D} \;=\; \frac{1}{2m} \sum_{i=1}^m (e_i e_i^\top + e_{m+i} e_{m+i}^\top) \;=\; \frac{1}{2m} I \;=\; \frac{1}{d} I
    \]
    
    \alert{Perfect isotropic coverage!} The offline data explores every dimension of the feature space equally.
\end{frame}

% --- Slide 4: Realizability (Base Case) ---
\begin{frame}{Step 1: Verifying Realizability (Base Case $h=H-1$)}
    Let's prove that $Q_{H-1}$ is linear.
    
    \textbf{The Values:}
    \begin{itemize}
        \item Leaves ($s^i$): Mean reward is $r_\infty$. $\implies Q(s^i, \cdot) = r_\infty$.
        \item Aggregator ($g$): Deterministic reward $\alpha_{H-1} = r_\infty \sqrt{m}$. $\implies Q(g, \cdot) = r_\infty \sqrt{m}$.
    \end{itemize}

    \vspace{0.2cm}
    \textbf{The Linear Fit:}
    Can we find a $\theta$ such that $\theta^\top \phi = Q$?
    Try the uniform vector: $\theta = \sum_{j=1}^{2m} r_\infty e_j$.
    
    \vspace{0.2cm}
    \textbf{Verification:}
    \begin{enumerate}
        \item \textbf{At $s^i$:} $\theta^\top \phi(s^i) = (\sum r_\infty e_j)^\top e_i = r_\infty$. \textcolor{green!60!black}{\checkmark}
        \item \textbf{At $g$:} $\theta^\top \phi(g) = (\sum r_\infty e_j)^\top (\frac{1}{\sqrt{m}} \sum e_k) = \frac{r_\infty}{\sqrt{m}} (m) = r_\infty \sqrt{m}$. \textcolor{green!60!black}{\checkmark}
    \end{enumerate}
    
    \small
    \emph{It fits perfectly! The linearity forces the value at $g$ to be $\sqrt{m}$ times the value at $s$.}
\end{frame}

% --- Slide 5: Realizability (Inductive Step) ---
\begin{frame}{Step 2: Verifying Realizability (Inductive Step $h < H-1$)}
    Assume $V_{h+1}(g_{h+1}) = \alpha_{h+1}$.
    
    \textbf{The Values at step $h$:}
    \begin{itemize}
        \item \textbf{Action $a_1$ (Jump):} $Q_h(s^i, a_1) = 0 + V_{h+1}(g_{h+1}) = \alpha_{h+1}$.
        \item \textbf{Aggregator $g_h$:} $V_h(g_h) = (\alpha_h - \alpha_{h+1}) + \alpha_{h+1} = \alpha_h$.
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{The Linear Fit:}
    We need $\theta$ to match $\alpha_{h+1}$ at leaves and $\alpha_h$ at aggregator.
    Try $\theta = \sum_{i=1}^m \alpha_{h+1} e_i$.
    
    \vspace{0.2cm}
    \textbf{Verification:}
    \begin{enumerate}
        \item \textbf{At $s^i$:} $\theta^\top e_i = \alpha_{h+1}$. \textcolor{green!60!black}{\checkmark}
        \item \textbf{At $g$:} $\theta^\top (\frac{1}{\sqrt{m}}\sum e_i) = \sqrt{m} \alpha_{h+1}$.
        \item Does $\sqrt{m} \alpha_{h+1} = \alpha_h$?
        \[
        \sqrt{m} (r_\infty m^{(H-(h+1))/2}) = r_\infty m^{(H-h)/2} = \alpha_h. \quad \textcolor{green!60!black}{\checkmark}
        \]
    \end{enumerate}
    \small
    \emph{Realizability holds, but implies geometric amplification at every step.}
\end{frame}

% --- Slide 6: Impossibility ---
\begin{frame}{Step 3: The Impossibility Result}
    \textbf{Putting it together:}
    \begin{itemize}
        \item We verified Realizability holds for all $h$.
        \item Value at start: $V^\pi(g_0) = \alpha_0 = m^{H/2} r_\infty$.
    \end{itemize}

    \vspace{0.3cm}
    \textbf{The Hypothesis Test:}
    To decide if $V^\pi(g_0)$ is 0 or 1, we must distinguish:
    \[
    r_\infty = 0 \quad \text{vs} \quad r_\infty = m^{-H/2}
    \]
    
    \textbf{The Bottleneck:}
    We never see $g_h$. We only see the noisy leaves at $H-1$.
    \begin{itemize}
        \item Signal Gap: $\Delta = m^{-H/2}$.
        \item Samples Needed: $N \approx 1/\Delta^2 = m^H = (d/2)^H$.
    \end{itemize}
    
    \centering
    \alert{We need exponential samples to track the value of the unseen state.}
\end{frame}

% --- Slide 2: Dynamics ---
\begin{frame}{State space and transitions (two chains)}
\small
Fix $m\ge 1$, set $d=2m$, $\Acal=\{a_1,a_2\}$, horizon $H$.

\vspace{0.5em}
\textbf{State space (layered).}
\[
\Scal_h := \{s_h^1,\dots,s_h^m\}\cup\{g_h\},\qquad h=0,\dots,H-1,
\quad\text{and } s_0=g_0.
\]

\vspace{0.75em}
\textbf{Deterministic transitions for $h\le H-2$:}
\begin{align*}
&\text{From any state, action $a_1$ jumps to aggregators:}
&&P(g_{h+1}\mid s,a_1)=1 \quad \forall s\in\Scal_h.\\
&\text{Action $a_2$ keeps index on observed chain:}
&&P(s_{h+1}^i\mid s_h^i,a_2)=1 \quad \forall i\in[m].\\
&\text{Aggregator chain stays on aggregators:}
&&P(g_{h+1}\mid g_h,a_2)=1.
\end{align*}

\vspace{0.5em}
\textbf{Interpretation:} $a_2$ keeps you in the observed world; $a_1$ (or being at $g_h$) takes you off it.
\end{frame}

% --- Slide 3: Features & Coverage ---
\begin{frame}{Features and Coverage}
    \textbf{The Features:}
    We use the standard basis for the observed states, and a dense "mean" vector for the aggregators.
    \begin{itemize}
        \item $\phi(s^i, a_1) = e_i$ \quad and \quad $\phi(s^i, a_2) = e_{m+i}$
        \item $\phi(g, \cdot) = \frac{1}{\sqrt{m}} \sum_{i=1}^m e_i$
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Observed Data Distribution ($\mu_h$):}
    Uniform distribution over the $m$ observed states $\{s_h^i\}_{i=1}^m$ and actions.
    
    \vspace{0.3cm}
    \textbf{Coverage Check:}
    Calculate the covariance matrix of the data:
    \[
    \Sigma_{D} \;=\; \frac{1}{2m} \sum_{i=1}^m (e_i e_i^\top + e_{m+i} e_{m+i}^\top) \;=\; \frac{1}{2m} I \;=\; \frac{1}{d} I
    \]
    
    \alert{Perfect isotropic coverage!} The offline data explores every dimension of the feature space equally.
\end{frame}

% --- Slide 4: Base Case (Bottom Up) ---
\begin{frame}{Verification Step 1: The Base Case ($h=H-1$)}
    Let's check Realizability at the last layer.
    
    \begin{itemize}
        \item \textbf{On the observed states ($s_{H-1}^i$):}
        The rewards are Bernoulli$(r_\infty)$.
        \[
        Q_{H-1}(s_{H-1}^i, \cdot) = \E[r] = r_\infty
        \]
        \item \textbf{On the aggregator ($g_{H-1}$):}
        The reward is deterministic and large.
        \[
        Q_{H-1}(g_{H-1}, \cdot) = \alpha_{H-1} = r_\infty \sqrt{m}
        \]
    \end{itemize}
    
    \textbf{Does a linear $\theta$ exist?}
    Consider $\theta = \sum_{j=1}^{2m} r_\infty e_j$ (the all-$r_\infty$ vector).
    \begin{enumerate}
        \item $\phi(s^i)^\top \theta = e_i^\top \theta = r_\infty$. \textbf{(Matches!)}
        \item $\phi(g)^\top \theta = (\frac{1}{\sqrt{m}} \sum e_i)^\top \theta = \frac{1}{\sqrt{m}} (m \cdot r_\infty) = r_\infty \sqrt{m}$. \textbf{(Matches!)}
    \end{enumerate}
\end{frame}

% --- Slide 5: Inductive Step (Amplification) ---
\begin{frame}{Verification Step 2: The Amplification ($h < H-1$)}
    Suppose $V_{h+1}(g_{h+1}) = \alpha_{h+1}$.
    
    \textbf{Q-values at layer $h$:}
    \begin{itemize}
        \item \textbf{Action $a_1$ (Jump):} Transitions to $g_{h+1}$.
        \[
        Q_h(s_h^i, a_1) = 0 + V_{h+1}(g_{h+1}) = \alpha_{h+1}
        \]
        \item \textbf{Aggregator $g_h$:} Also transitions to $g_{h+1}$.
        \[
        V_h(g_h) = r_h(g_h) + V_{h+1}(g_{h+1}) = (\alpha_h - \alpha_{h+1}) + \alpha_{h+1} = \alpha_h
        \]
    \end{itemize}
    
    \textbf{The Geometric Amplification:}
    If $\theta$ predicts $\alpha_{h+1}$ on the standard basis (leaves), it \emph{must} predict $\sqrt{m} \alpha_{h+1}$ on the dense vector (aggregator).
    \[
    \text{If } \theta^\top e_i = \alpha_{h+1} \implies \theta^\top \phi(g) = \sqrt{m} \alpha_{h+1}
    \]
    Thus, $V(g_h) = \sqrt{m} V(g_{h+1})$. The value grows by $\sqrt{m}$ at every step!
\end{frame}

% --- Slide 6: Impossibility ---
\begin{frame}{Step 3: The Impossibility Result}
    \textbf{Putting it together:}
    \begin{itemize}
        \item We verified Realizability holds for all $h$.
        \item Value at start: $V^\pi(g_0) = m^{H/2} r_\infty$.
    \end{itemize}

    \vspace{0.3cm}
    \textbf{The Hypothesis Test:}
    To decide if $V^\pi(g_0)$ is 0 or 1, we must distinguish:
    \[
    r_\infty = 0 \quad \text{vs} \quad r_\infty = m^{-H/2}
    \]
    
    \textbf{The Bottleneck:}
    We never see $g_h$. We only see the noisy leaves at $H-1$.
    \begin{itemize}
        \item Signal Gap: $\Delta = m^{-H/2}$.
        \item Samples Needed: $N \approx 1/\Delta^2 = m^H = (d/2)^H$.
    \end{itemize}
    
    \centering
    \alert{We need exponential samples to track the value of the unseen state.}
\end{frame}

% --- Slide 7: LSPE Discussion ---
\begin{frame}{Why LSPE Fails (Variance Explosion)}
    \textbf{The Paradox:}
    LSPE is an unbiased estimator. If the sample size is infinite, it converges to the true parameters. Why does it fail here?
    
    \vspace{0.3cm}
    \textbf{Answer: Exponential Variance.}
    \begin{itemize}
        \item The parameter $\theta_h$ has a "hidden direction" (the direction pointing to $\phi(g)$).
        \item This direction is orthogonal to the data distribution at step $h$.
        \item However, $\theta_h$ is coupled to $\theta_{h+1}$.
        \item The small noise in $\widehat{\theta}_{h+1}$ gets multiplied by $\sqrt{m}$ when we solve for $\widehat{\theta}_h$.
        \item Repeated $H$ times: $\text{Var}(\widehat{V}_0) \approx (\sqrt{m})^{2H} \times \text{Noise} = m^H$.
    \end{itemize}
    
    \small
    \emph{Even with unbiased estimators, the variance can scale with the "off-policy" condition number (here $\sqrt{d}$), compounded $H$ times.}
  \end{frame}

\end{document}

% =============================================================================
% Part II: The Construction (Revised)
% =============================================================================

\section{The Construction}

% --- Slide 0: High Level Intuition (New) ---
\begin{frame}{The High-Level Idea: Hiding a Secret}
    \textbf{The Goal:} Construct an MDP where the optimal value depends on a tiny parameter $r_\infty$, but we can only learn it by solving a "hard" regression problem.
    
    \vspace{0.3cm}
    \textbf{The Mechanism (Two Chains):}
    \begin{enumerate}
        \item \textbf{The Hidden Chain ($g$):} 
        Carries the true "signal" ($V \approx \text{Large}$).
        \item \textbf{The Observed Chain ($s$):} 
        Carries a "noisy version" of the signal ($V \approx \text{Small}$).
    \end{enumerate}
    
    \vspace{0.3cm}
    \textbf{The "Trap":}
    Linearly Realizable means $V(g) \approx \sqrt{d} \times V(s)$.
    \begin{itemize}
        \item To know the large value at $g$, we must estimate the small value at $s$.
        \item Estimating the small value requires cancelling out huge noise.
        \item This requires exponentially many samples.
    \end{itemize}
\end{frame}

% --- Slide 1: Schematic and Definitions ---
\begin{frame}{Offline PE hard instance (schematic)}
\begin{columns}[T,onlytextwidth]
  % ---------------- Left: dynamics ----------------
  \begin{column}{0.65\textwidth}
    \centering
    \resizebox{0.95\textwidth}{!}{ \HardInstanceFig }
  \end{column}

  % ---------------- Right: Legend ----------------
  \begin{column}{0.35\textwidth}
  \footnotesize
  
  \noindent
  \begin{tikzpicture}
    \node[draw,inner sep=3.5pt] (leg) {%
      \begin{tikzpicture}[>=Latex]
        \draw[thick] (0,0) -- (1.2,0) node[midway,above] {$a_1$};
        \draw[thick,dashed] (0,-0.5) -- (1.2,-0.5) node[midway,above] {$a_2$};
      \end{tikzpicture}
    };
  \end{tikzpicture}
  
  \vspace{0.4em}
  \noindent\textbf{Features.}\\%[-0.25em]
  $\phi(s_h^c,a_1)=e_c$\\
  $\phi(s_h^c,a_2)=e_{m+c}$\\
  $\phi(g_h,a)=\frac{1}{\sqrt m}\sum_{i=1}^m e_i$
  
  \vspace{0.4em}
  \noindent\textbf{Reward parameters.}\\%[-0.25em]
  $r_\infty\in\{0,\ m^{-H/2}\}$\\
  $\alpha_h := r_\infty\, m^{(H-h)/2}$
  
  \vspace{0.4em}
  \noindent\textbf{Aggregator rewards.}\\%[-0.25em]
  $r_h(g_h,\cdot)=\alpha_h-\alpha_{h+1}$.\\
  Last layer: $r_{H-1}(g_{H-1})=\alpha_{H-1}$.
  
  \end{column}
\end{columns}
\end{frame}


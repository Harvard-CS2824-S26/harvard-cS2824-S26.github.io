\documentclass[aspectratio=169]{beamer}

% The 'handout' option collapses all overlays into single slides
%\documentclass[aspectratio=169, handout]{beamer}

\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{algorithm, algorithmic}

% --- Standard Beamer Theme ---
\usetheme{Madrid}
\usecolortheme{default}


\include{macros}

\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc}


\title{RL with Linear Features: When Does It Work \\ \& When Doesn't It Work?}
\subtitle{Part 1: The Assumption Ladder \& Bellman Completeness\\
CS 2284: Foundations of Reinforcement Learning}
\author{Kiant\'{e} Brantley \& Sham Kakade}
\date{}

\begin{document}


\begin{frame}[plain]
    \titlepage
\end{frame}
\addtocounter{framenumber}{-1}


\section{Today}

\begin{frame}{Agenda}

    \textbf{Announcements}
    \begin{itemize}
        \item Second reading assignment is out.
    \end{itemize}

    \vfill

    \textbf{Recap++}
    \begin{itemize}
        \item Wrap up our tabular sample complexity analysis (Chapter 2).
        \item \textbf{Minimax Result:} We established the fundamental limits of tabular learning.
    \end{itemize}

    \vfill

    \textbf{Today}
    \begin{itemize}
        \item Function Approximation. \textbf{We'll move beyond tabular RL!}
    \end{itemize}

\end{frame}


% =============================================================================
% Part I: Motivation & Algorithm Setup
% =============================================================================


\section{Motivation}

\begin{frame}{Motivation: Beyond Tabular RL}
    \textbf{Recap: Tabular MDPs}
    \begin{itemize}
        \item State space $\Scal$, Action space $\Acal$.
        \item Sample complexity scales with $|\Scal||\Acal|$.
        \item \textbf{Problem:} In many real-world applications (robotics, games, healthcare), $|\Scal|$ is enormous or continuous.
    \end{itemize}

    \vspace{0.5cm}

    \textbf{Function Approximation}
    \begin{itemize}
        \item Introduce a feature map $\phi: \Scal \times \Acal \to \R^d$.
        \item Approximate values using linear functions:
        \[
        f_\theta(s,a) = \theta^\top \phi(s,a)
        \]
        \item \textbf{Goal:} Sample complexity polynomial in $d$ (and $H$), independent of $|\Scal|$.
    \end{itemize}
\end{frame}

\begin{frame}{Finite-horizon dynamic programming (reminder)}
    Stage-$h$ \textbf{optimal Bellman operator}:
    \[
    (\Tcal_h f)(s,a)
    \;:=\;
    r_h(s,a) + \E_{s'\sim P_h(\cdot\mid s,a)}\Big[\max_{a'\in\Acal} f(s',a')\Big].
    \]
    Optimal $Q$ satisfies backward recursion:
    \[
    Q^\star_H \equiv 0,
    \qquad
    Q_h^\star = \Tcal_h Q_{h+1}^\star \quad\text{for } h=H-1,\dots,0.
    \]
\end{frame}

\begin{frame}{Least-Squares Value Iteration (LSVI)}
    \textbf{Setting:} Finite Horizon $H$, Generative Model (Simulator).

    \vspace{0.3cm}

    \textbf{Algorithm:} Backward Induction via Regression.
    \begin{enumerate}
        \item Initialize $\widehat{V}_H(s) = 0$.
        \item For $h = H-1, \dots, 0$:
        \begin{itemize}
            \item \textbf{Collect Data:} Generate dataset $D_h = \{(s_i, a_i, r_i, s_i')\}_{i=1}^N$.
              \pause
            \item \textbf{Form Targets:} Compute regression targets using the \emph{next} value function:
            \[
            y_i = r_h(s_i, a_i) + \widehat{V}_{h+1}(s_i')
            \]
              \pause
            \item \textbf{Regression:} Solve for parameters $\widehat{\theta}_h$:
            \[
            \widehat{\theta}_h \in \argmin_{\theta \in \R^d} \sum_{i=1}^N \left( \theta^\top \phi(s_i, a_i) - y_i \right)^2
            \]
              \pause
            \item \textbf{Update:} Set $\widehat{Q}_h(s,a) = \widehat{\theta}_h^\top \phi(s,a)$ and $\widehat{V}_h(s) = \max_{a} \widehat{Q}_h(s,a)$.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{The Central Question \& The Intuition Trap}
  \begin{center}
    \Large \emph{When do linear features actually buy you $\poly(d)$ sample complexity --- and when do they fundamentally not?}
  \end{center}
  
  \vspace{0.3cm}
  
              \pause
  \textbf{The Intuition Trap:}
  \begin{itemize}
  \item Standard Supervised Learning: ``If the target function is linear, we can learn it with $O(d)$ samples.''
              \pause
  \item \textbf{But RL is different:} LSVI is a \textbf{composition} of regressions.
    \begin{itemize} 
    \item The target for stage $h$ depends on our \emph{own estimate} at $h+1$:
      \[
        \text{Target}_h(s,a) \approx r(s,a) + \E \Big[ \max_{a'} \widehat{Q}_{h+1}(s', a') \Big]
      \]
              \pause
    \item \textbf{Crucial Question:} Even if $Q^\star$ is linear, does the target defined by $\widehat{Q}_{h+1}$ remain learnable (linear)?
    \end{itemize}
  \item If the target ``falls off the manifold'', how do errors compound?
  \end{itemize}
\end{frame}

% =============================================================================
% Part II: The Assumption Ladder
% =============================================================================

\section{The Assumption Ladder}

\begin{frame}{The Assumption Ladder}
  We can organize linear RL assumptions from weakest (hardest) to strongest (easiest).
  
  \vspace{0.3cm}
  
  \begin{itemize}
              \pause
  \item[(A)] \textbf{Agnostic Approximation} \\
    \footnotesize No realizability. $Q^\star$ is ``close'' to linear. \\
    \textit{Status: Hard. Requires strong distribution assumptions.}
    
              \pause
    \vspace{0.2cm}
  \item[(B)] \textbf{Linear $Q^\star$ Realizability} \\
    \footnotesize $Q^\star_h(s,a) = (\theta^\star_h)^\top \phi(s,a)$. \\
    \textit{Status: \textbf{Insufficient.} Exponential lower bounds exist.}
    
              \pause
    \vspace{0.2cm}
  \item[(C)] \textbf{All-Policies Realizability} \\
    \footnotesize $Q^\pi_h(s,a) = (\theta^\pi_h)^\top \phi(s,a)$ for \textbf{all} $\pi$. \\
    \textit{Status: \textbf{Subtle.} Fails offline even with perfect coverage.}
    
              \pause
    \vspace{0.2cm}
  \item[(D)] \textbf{Linear Bellman Completeness} \\
    \footnotesize $\Tcal_h f \in \Fcal$ for all $f \in \Fcal$. \\
    \textit{Status: \textbf{Sufficient!} (This Lecture)}
  \end{itemize}
\end{frame}


\begin{frame}{Assumption (D): Linear Bellman Completeness}
    \textbf{Definition (Bellman Completeness)} \\
    For any linear function $f(s,a) = w^\top \phi(s,a)$, applying the Bellman optimality operator $\Tcal_h$ yields a function that is also linear in $\phi$.

    \[
    \forall w \in \R^d, \exists \theta \in \R^d \text{ such that }
    \]
    \[
    \underbrace{r_h(s,a) + \E_{s' \sim P_h(\cdot|s,a)} \left[ \max_{a'} w^\top \phi(s',a') \right]}_{(\Tcal_h f_w)(s,a)} = \theta^\top \phi(s,a)
    \]

    \vspace{0.3cm}
    \textbf{Key Implication:}
    \begin{itemize}
    \item If we run LSVI with finite samples, the \textbf{target} is always realizable.
    \item This reduces RL to a sequence of well-specified regression problems.
    \end{itemize}
  \end{frame}
  
\begin{frame}{Examples of Completeness}
    When does Linear Bellman Completeness hold?

    \begin{enumerate}
        \item \textbf{Tabular MDPs} \\
        $\phi(s,a)$ is a one-hot encoding of size $|\Scal||\Acal|$. \\
        Any function over $\Scal \times \Acal$ is linear in $\phi$.

        \vspace{0.1cm}

        \pause
      \item \textbf{Linear MDPs (Low-Rank Transition)} \\
        \[
        P_h(s'|s,a) = \sum_{i=1}^d \phi_i(s,a) \mu_i(s')
        \]
        Here, the transition dynamics themselves are linear in features.

        \vspace{0.1cm}

        \pause
        \item \textbf{Linear Quadratic Regulators (LQR)?} \\
        Yes, with quadratic features: $\phi(s,a) = [s^\top, a^\top, s^\top s, \dots]^\top$. \\
        (Value functions are quadratic in $s,a$).
    \end{enumerate}

    \vspace{0.2cm}
    \alert{\textit{Warning: Adding ``junk'' features can break completeness!} }
\end{frame}



\begin{frame}{The Error Propagation Question (Intuition)}
  
  Suppose we run approximate dynamic programming (like LSVI) where we force every estimate $\widehat{Q}_h$ to be a linear function:
  \[
    \widehat{Q}_H \equiv 0, \qquad \widehat{Q}_h \leftarrow \text{Project}_{\Fcal} \left( \Tcal_h \widehat{Q}_{h+1} \right).
  \]
  
  \textbf{The Subtle Danger:}
  \begin{itemize}
      \item Even if $Q^\star$ is linear, the \textbf{target} $\Tcal_h \widehat{Q}_{h+1}$ might \emph{not} be linear!
      \item If the target falls ``off-manifold'' (outside $\Fcal$), we incur a projection error (bias) at step $h$.
      \item \textbf{The Crucial Question:} How do these errors compound?
      \begin{itemize}
          \item Does the error at $h+1$ get amplified when we back up to $h$?
          \item Without \textbf{Completeness} (closure), this error can grow exponentially with $H$.
      \end{itemize}
  \end{itemize}
  
  \vspace{0.3cm}
  
  \centering
  \emph{Realizability of $Q^\star$ alone does not guarantee the intermediate targets remain learnable.}
\end{frame}


% =============================================================================
% Part IV: The Sampling Challenge
% =============================================================================

\section{The Sampling Challenge}

\begin{frame}{Refresher: LSVI Regression Step}
    To analyze the algorithm, let's focus on exactly what happens at stage $h$.
    
    \vspace{0.3cm}
    
    We have a fixed next-stage value $\widehat{V}_{h+1}$. We gather data $D_h$ and solve:
    \[
    \widehat{\theta}_h = \argmin_{\theta \in \R^d} \sum_{i=1}^N \left( \underbrace{\theta^\top \phi(s_i, a_i)}_{\text{Prediction}} - \underbrace{(r_i + \widehat{V}_{h+1}(s_i'))}_{\text{Target } y_i} \right)^2
    \]
    
    \vspace{0.2cm}
    
    \textbf{Why does this work?}
    \begin{itemize}
        \item \textbf{Completeness} implies the \emph{true expected target} is linear:
        \[
        \E[y_i \mid s_i, a_i] = (\Tcal_h \widehat{Q}_{h+1})(s_i, a_i) = \theta^\star_h \phi(s_i, a_i)
        \]
        \item So this is \textbf{well-specified} linear regression!
        \item Bias is zero; we only need to control variance.
    \end{itemize}
\end{frame}

\begin{frame}{Fixed Design OLS (The Tool)}
    Consider the standard linear regression setting:
    \[
    y_i = x_i^\top \theta^\star + \xi_i, \quad \text{with } \E[\xi_i|x_i]=0 \text{ (sub-Gaussian)}.
    \]
    
    The OLS estimator is $\widehat{\theta} = \Lambda^{-1}\left(\frac{1}{N} \sum_{i=1}^N x_i y_i\right)$, where $\Lambda = \frac{1}{N}\sum_i x_i x_i^\top$.
    
        \pause
    \begin{block}{Fixed-design OLS Bound}
    With probability at least $1-\delta$, the prediction error is bounded in the $\Lambda$-norm:
    \[
    \|\widehat\theta - \theta^\star\|_{\Lambda}
    \;\lesssim\;
    \sigma\,\sqrt{\frac{d\log(1/\delta)}{N} }.
    \]
    \end{block}
    
    \vspace{0.2cm}
    
        \pause
    This translates to a \textit{pointwise} bound using leverage scores:
    \[
    |(\widehat\theta-\theta^\star)^\top\phi(s,a)|
    \;\le\;
    \|\widehat\theta - \theta^\star\|_{\Lambda} \sqrt{\phi(s,a)^\top \Lambda^{-1} \phi(s,a)}
    \]
\end{frame}

\begin{frame}{The Hidden Failure Mode of OLS}
    
    \textbf{The Problem:}
    \begin{itemize}
        \item The OLS bound depends on $\Lambda = \frac{1}{N}\sum \phi(s_i,a_i)\phi(s_i,a_i)^\top$.
        \item It bounds the \textbf{average} prediction error (weighted by training data).
        \item RL requires \textbf{Uniform ($\ell_\infty$)} error bounds. We must predict well at \emph{any} state the optimal policy might visit.
    \end{itemize}
    
    \vspace*{-0.4cm}

    \centering
    \begin{tikzpicture}[scale=0.75]
        % Draw axes
        \draw[->] (-1,0) -- (6,0) node[right] {$x_1$};
        \draw[->] (0,-1) -- (0,4) node[above] {$x_2$};

        % Draw "Training Data" ellipse
        \draw[thick, blue, fill=blue!10, rotate=10] (2.5,0.0) ellipse (2.5cm and 0.5cm);
        \node[blue] at (2.5, -0.8) {Training Data Distribution ($\Lambda$)};

        % Draw data points
        \foreach \x in {1, 1.5, 2, 2.5, 3, 3.5, 4}
            \filldraw (\x, {0.2*(\x-2.5) + rand*0.2 + 0.5}) circle (2pt);

        % Draw the "Unknown State"
        \filldraw[red] (2.5, 3.0) circle (3pt);
        \node[red, above] at (2.5, 3.2) {Unseen State $(s,a)$};
        \draw[->, red, thick] (2.5, 3.0) -- (2.5, 0.8);
        \node[red, right, align=left] at (2.7, 2.5) {High Variance! \\ $\phi^\top \Lambda^{-1} \phi$ is huge};

    \end{tikzpicture}
    
    \small
    Backward induction may query \emph{outside} the ellipse, causing
    huge expansion.
\end{frame}

% =============================================================================
% Part V: D-Optimal Design
% =============================================================================

\section{D-Optimal Design}

\begin{frame}{D-optimal design: the leverage-minimizing geometry}
To guarantee uniform bounds, we must choose our training data carefully.

\vspace*{0.1cm}  
The feature set is:
\[
\Phi := \{\phi(s,a):(s,a)\in\Scal\times\Acal\}\subset\R^d.
\]

\begin{block}{D-optimal design (lemma; geometric fact)}
Suppose $\Phi$ is compact. There exists a distribution $\rho$ supported on at most $d(d+1)/2$ state--action pairs s.t.
with
\[
\Sigma := \E_{(s,a)\sim\rho}\big[\phi(s,a)\phi(s,a)^\top\big],
\]
we have $\Sigma\succ 0$ and
\[
\sup_{(s,a)}\ \phi(s,a)^\top \Sigma^{-1}\phi(s,a) \;=\; d.
\]
Furthermore, no distirbution $\rho$ can achive a lower (worst-case)
leverage score.
\end{block}
\end{frame}

\begin{frame}{Geometric intuition }

\textbf{Leverage control:} the quantity $\phi^\top\Sigma^{-1}\phi$ is exactly the (population) leverage

\vspace{0.5cm}

Equivalent viewpoints (pick your favorite story):
\begin{itemize}
  \item \textbf{Kiefer--Wolfowitz:} $\rho$ maximizes $\log\det\!\Big(\E_\rho[\phi\phi^\top]\Big)$
  \item \textbf{John's ellipsoid:} the ellipsoid
  \[
  \mathcal{E}=\{v:\ v^\top \Sigma^{-1}v \le d\}
  \]
  is the minimum-volume centered ellipsoid containing $\Phi$
  %\item \textbf{Leverage control:} the quantity $\phi^\top\Sigma^{-1}\phi$ is exactly the (population) leverage
\end{itemize}
Message: there is always a way to sample from only $O(d^2)$ points while keeping worst-case leverage $\le d$.
\end{frame}

\begin{frame}{From Global to Pointwise Error}
  Sample $N$ points from the D-optimal design $\rho$. Then $\Lambda =
  \frac{1}{N}\sum \phi \phi^\top$, and 
  our empirical cov is $\Lambda \approx \Sigma$.

    \vspace{0.3cm}

    \pause
    \textbf{1. The Leverage Score Bound (Geometry):}
    Since $\Lambda \approx \Sigma$, D-optimal design guarantees:
    \[
    \sup_{\phi \in \Phi} \phi^\top \Lambda^{-1} \phi \;\approx\; \sup_{\phi \in \Phi} \phi^\top \Sigma^{-1} \phi \;\le\; d
    \]

    \vspace{0.1cm}

    \pause
    \textbf{2. The OLS Bound (Statistics):}
    From the first slide, we know $\|\widehat\theta - \theta^\star\|_\Lambda \lesssim \sigma \sqrt{\frac{d}{N}}$.

    \vspace{0.3cm}

    \pause
    \textbf{3. Resulting Pointwise Guarantee:}
Use $|\textrm{pointwise error}| \le \|\widehat\theta - \theta^\star\cdot\|_\Lambda \sqrt{\text{Leverage}}$:
    \[
    \sup_{(s,a)} | \widehat{Q}_h(s,a) - \Tcal_h \widehat{Q}_{h+1}(s,a) | \;\lesssim\; \left( \sigma \sqrt{\frac{d}{N}} \right) \cdot \sqrt{d} \;=\; \frac{\sigma d}{\sqrt{N}}
    \]
    
    This allows us to control the \emph{max-norm} Bellman residual!
\end{frame}



% =============================================================================
% Part VI: Putting it Together (The Rough Sketch)
% =============================================================================

\section{Main Result}

\begin{frame}{Returning to LSVI}
    We now have all the pieces to analyze LSVI.

    \vspace{0.3cm}

    \textbf{1. The Data Collection (Generative Model)}
    \begin{itemize}
        \item For each stage $h$, we don't just sample randomly.
        \item We compute the D-optimal design $\rho^\star$ on $\Phi$.
        \item We query the simulator $N$ times distributed according to $\rho^\star$.
    \end{itemize}

    \vspace{0.3cm}

    \textbf{2. The Rough Sketch of the Proof}
    \begin{center}
    \small
    Regression Error $\xrightarrow{\text{D-Opt}}$ Pointwise Error $\xrightarrow{\text{Completeness}}$
    
    \vspace{0.2cm}
    Bellman Residual $\xrightarrow{\text{Sim. Lemma}}$ Policy Loss
    \end{center}
\end{frame}

\begin{frame}{The Main Theorem (Informal)}
  \textbf{Theorem (LSVI with Generative Model)} 

    Assume Linear Bellman Completeness. If we set:
    \[
    N \approx \frac{H^6 d^2}{\epsilon^2}
    \]
    and collect data using D-optimal design, then LSVI returns a policy $\widehat{\pi}$ such that with high probability:
    \[
    V^\star(s_0) - V^{\widehat{\pi}}(s_0) \le \epsilon
    \]

    \pause
    \vspace{0.5cm}
    \textbf{Takeaway:}
    \alert{We have achieved sample complexity polynomial in $d$ and $H$, independent of $|\Scal|$!}
    \begin{itemize}
        \item \textbf{Completeness} ensures realizability.
        \item \textbf{D-Optimal Design} ensures uniform error control.
    \end{itemize}
  \end{frame}

\section{Conclusion}

\begin{frame}{Summary \& Looking Ahead}
    \textbf{Today:} Scaling RL to large state spaces (using features)
    \begin{itemize}
    \item \textbf{The Algorithm:} Dynamic Programming as a sequence of regression problems (LSVI)
    \item \textbf{The Assumption Ladder:} consider different natural
      structural assumptions
     \item \textbf{Sampling:} use \textbf{D-Optimal Design} to control the uniform ($\ell_\infty$) error 
     \item \textbf{Main Result:} \textbf{Linear BC} +
       \textbf{D-Optimal Design} is sufficient for $\poly(d,H)$ sample complexity.
    \end{itemize}

    \vfill

    \textbf{Next Time (Lecture 2):}
    \begin{itemize}
        \item \textbf{Rigorous Analysis:} 
        \item \textbf{Offline RL:} adapt LSVI when we cannot choose our sampling distribution (Coverage Assumptions).
    \end{itemize}
\end{frame}
  

\end{document}

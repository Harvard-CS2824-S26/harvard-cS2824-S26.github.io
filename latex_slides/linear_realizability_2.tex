\documentclass[aspectratio=169]{beamer}

% The 'handout' option collapses all overlays into single slides
%\documentclass[aspectratio=169, handout]{beamer}

\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{algorithm, algorithmic}

% --- Standard Beamer Theme ---
\usetheme{Madrid}
\usecolortheme{default}


\include{macros}

\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc}


\title{RL with Linear Features: When Does It Work \\ \& When Doesn't It Work?}
\subtitle{Part 2: The Offline RL Case\\
CS 2284: Foundations of Reinforcement Learning}
\author{Kiant\'{e} Brantley \& Sham Kakade}
\date{}

\begin{document}

\begin{frame}[plain]
    \titlepage
\end{frame}
\addtocounter{framenumber}{-1}


\section{Today}

\begin{frame}{Agenda}

    \textbf{Announcements}
    \begin{itemize}
        \item Second reading assignment is out.
    \end{itemize}

    \vfill

    \textbf{Recap++}
    \begin{itemize}
        \item Wrap up our tabular sample complexity analysis (Chapter 2).
        \item \textbf{Minimax Result:} We established the fundamental limits of tabular learning.
    \end{itemize}

    \vfill

    \textbf{Today}
    \begin{itemize}
        \item Function Approximation. \textbf{We'll move beyond tabular RL!}
    \end{itemize}

\end{frame}

% --- Lecture 2 Title Slide ---
\begin{frame}
    \centering
    \LARGE \textbf{Lecture 2} \\
    \vspace{0.5em}
    \large Analysis of LSVI \& The Simulation Lemma
\end{frame}

\section{Analysis}

\begin{frame}{Roadmap: Proving LSVI Works}
    Recall our strategy:
    \[
      \text{Regression Error} \xrightarrow{\text{D-Opt}} \text{Pointwise Error} \xrightarrow{\text{Completeness}} \text{Bellman Residual}
      \xrightarrow{\text{Sim. Lemma}} \text{Policy Loss}
    \]

    \vspace{0.5cm}
    
    We will formalize this in three steps:
    \begin{enumerate}
        \item \textbf{Regression Analysis:} Bound the error $\|\widehat{\theta}_h - \theta_h^\star\|$ using OLS and D-Optimal Design.
        \item \textbf{Bellman Residuals:} Translate parameter error into uniform value function error $\text{Res}_h$.
        \item \textbf{The Simulation Lemma:} Show how residuals propagate to the final policy performance $V^\star - V^{\widehat{\pi}}$.
    \end{enumerate}
\end{frame}

% =============================================================================
% Step 1: Fixed Design Regression
% =============================================================================


\begin{frame}{Step 1: The Regression at Stage $h$}
    Consider a fixed stage $h$. We have the "next" value function $\widehat{V}_{h+1}$.
    
    \textbf{The Data:}
    \begin{itemize}
        \item Dataset $D_h = \{(s_i, a_i, r_i, s_i')\}_{i=1}^N$ collected via D-optimal design $\rho$.
        \item Design Matrix $\Lambda_h = \sum_{i=1}^N \phi(s_i, a_i) \phi(s_i, a_i)^\top$.
    \end{itemize}

    \textbf{The Target:}
    \[
    y_i = r_h(s_i, a_i) + \widehat{V}_{h+1}(s_i')
    \]
    By \textbf{Completeness}, the true expected target is linear:
    \[
    \E[y_i | s_i, a_i] = (\Tcal_h \widehat{Q}_{h+1})(s_i, a_i) = (\theta_h^\star)^\top \phi(s_i, a_i)
    \]
    Thus, $y_i = (\theta_h^\star)^\top \phi(s_i, a_i) + \xi_i$, where $\xi_i$ is sub-Gaussian noise.
\end{frame}

\begin{frame}{Step 1: The Fixed Design Bound}
    We apply the standard Fixed-Design OLS bound.

    \begin{block}{OLS Generalization Bound}
    With probability $1-\delta'$,
    \[
    \|\widehat{\theta}_h - \theta_h^\star\|_{\Lambda_h} \;\le\; c \cdot H \sqrt{d \log(H/\delta)}
    \]
    (Here the noise scale is $H$ because values are bounded by $H$).
    \end{block}

    \vspace{0.3cm}
    \textbf{Applying D-Optimal Design:}
    Since we sampled $N$ times from $\rho$, we have $\Lambda_h \succeq N \Sigma_\rho$.
    \[
    \implies \Lambda_h^{-1} \preceq \frac{1}{N} \Sigma_\rho^{-1}
    \]
\end{frame}

\begin{frame}{Step 1: From Parameters to Pointwise Error}
    We want to bound the prediction error at \emph{any} $(s,a)$.
    
    \[
    | (\widehat{\theta}_h - \theta_h^\star)^\top \phi(s,a) | 
    \;\le\; \|\widehat{\theta}_h - \theta_h^\star\|_{\Lambda_h} \sqrt{\phi(s,a)^\top \Lambda_h^{-1} \phi(s,a)}
    \]

    \vspace{0.2cm}
    Plug in our bounds:
    \begin{itemize}
        \item Parameter Error: $\approx H \sqrt{d}$
        \item Leverage Score (D-Opt): $\sqrt{\phi^\top \Sigma_\rho^{-1} \phi / N} \le \sqrt{d/N}$
    \end{itemize}

    \vspace{0.2cm}
    \textbf{Result (Uniform Error):}
    \[
    \sup_{(s,a)} | \widehat{Q}_h(s,a) - (\Tcal_h \widehat{Q}_{h+1})(s,a) | \;\lesssim\; H d \sqrt{\frac{1}{N}}
    \]
\end{frame}

% =============================================================================
% Step 2: The Simulation Lemma
% =============================================================================


\begin{frame}{Step 2: The Simulation Lemma}
    \textbf{Definitions:}
    \begin{itemize}
        \item Let $\widehat{V}_h(s) := \max_{a} \widehat{Q}_h(s,a)$ and $\widehat{\pi}_h(s) := \argmax_a \widehat{Q}_h(s,a)$.
        \item Define the stage-$h$ \textbf{Bellman Residual} as the prediction error:
        \[
        \mathrm{Res}_h \;:=\; \big\|\widehat Q_h - \Tcal_h \widehat Q_{h+1}\big\|_\infty
        \]
    \end{itemize}

    \begin{block}{Lemma 3.3 (Small residual $\Rightarrow$ good policy)}
    Assume $\widehat Q_H\equiv 0$ and $\mathrm{Res}_h \le \eta$ for all $h$. Then:
    \begin{enumerate}
        \item \textbf{Value Accuracy:} For all $h \in [H]$,
        \[
        \|\widehat Q_h - Q_h^\star\|_\infty \;\le\; (H-h)\eta.
        \]
        \item \textbf{Policy Loss:} For the greedy policy $\widehat\pi$,
        \[
        V_0^\star(s) - V^{\widehat\pi}_0(s) \;\le\; 2H^2\eta.
        \]
    \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}{Proof of Claim 1: Value Accuracy}
    We prove $\|\widehat Q_h - Q_h^\star\|_\infty \le (H-h)\eta$ by backward induction.
    
    \vspace{0.3cm}
    
    \textbf{Base Case ($h=H$):} $Q_H^\star = \widehat{Q}_H = 0$, so error is 0.
    
    \textbf{Inductive Step:}
    Consider the error at stage $h$:
    \begin{align*}
    \|\widehat{Q}_h - Q_h^\star\|_\infty 
    &= \|\widehat{Q}_h - \Tcal_h Q_{h+1}^\star\|_\infty \\
    &\le \underbrace{\|\widehat{Q}_h - \Tcal_h \widehat{Q}_{h+1}\|_\infty}_{\text{Residual } \le \eta} + \underbrace{\|\Tcal_h \widehat{Q}_{h+1} - \Tcal_h Q_{h+1}^\star\|_\infty}_{\text{Recursive Error}}
    \end{align*}
    
    Using the contraction property (non-expansion) of $\Tcal_h$:
    \[
    \|\Tcal_h \widehat{Q}_{h+1} - \Tcal_h Q_{h+1}^\star\|_\infty \le \|\widehat{Q}_{h+1} - Q_{h+1}^\star\|_\infty
    \]
    
    Thus, $\text{Error}_h \le \eta + \text{Error}_{h+1}$. Unrolling gives $\sum \eta = (H-h)\eta$. \qed
\end{frame}

\begin{frame}{Proof of Claim 2: Policy Loss}
    This follows the standard "Performance Difference" logic.
    
    \vspace{0.3cm}
    
    \textbf{The Argument:}
    \begin{itemize}
        \item The loss of a greedy policy is bounded by the sum of suboptimality gaps at each step.
        \item Since $\widehat{\pi}$ is greedy with respect to $\widehat{Q}$, the single-step loss is bounded by $2 \times$ estimation error:
        \[
        Q^\star_h(s, \pi^\star) - Q^\star_h(s, \widehat{\pi}) \;\le\; 2 \|\widehat{Q}_h - Q_h^\star\|_\infty
        \]
    \end{itemize}
    
    \textbf{Total Loss:}
    Summing over $H$ steps:
    \[
    V_0^\star - V_0^{\widehat{\pi}} \;\le\; \sum_{h=0}^{H-1} 2 \underbrace{\|\widehat{Q}_h - Q_h^\star\|_\infty}_{\le (H-h)\eta} \;\le\; \sum_{h=0}^{H-1} 2(H-h)\eta \approx H^2 \eta
    \]
\end{frame}


\begin{frame}{Final Result: LSVI Sample Complexity}
    \textbf{Putting it all together:}

    \begin{enumerate}
        \item We want final error $V^\star - V^{\widehat{\pi}} \le \epsilon$.
        \item By Simulation Lemma, we need $\text{Res}_h \le \epsilon / (2H^2)$.
        \item By Regression Analysis, we need $H d / \sqrt{N} \approx \epsilon / H^2$.
    \end{enumerate}

    \vspace{0.3cm}
    
    \textbf{Solve for $N$:}
    \[
    \frac{H d}{\sqrt{N}} \approx \frac{\epsilon}{H^2} \implies \sqrt{N} \approx \frac{H^3 d}{\epsilon} \implies N \approx \frac{H^6 d^2}{\epsilon^2}
    \]

    \begin{block}{Theorem (LSVI Generative)}
    LSVI with D-optimal design yields an $\epsilon$-optimal policy with $\tilde{O}(H^6 d^2 / \epsilon^2)$ samples.
    \end{block}
\end{frame}

% =============================================================================
% Part IV: Extension to Offline RL
% =============================================================================

\section{Offline Reinforcement Learning}

\begin{frame}{Switching to Offline RL}
    \textbf{The Setting:}
    \begin{itemize}
        \item We can no longer query the simulator.
        \item We are given static datasets $D_0, \dots, D_{H-1}$.
        \item \textbf{Key Question:} When does LSVI still work?
    \end{itemize}

    \vspace{0.3cm}

    \textbf{The Challenge:}
    \begin{itemize}
        \item In Generative Mode, we used \emph{D-Optimal Design} to ensure:
        \[
        \Lambda_h \succeq N \Sigma_{\rho^\star} \implies \text{Good Coverage Everywhere}
        \]
        \item In Offline RL, we are stuck with the behavior policy's distribution.
    \end{itemize}
\end{frame}

\begin{frame}{The Coverage Assumption}
    To guarantee success, the offline data must "cover" the feature space at least as well as the optimal design (up to a constant).

    \begin{block}{Assumption: Uniform Coverage}
    There exists a constant $\kappa \ge 1$ such that for all $h$, the empirical covariance $\widehat{\Sigma}_h$ satisfies:
    \[
    \widehat{\Sigma}_h \;\succeq\; \frac{1}{\kappa} \Sigma_{\rho^\star}
    \]
    where $\Sigma_{\rho^\star}$ is the covariance of the D-optimal design.
    \end{block}

    \vspace{0.3cm}
    \textbf{Interpretation:}
    \begin{itemize}
        \item $\kappa$ is the "relative condition number."
        \item If $\kappa=1$, our data is perfect (D-optimal).
        \item If $\kappa$ is huge, we have missing directions (poor coverage).
    \end{itemize}
\end{frame}

\begin{frame}{Analysis of Offline LSVI}
    The analysis remains almost identical! We just swap the variance bound.

    \vspace{0.3cm}

    \textbf{Generative (D-Optimal):}
    \[
    \phi^\top \Lambda^{-1} \phi \;\le\; \frac{d}{N}
    \]

    \textbf{Offline (Coverage $\kappa$):}
    \[
    \Lambda \approx N \widehat{\Sigma} \succeq \frac{N}{\kappa} \Sigma_{\rho^\star} \implies \Lambda^{-1} \preceq \frac{\kappa}{N} \Sigma_{\rho^\star}^{-1}
    \]
    \[
    \implies \phi^\top \Lambda^{-1} \phi \;\le\; \frac{\kappa d}{N}
    \]

    \vspace{0.3cm}
    \textbf{Result:}
    Sample complexity scales linearly with coverage $\kappa$:
    \[
    N \approx \frac{\kappa H^6 d^2}{\epsilon^2}
    \]
\end{frame}

% =============================================================================
% Part V: Policy Evaluation (LSPE)
% =============================================================================

\section{Policy Evaluation (LSPE)}

\begin{frame}{Task 2: Policy Evaluation}
    Sometimes we don't want to find $\pi^\star$, but just evaluate a fixed policy $\pi$.

    \vspace{0.3cm}

    \textbf{Algorithm: Least-Squares Policy Evaluation (LSPE)}
    \begin{itemize}
        \item Same backward regression structure.
        \item \textbf{Target Change:} Instead of $\max_{a'} \widehat{Q}_{h+1}(s', a')$, we use the value of our specific policy:
        \[
        y_i = r_i + \widehat{Q}_{h+1}(s_i', \pi(s_i'))
        \]
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Weaker Assumption:}
    \begin{itemize}
        \item We don't need closure under $\Tcal$ (Optimality).
        \item We only need closure under $\Tcal^\pi$ (Policy Operator).
    \end{itemize}
\end{frame}

\begin{frame}{LSPE Analysis: Why it's better}
    Evaluating a fixed policy is easier than finding the optimal one.

    \vspace{0.3cm}

    \textbf{1. No "Greedy" Error Propagation}
    \begin{itemize}
        \item In LSVI, we incur error from the Bellman residual AND the greedy step.
        \item In LSPE, we only track the estimation error of a fixed operator.
    \end{itemize}

    \textbf{2. Tighter Simulation Lemma}
    \begin{itemize}
        \item \textbf{Control (LSVI):} $V^\star - V^{\widehat{\pi}} \le 2 H^2 \eta$.
        \item \textbf{Evaluation (LSPE):} $|V^\pi - \widehat{V}^\pi| \le H \eta$.
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Result:}
    Better dependence on horizon $H$ (typically $H^4$ instead of $H^6$).
\end{frame}


% =============================================================================
% Part V: The Danger of Realizability (Split Universe Analysis)
% =============================================================================

\section{The Split Universe: Completeness vs. Realizability}

\begin{frame}{1. The Critical Decomposition}
    To analyze error propagation, we introduce the \textbf{Infinite-Sample Target}.
    
    \vspace{0.2cm}
    
    \textbf{Definition ($f^\star_h$):} The function LSVI would learn with infinite data.
    \[
    f^\star_h \;:=\; \argmin_{f \in \Fcal} \E_{\rho} \left[ (f(s,a) - \Tcal_h \widehat{Q}_{h+1}(s,a))^2 \right] \;=\; \Pi_{\Fcal, \rho} ( \Tcal_h \widehat{Q}_{h+1} )
    \]
    \emph{(Ideally, we want $f^\star_h$ to track the optimal value $Q^\star_h$)}.
    
    \vspace{0.3cm}
    
    \textbf{The Error Triangle:}
    We split the total error into Statistical Variance and Recursive Stability.
    \[
    \|\widehat{Q}_h - Q^\star_h\|_\infty \le \underbrace{\|\widehat{Q}_h - f^\star_h\|_\infty}_{\text{Statistical Error}} + \underbrace{\|f^\star_h - Q^\star_h\|_\infty}_{\text{Recursive Stability}}
    \]
    \begin{itemize}
        \item \textbf{Statistical Error:} Controlled by $N$ and D-Optimal Design ($\approx \sqrt{d/N}$).
        \item \textbf{Recursive Stability:} This is where the universe splits.
    \end{itemize}
\end{frame}



\begin{frame}{2. The Split Universe}
    How does the Recursive Error $\|f^\star_h - Q^\star_h\|_\infty$ behave?\\
    Recall $f^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h \widehat{Q}_{h+1})$
    and $Q^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h Q^\star_{h+1})$ (by realizability).
    
    \vspace{0.2cm}
    
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Universe A: Completeness}
        \textbf{Assumption:} $\Tcal_h$ preserves linearity.
        
        \vspace{0.2cm}
         Since $\widehat{Q}_{h+1} \in \Fcal$, we have:
        $f^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h \widehat{Q}_{h+1}) =
        \Tcal_h \widehat{Q}_{h+1}$, and thus:

        \vspace*{-0.3cm}
        \begin{align*}
        \|f^\star_h - Q^\star_h\|_\infty &=\|\Tcal_h \widehat{Q}_{h+1}
        - \Tcal_h Q_{h+1}^\star \|_\infty\\
          &\leq \| \widehat{Q}_{h+1} - Q_{h+1}^\star \|_\infty
        \end{align*}
        \textbf{Result:} Error is stable (contraction).
        \end{block}

        \column{0.48\textwidth}
        \begin{block}{Universe B: Realizability Only}
        \textbf{Assumption:} Only $Q^\star \in \Fcal$.
        
        \vspace{0.2cm}
        The Bellman backup $\Tcal_h \widehat{Q}_{h+1}$ may be \textbf{non-linear} (off-manifold).
        
        \vspace{0.2cm}
        We must project it back to $\Fcal$:
        \[
        f^\star_h = \Pi_{\Fcal,\rho} (\text{Non-Linear Target})
        \]
        \textbf{Result:} We pay for the stability of the projection operator $\Pi_{\Fcal,\rho}$.
        \end{block}
    \end{columns}
\end{frame}



\begin{frame}{3. The Amplification Mechanism (Universe B)}
    Without Completeness, we must bound the stability of the projection $\Pi_{\Fcal,\rho}$.
    Let $\Delta = \Tcal_h \widehat{Q}_{h+1} - \Tcal_h Q^\star_{h+1}$.
    
    \vspace{0.2cm}
    
    \textbf{The Chain of Inequalities:}
    \begin{align*}
        \|f^\star_h - Q^\star_h\|_\infty &= \|\Pi_{\Fcal,\rho} \Delta\|_\infty \\
        &\le \sqrt{d} \cdot \|\Pi_{\Fcal,\rho} \Delta\|_{L_2(\rho)} \quad \text{(\textbf{Step 1:} Norm Equivalence)} \\
        &\le \sqrt{d} \cdot \|\Delta\|_{L_2(\rho)} \quad \quad \quad \text{(\textbf{Step 2:} $L_2$ Stability of LS)} \\
        &\le \sqrt{d} \cdot \|\Delta\|_\infty \quad \quad \quad \quad \quad \text{(\textbf{Step 3:} Norm Monotonicity)} \\
        &\le \sqrt{d} \cdot \|\widehat{Q}_{h+1} - Q^\star_{h+1}\|_\infty
    \end{align*}

    \vspace{0.1cm}
    
    \textbf{The Verdict:}
    The "price" of converting the $L_2$ guarantee (regression) to the $L_\infty$ guarantee (DP) is exactly $\sqrt{d}$.
    
    \centering
    \alert{Total Amplification over $H$ steps $\approx (\sqrt{d})^H$.}
\end{frame}

% =============================================================================
% Part VI: Conclusion / Tease
% =============================================================================

\section{Conclusion}

\begin{frame}{Summary \& What's Next}
    \textbf{Summary of Lecture 2:}
    \begin{itemize}
        \item \textbf{Generative Model:} LSVI + D-Optimal Design achieves $\poly(d, H)$ sample complexity.
        \item \textbf{Offline RL:} The same algorithm works if we assume coverage ($\kappa$).
        \item \textbf{The Key Ingredient:} \textbf{Linear Bellman Completeness} ensures the regression targets remain realizable, preventing bias propagation.
    \end{itemize}

    \vspace{0.5cm}

    \textbf{Next Lecture (The Negative Results):}
    \begin{itemize}
        \item What if we \emph{don't} have Completeness?
        \item What if we only know $Q^\star$ is linear?
        \item We will show that without Completeness, sample complexity can become \textbf{Exponential in $H$ or $d$}.
    \end{itemize}
\end{frame}

\end{document}

\documentclass[aspectratio=169]{beamer}

% The 'handout' option collapses all overlays into single slides
%\documentclass[aspectratio=169, handout]{beamer}

\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{algorithm, algorithmic}

% --- Standard Beamer Theme ---
\usetheme{Madrid}
\usecolortheme{default}


\include{macros}

\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc}


\title{RL with Linear Features: When Does It Work \\ \& When Doesn't It Work?}
\subtitle{Part 2: Linear BC \& The Offline RL Case\\
CS 2284: Foundations of Reinforcement Learning}
\author{Kiant\'{e} Brantley \& Sham Kakade}
\date{}

\begin{document}

\begin{frame}[plain]
    \titlepage
\end{frame}
\addtocounter{framenumber}{-1}


\section{Today}

\begin{frame}{Agenda}

    \textbf{Announcements}
    \begin{itemize}
        \item HW1 due Monday.
    \end{itemize}

    \vfill

    \textbf{Recap}
    \begin{itemize}
        \item Regression + D-opt design.
    \end{itemize}

    \vfill

    \textbf{Today}
    \begin{itemize}
    \item Finish linear BC analysis.
    \item Offline RL
    \end{itemize}

\end{frame}

% --- Transition slide: Recap (beamer) ---
% Assumes your document is \documentclass{beamer} and your theme/fonts already set.

\section{Recap}

\begin{frame}[plain]
  \vfill
  \begin{center}
    {\Huge \bfseries Recap}
  \end{center}
  \vfill
\end{frame}
\addtocounter{framenumber}{-1}


\begin{frame}{Least-Squares Value Iteration (LSVI)}
    \textbf{Setting:} Finite Horizon $H$, Generative Model (Simulator).

    \vspace{0.3cm}

    \textbf{Algorithm:} Backward Induction via Regression.
    \begin{enumerate}
        \item Initialize $\widehat{V}_H(s) = 0$.
        \item For $h = H-1, \dots, 0$:
        \begin{itemize}
            \item \textbf{Collect Data:} Generate dataset $D_h = \{(s_i, a_i, r_i, s_i')\}_{i=1}^N$.

            \item \textbf{Form Targets:} Compute regression targets using the \emph{next} value function:
            \[
            y_i = r_h(s_i, a_i) + \widehat{V}_{h+1}(s_i')
            \]

            \item \textbf{Regression:} Solve for parameters $\widehat{\theta}_h$:
            \[
            \widehat{\theta}_h \in \argmin_{\theta \in \R^d} \sum_{i=1}^N \left( \theta^\top \phi(s_i, a_i) - y_i \right)^2
            \]

            \item \textbf{Update:} Set $\widehat{Q}_h(s,a) = \widehat{\theta}_h^\top \phi(s,a)$ and $\widehat{V}_h(s) = \max_{a} \widehat{Q}_h(s,a)$.
        \end{itemize}
    \end{enumerate}
\end{frame}




\begin{frame}{The Assumption Ladder}
  We can organize linear RL assumptions from weakest (hardest) to strongest (easiest).
  
  \vspace{0.3cm}
  
  \begin{itemize}
  \item[(A)] \textbf{Agnostic Approximation} \\
    \footnotesize No realizability. $Q^\star$ is ``close'' to linear. \\
    \textit{Status: Hard. Requires strong distribution assumptions.}
    

    \vspace{0.2cm}
  \item[(B)] \textbf{Linear $Q^\star$ Realizability} \\
    \footnotesize $Q^\star_h(s,a) = (\theta^\star_h)^\top \phi(s,a)$. \\
    \textit{Status: \textbf{Insufficient.} Exponential lower bounds exist.}
    

    \vspace{0.2cm}
  \item[(C)] \textbf{All-Policies Realizability} \\
    \footnotesize $Q^\pi_h(s,a) = (\theta^\pi_h)^\top \phi(s,a)$ for \textbf{all} $\pi$. \\
    \textit{Status: \textbf{Subtle.} Fails offline even with perfect coverage.}
    

    \vspace{0.2cm}
  \item[(D)] \textbf{Linear Bellman Completeness} \\
    \footnotesize $\Tcal_h f \in \Fcal$ for all $f \in \Fcal$. \\
    \textit{Status: \textbf{Sufficient!} (This Lecture)}
  \end{itemize}
\end{frame}
%\addtocounter{framenumber}{-1}


\begin{frame}{Fixed Design OLS (The Tool)}
    Consider the standard linear regression setting:
    \[
    y_i = x_i^\top \theta^\star + \xi_i, \quad \text{with } \E[\xi_i|x_i]=0 \text{ (sub-Gaussian)}.
    \]
    
    The OLS estimator is $\widehat{\theta} = \Lambda^{-1}\left(\frac{1}{N} \sum_{i=1}^N x_i y_i\right)$, where $\Lambda = \frac{1}{N}\sum_i x_i x_i^\top$.
    
    \begin{block}{Fixed-design OLS Bound}
    With probability at least $1-\delta$, the prediction error is bounded in the $\Lambda$-norm:
    \[
    \|\widehat\theta - \theta^\star\|_{\Lambda}
    \;\lesssim\;
    \sigma\,\sqrt{\frac{d\log(1/\delta)}{N} }.
    \]
    \end{block}
    
    \vspace{0.2cm}
    
    This translates to a \textit{pointwise} bound using leverage scores:
    \[
    |(\widehat\theta-\theta^\star)^\top\phi(s,a)|
    \;\le\;
    \|\widehat\theta - \theta^\star\|_{\Lambda} \sqrt{\phi(s,a)^\top \Lambda^{-1} \phi(s,a)}
    \]
\end{frame}

\begin{frame}{OLS review}

  \vspace*{-3.4cm}
  Consider the standard linear regression setting:
    \[
    y_i = x_i^\top \theta^\star + \xi_i, \quad \text{with } \E[\xi_i|x_i]=0 \text{ (sub-Gaussian)}.
    \]
        The OLS estimator is $\widehat{\theta} = \Lambda^{-1}\left(\frac{1}{N} \sum_{i=1}^N x_i y_i\right)$, where $\Lambda = \frac{1}{N}\sum_i x_i x_i^\top$.
    
\end{frame}

\begin{frame}{D-optimal design: the leverage-minimizing geometry}
To guarantee uniform bounds, we must choose our training data carefully.

\vspace*{0.1cm}  
The feature set is:
\[
\Phi := \{\phi(s,a):(s,a)\in\Scal\times\Acal\}\subset\R^d.
\]

\begin{block}{D-optimal design (lemma; geometric fact)}
Suppose $\Phi$ is compact. There exists a distribution $\rho$ supported on at most $d(d+1)/2$ state--action pairs s.t.
with
\[
\Sigma := \E_{(s,a)\sim\rho}\big[\phi(s,a)\phi(s,a)^\top\big],
\]
we have $\Sigma\succ 0$ and
\[
\sup_{(s,a)}\ \phi(s,a)^\top \Sigma^{-1}\phi(s,a) \;=\; d.
\]
Furthermore, no distribution $\rho$ can achieve a lower (worst-case)
leverage score.
\end{block}
\end{frame}



\begin{frame}[plain]
  \vfill
  \begin{center}
    {\Huge \bfseries Today: Analysis of LSVI}
  \end{center}
  \vfill
\end{frame}
\addtocounter{framenumber}{-1}


\section{Analysis}

\begin{frame}{Roadmap: Proving LSVI Works}
    Recall our strategy:
    \[
      \text{Regression Error} \xrightarrow{\text{D-Opt}} \text{Pointwise Error} \xrightarrow{\text{Completeness}} \text{Bellman Residual}
      \xrightarrow{\text{Sim. Lemma}} \text{Policy Loss}
    \]

    \vspace{0.5cm}
    
    We will formalize this in three steps:
    \begin{enumerate}
        \item \textbf{Regression Analysis:} Bound the error $\|\widehat{\theta}_h - \theta_h^\star\|$ using OLS.
        \item \textbf{Bellman Residuals:} D-Optimal Design lets us translate parameter err into uniform value function error $\text{Res}_h$.
        \item \textbf{The Simulation Lemma:} Show how residuals propagate to the final value/policy.
    \end{enumerate}
\end{frame}



\begin{frame}{Step 1: The Regression at Stage $h$}
    Consider a fixed stage $h$. We have the "next" value function $\widehat{V}_{h+1}$.
    
    \textbf{The Data:}
    \begin{itemize}
        \item Dataset $D_h = \{(s_i, a_i, r_i, s_i')\}_{i=1}^N$
          collected via D-optimal design $\rho$.
          \\
          %\footnotesize{(suppose $N$ is a multiple of $d(d+1)/2$)}.
          \pause
        \item Design Matrix $\Lambda_h = \frac{1}{N}\sum_{i=1}^N \phi(s_i, a_i)
          \phi(s_i, a_i)^\top \approx \Sigma_\rho$.
    \end{itemize}

    \pause
    \textbf{The Target:}
    \[
    y_i = r_h(s_i, a_i) + \widehat{V}_{h+1}(s_i')
    \]
    \pause
    By \textbf{Completeness}, the true expected target is linear:
    \[
    \E[y_i | s_i, a_i] = (\Tcal_h \widehat{Q}_{h+1})(s_i, a_i) = (\theta_h^\star)^\top \phi(s_i, a_i)
    \]
    Thus, $y_i = (\theta_h^\star)^\top \phi(s_i, a_i) + \xi_i$, where $\xi_i$ is sub-Gaussian noise.
\end{frame}

\begin{frame}{Step 1: The Fixed Design Bound}
    We apply the standard Fixed-Design OLS bound.

    \begin{block}{OLS Generalization Bound}
    With probability $1-\delta$, for all $h$
    \[
    \|\widehat{\theta}_h - \theta_h^\star\|_{\Lambda_h} \;\le\; c \cdot H \sqrt{\frac{d \log(H/\delta)}{N}}
    \]
    (Here the noise scale is $H$ because values are bounded by $H$).
    \end{block}

\end{frame}

\begin{frame}{Step 2: From Parameters to Pointwise Error}
    We want to bound the prediction error at \emph{any} $(s,a)$.
    
    \[
    | (\widehat{\theta}_h - \theta_h^\star)^\top \phi(s,a) | 
    \;\le\; \|\widehat{\theta}_h - \theta_h^\star\|_{\Lambda_h} \sqrt{\phi(s,a)^\top \Lambda_h^{-1} \phi(s,a)}
    \]

    \vspace{0.2cm}
    \pause
    Plug in our bounds (using $\Lambda_h \approx \Sigma_\rho$):
    \begin{itemize}
        \item Parameter Error (OLS): $\approx H \sqrt{\frac{d}{N}}$
        \item Leverage Score (D-Opt): $\sqrt{\phi^\top \Lambda_h^{-1} \phi } \approx \sqrt{\phi^\top \Sigma_\rho^{-1} \phi } \le \sqrt{d}$
    \end{itemize}

    \vspace{0.2cm}
    \pause
    \textbf{Result (Uniform Error):}
    Multiply them together:
    \[
    \sup_{(s,a)} | \widehat{Q}_h(s,a) - (\Tcal_h \widehat{Q}_{h+1})(s,a) | \;\lesssim\; H \sqrt{\frac{d}{N}} \cdot \sqrt{d} \;=\; \frac{H d}{\sqrt{N}}
    \]
\end{frame}


\begin{frame}{Step 3: The Simulation Lemma}
    \textbf{Definitions:}
    \begin{itemize}
        \item Let $\widehat{V}_h(s) := \max_{a} \widehat{Q}_h(s,a)$ and $\widehat{\pi}_h(s) := \argmax_a \widehat{Q}_h(s,a)$.
        \item Define the stage-$h$ \textbf{Bellman Residual} as the prediction error:
        \[
        \mathrm{Res}_h \;:=\; \big\|\widehat Q_h - \Tcal_h \widehat Q_{h+1}\big\|_\infty
        \]
    \end{itemize}

    \pause
    \begin{block}{Lemma 3.3 (Small residual $\Rightarrow$ good policy)}
    Assume $\widehat Q_H\equiv 0$ and $\mathrm{Res}_h \le \eta$ for all $h$. Then:
    \begin{enumerate}
        \item \textbf{Value Accuracy:} For all $h \in [H]$,
        \[
        \|\widehat Q_h - Q_h^\star\|_\infty \;\le\; (H-h)\eta.
      \]
      \pause
    \vspace*{-0.5cm}\item \textbf{Policy Loss:} For the greedy policy $\widehat\pi$,
        \[
        V_0^\star(s) - V^{\widehat\pi}_0(s) \;\le\; 2H^2\eta.
        \]
    \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}{Proof of Claim 1: Value Accuracy}
    We prove $\|\widehat Q_h - Q_h^\star\|_\infty \le (H-h)\eta$ by backward induction.
    
    \vspace{0.3cm}
    
    \textbf{Base Case ($h=H$):} $Q_H^\star = \widehat{Q}_H = 0$, so error is 0.
    
    \textbf{Inductive Step:}
    Consider the error at stage $h$:
    \pause
    \begin{align*}
    \|\widehat{Q}_h - Q_h^\star\|_\infty 
    &= \|\widehat{Q}_h - \Tcal_h Q_{h+1}^\star\|_\infty \\
    &\le \underbrace{\|\widehat{Q}_h - \Tcal_h \widehat{Q}_{h+1}\|_\infty}_{\text{Residual } \le \eta} + \underbrace{\|\Tcal_h \widehat{Q}_{h+1} - \Tcal_h Q_{h+1}^\star\|_\infty}_{\text{Recursive Error}}
    \end{align*}
    
    \pause
    Using the contraction property (non-expansion) of $\Tcal_h$:
    \[
    \|\Tcal_h \widehat{Q}_{h+1} - \Tcal_h Q_{h+1}^\star\|_\infty \le \|\widehat{Q}_{h+1} - Q_{h+1}^\star\|_\infty
    \]
    
    Thus, $\text{Error}_h \le \eta + \text{Error}_{h+1}$. Unrolling gives $\sum \eta = (H-h)\eta$. \qed
\end{frame}

\begin{frame}[plain,noframenumbering]{Proof of Claim 2: Policy Loss}
    This follows the standard "Performance Difference" logic.
    
    \vspace{0.3cm}
    
    \textbf{The Argument:}
    \begin{itemize}
        \item The loss of a greedy policy is bounded by the sum of suboptimality gaps at each step.
        \item Since $\widehat{\pi}$ is greedy with respect to $\widehat{Q}$, the single-step loss is bounded by $2 \times$ estimation error:
        \[
        Q^\star_h(s, \pi^\star) - Q^\star_h(s, \widehat{\pi}) \;\le\; 2 \|\widehat{Q}_h - Q_h^\star\|_\infty
        \]
    \end{itemize}
    
    \textbf{Total Loss:}
    Summing over $H$ steps:
    \[
    V_0^\star - V_0^{\widehat{\pi}} \;\le\; \sum_{h=0}^{H-1} 2 \underbrace{\|\widehat{Q}_h - Q_h^\star\|_\infty}_{\le (H-h)\eta} \;\le\; \sum_{h=0}^{H-1} 2(H-h)\eta \approx H^2 \eta
    \]
\end{frame}
\addtocounter{framenumber}{-1}

\begin{frame}{Final Result: LSVI Sample Complexity}
    \textbf{Putting it all together:}

    \begin{enumerate}
        \item We want final error $V^\star - V^{\widehat{\pi}} \le \epsilon$.
        \item By Simulation Lemma, we need $\text{Res}_h \le \epsilon / (2H^2)$.
        \item By Regression Analysis, we need $H d / \sqrt{N} \approx \epsilon / H^2$.
    \end{enumerate}

    \vspace{0.3cm}
    
    \pause
    \textbf{Solve for $N$:}
    \[
    \frac{H d}{\sqrt{N}} \approx \frac{\epsilon}{H^2} \implies \sqrt{N} \approx \frac{H^3 d}{\epsilon} \implies N \approx \frac{H^6 d^2}{\epsilon^2}
    \]

    \begin{block}{Theorem (LSVI Generative)}
    LSVI with D-optimal design yields an $\epsilon$-optimal policy with $\tilde{O}(H^6 d^2 / \epsilon^2)$ samples.
    \end{block}
\end{frame}

% =============================================================================
% Part IV: Extension to Offline RL
% =============================================================================

\section{Offline Reinforcement Learning}

\begin{frame}[plain]
  \vfill
  \begin{center}
    {\Huge \bfseries Offline RL Recap}
  \end{center}
  \vfill
\end{frame}
\addtocounter{framenumber}{-1}


\begin{frame}{Switching to Offline RL}
    \textbf{The Setting:}
    \begin{itemize}
        \item We can no longer query the simulator.
        \item We are given static datasets $D_0, \dots, D_{H-1}$.
        \item \textbf{Key Question:} When does LSVI still work?
    \end{itemize}

    \vspace{0.3cm}

    \textbf{The Challenge:}
    \begin{itemize}
        \item In Generative Mode, we used \emph{D-Optimal Design} to ensure:
        \[
        \Lambda_h \approx  \Sigma_{\rho^\star} \implies \text{Good Coverage Everywhere}
        \]
        \item In Offline RL, we are stuck with the behavior policy's distribution.
    \end{itemize}
\end{frame}

\begin{frame}{The Coverage Assumption}
    To guarantee success, the offline data must "cover" the feature space at least as well as the optimal design (up to a constant).

    \pause
    \begin{block}{Assumption: Uniform Coverage}
    There exists a constant $\kappa \ge 1$ such that for all $h$, the empirical covariance $\Lambda_h$ satisfies:
    \[
    \Lambda_h \;\succeq\; \frac{1}{\kappa} \Sigma_{\rho^\star}
    \]
    where $\Sigma_{\rho^\star}$ is the covariance of the D-optimal design.
    \end{block}

    \vspace{0.3cm}
    \pause
    \textbf{Interpretation:}
    \begin{itemize}
        \item $\kappa$ is the "relative condition number."
        \item If $\kappa=1$, our data is perfect (D-optimal).
        \item If $\kappa$ is huge, we have missing directions (poor coverage).
    \end{itemize}
\end{frame}

\begin{frame}{Analysis of Offline LSVI}
    The analysis remains almost identical! We just swap the leverage bound.

    \vspace{0.3cm}

\pause    \textbf{1. Generative (D-Optimal):}
    \[
    \phi^\top \Lambda^{-1} \phi \approx \phi^\top \Sigma_{\rho^\star}^{-1} \phi \;\le\; d
    \]

\pause    \textbf{2. Offline (Coverage $\kappa$):}
    \[
    \Lambda \succeq \frac{1}{\kappa} \Sigma_{\rho^\star} \implies \Lambda^{-1} \preceq \kappa \Sigma_{\rho^\star}^{-1}
    \]
    \[
    \implies \phi^\top \Lambda^{-1} \phi \;\le\; \kappa \left( \phi^\top \Sigma_{\rho^\star}^{-1} \phi \right) \;\le\; \kappa d
    \]

    \vspace{0.3cm}
\pause    \textbf{Result:}
    The error scales by $\sqrt{\kappa}$. Sample complexity scales linearly with $\kappa$:
    \[
    \text{Error} \approx \frac{H d \sqrt{\kappa}}{\sqrt{N}} \implies N \approx \frac{\kappa H^6 d^2}{\epsilon^2}
    \]
\end{frame}


% =============================================================================
% Part V: Policy Evaluation (LSPE)
% =============================================================================

\section{Policy Evaluation (LSPE)}

\begin{frame}{Task 2: Policy Evaluation}
    Sometimes we don't want to find $\pi^\star$, but just evaluate a fixed policy $\pi$.

    \vspace{0.3cm}

\pause    \textbf{Algorithm: Least-Squares Policy Evaluation (LSPE)}
    \begin{itemize}
        \item Same backward regression structure.
        \item \textbf{Target Change:} Instead of $\max_{a'} \widehat{Q}_{h+1}(s', a')$, we use the value of our specific policy:
        \[
        y_i = r_i + \widehat{Q}_{h+1}(s_i', \pi(s_i'))
        \]
    \end{itemize}

    \vspace{0.3cm}
\pause    \textbf{Weaker Assumption:}
    \begin{itemize}
        \item We don't need closure under $\Tcal$ (Optimality).
        \item We only need closure under $\Tcal^\pi$ (Policy Operator).
    \end{itemize}
\end{frame}

\begin{frame}{LSPE Analysis: Why it's better}
    Evaluating a fixed policy is easier than finding the optimal one.

    \vspace{0.3cm}

    \textbf{1. No "Greedy" Error Propagation}
    \begin{itemize}
        \item In LSVI, we incur error from the Bellman residual AND the greedy step.
        \item In LSPE, we only track the estimation error of a fixed operator.
    \end{itemize}

    \textbf{2. Tighter Simulation Lemma}
    \begin{itemize}
        \item \textbf{Control (LSVI):} $V^\star - V^{\widehat{\pi}} \le 2 H^2 \eta$.
        \item \textbf{Evaluation (LSPE):} $|V^\pi - \widehat{V}^\pi| \le H \eta$.
    \end{itemize}

    \vspace{0.3cm}
\pause    \textbf{Result:}
    Better dependence on horizon $H$ (typically $H^4$ instead of $H^6$).
\end{frame}


% =============================================================================
% Part V: The Danger of Realizability (Split Universe Analysis)
% =============================================================================

\section{The Split Universe: Completeness vs. Realizability}

\begin{frame}{1. The Critical Decomposition}
    To analyze error propagation, we introduce the \textbf{Infinite-Sample Target}.
    
    \vspace{0.2cm}
    
    \textbf{Definition ($f^\star_h$):} The function LSVI would learn with infinite data.
    \[
    f^\star_h \;:=\; \argmin_{f \in \Fcal} \E_{\rho} \left[ (f(s,a) - \Tcal_h \widehat{Q}_{h+1}(s,a))^2 \right] \;=\; \Pi_{\Fcal, \rho} ( \Tcal_h \widehat{Q}_{h+1} )
    \]
    \emph{(Ideally, we want $f^\star_h$ to track the optimal value $Q^\star_h$)}.
    
    \vspace{0.3cm}
    
\pause    \textbf{The Error Triangle:}
    We split the total error into Statistical Variance and Recursive Stability.
    \[
    \|\widehat{Q}_h - Q^\star_h\|_\infty \le \underbrace{\|\widehat{Q}_h - f^\star_h\|_\infty}_{\text{Statistical Error}} + \underbrace{\|f^\star_h - Q^\star_h\|_\infty}_{\text{Recursive Stability}}
    \]
\pause    \begin{itemize}
        \item \textbf{Statistical Error:} Controlled by $N$ and D-Optimal Design ($\approx \sqrt{d/N}$).
        \item \textbf{Recursive Stability:} This is where the universe splits.
    \end{itemize}
\end{frame}



\begin{frame}{2. The Split Universe}
    How does the Recursive Error $\|f^\star_h - Q^\star_h\|_\infty$ behave?\\
    Recall $f^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h \widehat{Q}_{h+1})$
    and $Q^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h Q^\star_{h+1})$ (by realizability).
    
    \vspace{0.2cm}
    
    \begin{columns}[t]
        \column{0.48\textwidth}
        \pause
        \begin{block}{Universe A: Completeness}
        \textbf{Assumption:} $\Tcal_h$ preserves linearity.
        
        \vspace{0.2cm}
         Since $\widehat{Q}_{h+1} \in \Fcal$, we have:
        $f^\star_h = \Pi_{\Fcal,\rho} (\Tcal_h \widehat{Q}_{h+1}) =
        \Tcal_h \widehat{Q}_{h+1}$, and thus:

        \vspace*{-0.3cm}
        \begin{align*}
        \|f^\star_h - Q^\star_h\|_\infty &=\|\Tcal_h \widehat{Q}_{h+1}
        - \Tcal_h Q_{h+1}^\star \|_\infty\\
          &\leq \| \widehat{Q}_{h+1} - Q_{h+1}^\star \|_\infty
        \end{align*}
        \textbf{Result:} Error is stable (contraction).
        \end{block}

        \column{0.48\textwidth}
        \pause
        \begin{block}{Universe B: Realizability Only}
        \textbf{Assumption:} Only $Q^\star \in \Fcal$.
        
        \vspace{0.2cm}
        The Bellman backup $\Tcal_h \widehat{Q}_{h+1}$ may be \textbf{non-linear} (off-manifold).
        
        \vspace{0.2cm}
        We must project it back to $\Fcal$:
        \[
        f^\star_h = \Pi_{\Fcal,\rho} (\text{Non-Linear Target})
        \]
        \textbf{Result:} We pay for the stability of the projection operator $\Pi_{\Fcal,\rho}$.
        \end{block}
    \end{columns}
\end{frame}



\begin{frame}{3. The Amplification Mechanism (Universe B)}
    Without Completeness, we must bound the stability of the projection $\Pi_{\Fcal,\rho}$.
    Let $\Delta = \Tcal_h \widehat{Q}_{h+1} - \Tcal_h Q^\star_{h+1}$.
    
    \vspace{0.2cm}
    
    \textbf{The Chain of Inequalities:}
    \begin{align*}
        \|f^\star_h - Q^\star_h\|_\infty &= \|\Pi_{\Fcal,\rho} \Delta\|_\infty \\
        &\le \sqrt{d} \cdot \|\Pi_{\Fcal,\rho} \Delta\|_{L_2(\rho)} \quad \text{(\textbf{Step 1:} Norm Equivalence)} \\
        &\le \sqrt{d} \cdot \|\Delta\|_{L_2(\rho)} \quad \quad \quad \text{(\textbf{Step 2:} $L_2$ Stability of LS)} \\
        &\le \sqrt{d} \cdot \|\Delta\|_\infty \quad \quad \quad \quad \quad \text{(\textbf{Step 3:} Norm Monotonicity)} \\
        &\le \sqrt{d} \cdot \|\widehat{Q}_{h+1} - Q^\star_{h+1}\|_\infty
    \end{align*}

    \vspace{0.1cm}
    
    \textbf{The Verdict:}
    The "price" of converting the $L_2$ guarantee (regression) to the $L_\infty$ guarantee (DP) is exactly $\sqrt{d}$.
    
    \centering
    \alert{Total Amplification over $H$ steps $\approx (\sqrt{d})^H$.}
\end{frame}

% =============================================================================
% Part VI: Conclusion / Tease
% =============================================================================

\section{Conclusion}

\begin{frame}{Summary \& What's Next}
    \textbf{Summary of Lecture 2:}
    \begin{itemize}
        \item \textbf{Generative Model:} LSVI + D-Optimal Design achieves $\poly(d, H)$ sample complexity.
        \item \textbf{Offline RL:} The same algorithm works if we assume coverage ($\kappa$).
        \item \textbf{The Key Ingredient:} \textbf{Linear Bellman Completeness} ensures the regression targets remain realizable, preventing bias propagation.
    \end{itemize}

    \vspace{0.5cm}

    \textbf{Next Lecture (The Negative Results):}
    \begin{itemize}
        \item What if we \emph{don't} have Completeness?
        \item What if we only know $Q^\star$ is linear?
        \item We will show that without Completeness, sample complexity can become \textbf{Exponential in $H$ or $d$}.
    \end{itemize}
\end{frame}

\end{document}

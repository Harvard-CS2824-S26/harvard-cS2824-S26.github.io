<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"> 
<html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-46766886-4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-46766886-4');
  </script>


<script type="text/javascript">
  function visibility_on(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'none')
           e.style.display = 'block';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'none')
           e.style.display = 'block';
  }
  function visibility_off(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'block')
           e.style.display = 'none';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'block')
           e.style.display = 'none';
  }
  function toggle_visibility(id) {
      var e = document.getElementById(id+"_text");
      if(e.style.display == 'inline')
         e.style.display = 'block';
      else
         e.style.display = 'inline';
      var e = document.getElementById(id+"_img");
      if(e.style.display == 'inline')
         e.style.display = 'block';
      else
         e.style.display = 'inline';
  }
  function toggle_vis(id) {
      var e = document.getElementById(id);
      if (e.style.display == 'none')
          e.style.display = 'inline';
      else
          e.style.display = 'none';
  }
</script>


  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>CS 2824 Foundations of RL</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>


<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>CS 2824: Foundations of Reinforcement Learning</name>
              </p>

	      
              <br>Modern Artificial Intelligent (AI) systems often need the ability to make sequential decisions in an unknown, 
              uncertain, possibly hostile environment, by actively interacting with the environment to collect relevant data. 
              Reinforcement Learning (RL) is a general framework that can capture the interactive learning setting and 
              has been used to design intelligent agents that achieve super-human level performances on 
              challenging tasks such as Go, computer games, and robotics manipulation.  </br>
              <br>This graduate level course focuses on theoretical and algorithmic foundations of Reinforcement Learning. The four main themes of the course are
              <strong>(1) fundamentals (MDPs, computation, statistics,
              generalization) (2) provably efficient exploration (and
              high dimensional RL) (3) direct policy optimization
              (e.g. policy gradient methods).</strong> </br>

              <br>After taking this course, students will be able to
            understand both classic and state-of-art provably correct
            RL algorithms and their analysis. Students will be able to
            conduct research on RL related topics. </br> 
            </td>
          </tr>
        </table>

        

    

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Staff</heading>
              </p>
              <p>
                <br><strong>Instructors</strong>: <a href="https://xkianteb.github.io">Kiant&eacute; Brantley</a> and <a href="https://shamulent.github.io">Sham Kakade</a> </br>
                <br><strong>TFs</strong>: <a
          href="https://lfesser97.github.io">Lukas Fesser</a>, <a
          href="https://jaeyeonkim01.github.io">Jaeyeon Kim</a>, and <a
          href="https://alexandrumeterez.github.io">Alex Meterez</a>. </br>
                <br><strong> Lecture time: </strong> Tuesday/Thursday 12:45-2p  </br>
	        <br><strong>Office hours: </strong> By Appointment</br>
                <br><strong>Location: </strong> SEC LL2.224</br>
                
                <br><strong>Contact:</strong> 
	      Please communicate to the instructors and TFs only
                through the Ed account. Emails not sent to this list, with regards to the course,
                will not be responded to in a timely manner.
	      </br>

	      	      <br>
	      <strong> Announcements:
	       </strong> Course announcements will be made via Canvas
	      and Edstem. It is the students' responsibility to follow both. </br>

              </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Prerequisites</heading>
              </p>
              <p> 
                <br>This is an <strong>advanced and theory-heavy course</strong>: there is no programming assignment and students 
                are required to work on a theory-focused course project. Students need a strong grasp on Machine Learning, Probability and Statistics, Optimization, and Linear Algebra. For undergraduate and masters students enrollment: permission of
          instructor required through course petition.</br>
              </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Grading Policies</heading>
              </p>
              <p> 
                <br> Assignments 60 Homework%, Project 30%, Reading 10%, (+Participation bonus 5%)</br>
                <br> All homework will be mathematical in nature, focussing on the theory of RL and bandits; 
                there will not be a programming component. 
                The entire HW must be submitted in one single typed pdf document (not handwritten). 
                HW0 is MANDATORY to pass to satisfactory level; 
                it is to check your knowledge of the prerequisites in probability, statistics, and linear algebra. </br>
	      <br><strong>Homework Rules: </strong>
	      Homework must be done individually: each student must understand, write, and hand in their own answers. It is
          acceptable for students to discuss problems with each other;  
                it is not acceptable for students to share answers and look at another students written answers. 
                You must also indicate on each homework with whom you
          collaborated with and what online resources you used. <strong>You
          must attempt and submit all HW (even if it is for 0 credit)
          in order to pass the class.</strong></br>
                <br><strong>Late days: </strong> Homeworks and Reading
          Assignments must be submitted by the posted due date. 
                  You are allowed up to 6 total LATE DAYs for the
          homeworks and reading assignments throughout the entire semester. These will be automatically deducted if your assignment is late. 
                  For example, any day in which an assignment is late by up to 24 hours, 
                  then one late day will be used. After your late days are used up, 
                  late penalties will be applied: any assignment turned in late will incur a reduction in score by 33% for each late day, 
                  so if an assignment is up to 24 hours late, it incurs a penalty of 33%. 
                  Else if it is up to 48 hours late, it incurs a penalty of 66%. 
                  And any longer, it will receive no credit. We will track all your late days and any deductions will be applied in computing the final grades. 
                  If you are unable to turn in HWs on time, aside from permitted days, then do not enroll in the course.</br>

                <br><strong>Regrading: </strong> If we made a mistake,
                you must let us know (in writing via Ed) within a week
                of when the HW was returned. </br>

                <br><Strong>Participation/extra effort
              bonus:</strong>  We encourage participation including
              asking/answering questions in lectures and ED
              discussion, and extra effort on reading the book
              chapters (e.g., proof reading additional chapters and
              sending back comments/feedback). </br> 
              </p>

            </td>
          </tr>
        </table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
	    <td width="100%" valign="middle">
	      <p align="center">
	      <heading>Reading Assignment</heading>
	      </p>
	      
	      <p>
	      Reading assignments are meant to be completed actively
	      and carefully. Student are responsible for reading
	      the assigned readings in <a
	  href="https://rltheorybook.github.io">"Reinforcement Learning Theory and
	  Algorithms"</a> (ABJKS pdf link <a
	  href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">here</a>) and
	      engaging with the text to support learning. Note that
	      LATE DAY POLICY also applies to the reading assignments.
	      The readings are intended to help you develop a strong,
	      working mastery of the material. 
	      </p>
	      
	      <p>
	      Students are encouraged to use ChatGPT (or another LLM tool) for all reading assignments. Harvard-enrolled students should have access to a university-sponsored account.
	      </p>
	      
	      
	    </td>
	  </tr>
	</table>



	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Course Project</heading>
              </p>
              <p>
                Please see the course projects from ideas from an
          older version of the course <a
	      href="CS2824projects.html">page</a>.
	      Students will do project presentations during the last
          three lectures of the course.
	      <strong> It is a course requirement that you be in attendance for
          all student presentations</strong>. See the dates
          below. Only the dates of 04/23/26 and 04/28/26 will be
          excused for ICLR, with instructor permission. 
              </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Diversity in STEM</heading>
              </p>
              <p> 
                While many academic disciplines have historically been dominated by one cross section of society, 
                the study of and participation in STEM disciplines is a joy that the instructors hope that everyone can pursue, 
                regardless of their socio-economic background, race, gender, etc. 
                The instructors encourage students to both be mindful of these issues, and, 
                in good faith, try to take steps to fix them. You are the next generation here.
              </p>
            </td>
          </tr>
        </table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
	    <td width="100%" valign="middle">
	      <p align="center">
	      <heading>Course Notes: RL Theory and Algorithms </heading>
	      </p>
              <p> 
	      The course will be largely based of the working draft of
	  the book <a
	  href="https://rltheorybook.github.io">"Reinforcement Learning Theory and
	  Algorithms"</a>.
	  We will be updating the notes in <a
	  href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">ABJKS</a>
	  throughout the course of the term. If you find typos or errors, please let us
	  know. We would appreciate it!
              </p>
            </td>
          </tr>
        </table>


	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
	    <td width="100%" valign="middle">
	      <p align="center">
	      <heading>Tentative Dates (see Ed for announcements)</heading>
	      </p>
	      <p>
	      HW0: Due 01/30<br>
	      HW1: Out 02/05, Due 02/16<br>
	      HW2: Out 02/24, Due 03/13<br>
	      HW3: Out 03/24, Due 04/08
	      </p>
	    </td>
	  </tr>
	</table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Schedule (tentative)</heading>
              </p>
              
              <table>
		<tbody>
                  <tr height="50" bgcolor="#F8F8FF">
                    <td></td>
                    <td></td>
                    <td> <strong>Lecture</strong></td>
                    <td>Reading</td>
                    <td>Slides/HW </td>
                  </tr>
		  
                  <tr height="50">
                    <td>01/27/26</td>
                    <td></td>
                    <td> <strong>Fundamentals</strong>: Markov Decision Processes</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.1</a></td>
                    <td><a href="slides/lecture_1.pdf">Slides</a>,<br>
                        <a href="annotated_slides/lecture_1.pdf">Annotated slides</a>
                    </td>
                  </tr>

                  <tr height="50" bgcolor="#F8F8FF">
                    <td> 01/29/26</td>
                    <td></td>
                    <td><strong>Fundamentals</strong>: Value Iteration</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.1</a></td>
                    <td><a href="slides/lecture_2.pdf">Slides</a>,<br>
                        <a href="annotated_slides/lecture_2.pdf">Annotated slides</a>
                    </td>

                  </tr>

                  <tr height="50">     
                    <td> 02/03/26</td>
                    <td></td>
                    <td><strong>Fundamentals</strong>: Policy Iteration and LP-Formulation </td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.1</a></td>
                    <td><a href="slides/lecture_3.pdf">Slides</a>,<br>
                        <a href=""></a>
                    </td>
                  </tr>

                <tr bgcolor="#F8F8FF" height="50">
                    <td> 02/05/26</td>
                    <td></td>
                    <td><strong>Fundamentals</strong>: Tabular MDP with a Generative Model</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.2 </a></td>
                    <td><a href="slides/lecture_4_Sample_Complexity.pdf">Slides</a>,<br>
                        <a href="annotated_slides/lecture_4_Sample_Complexity.pdf">Annotated slides</a></a>
		            </td>
                </tr>

<tr height="50">
                    <td> 02/10/26</td>
                    <td></td>
                    <td><strong>Fundamentals</strong>: RL with Linear Features, when it works</td> 
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.3 </a></td>
                    <td><a href="slides/linear_realizability_1.pdf">Slides</a>,<br>
                        <a href="slides/linear_realizability_1_annotated.pdf">Annotated slides</a>
                    </td>
                  </tr>

                  <tr bgcolor="#F8F8FF" height="50">  
                    <td> 02/12/26</td>
                    <td></td>
                    <td><strong>Fundamentals</strong>: RL with Linear Features, the offline case</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.3 </a></td>
                    <td><a href="slides/linear_realizability_2.pdf">Slides</a>,<br>
                        <a href=""></a>
                    </td>
                  </tr>

                  <tr height="50">  
                    <td> 02/17/26</td>
                    <td></td>
                    <td><strong>Fundamentals</strong>: RL with Linear Features, when it doesn't</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.3 </a></td>
                    <td><a href=""></a>,<br>
                        <a href=""></a>
                    </td>
                  </tr>

                  <tr bgcolor="#F8F8FF" height="50">
                    <td> 02/19/26</td>
                    <td></td>
                    <td><strong>Exploration</strong>: Multi-armed Bandits</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.5</a></td>
                    <td><a href=""></a>,<br>
                        <a href=""></a>
                    </td>
                  </tr>

                  <tr height="50">
                    <td> 02/24/26</td>
                    <td></td>
                    <td><strong>Exploration</strong>: Efficient Exploration in Tabular MDPs </td> 
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.6</a></td>
                    <td><a href=""></a>,<br>
                        <a href=""></a>
                    </td>
                  </tr>

                  <tr bgcolor="#F8F8FF" height="50">
                    <td> 02/26/26</td>
                    <td></td>
                    <td><strong>Exploration</strong>: Linear Bandits</td> 
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.5</a></td>
                    <td><a href=""></a>,<br>
                        <a href=""></a>
                    </td>
                  </tr>
                  
                  <tr height="50">
                    <td> 03/03/26</td>
                    <td></td>
                    <td><strong>Exploration</strong>: Efficient Exploration in Linear MDPs</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.7</a></td>
                    <td><a href=""></a>,<br>
                        <a href=""></a>
                    </td>
                  </tr>

                  <tr bgcolor="#F8F8FF" height="50">
                    <td> 03/05/26</td>
                    <td></td>
                    <td><strong>Exploration</strong>: RL w/ function approximation</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.8</a></td>
                    <td><a href=""></a>, <br>
                        <a href=""></a>
                    </td>
                  </tr>

                  <tr height="50">
                    <td> 03/10/26</td>
                    <td></td>
                    <td><strong>Exploration</strong>: RL w/ function approximation (continued)</td>
                    <td>  </td>
                    <td><a href=""></a>, <br>
                        <a href=""></a></td>
                  </tr>

                  <tr bgcolor="#F8F8FF" height="50">
                    <td> 03/12/26</td>
                    <td></td>
                    <td><strong>TBD</strong></td>
                    <td></td>
                    <td>
                    </td>
                  </tr>

                  <tr height="50">
                    <td> 03/17/26</td>
                    <td></td>
                    <td><strong>Spring Recess</strong></td>
                    <td>  </td>
                    <td></td>
                  </tr>

                  <tr bgcolor="#F8F8FF" height="50">
                    <td> 03/19/26</td>
                    <td></td>
                    <td><strong>Spring Recess</strong></td>
                    <td>  </td>
                    <td></td>
                  </tr>

                  <tr height="50">
                    <td> 03/24/26</td>
                    <td></td>
                    <td><strong>Policy Optimization</strong>: Policy Gradient</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.9 & 10</a> </td>
                    <td><a href=""></a>, <br>
                        <a href=""></a>
                    </td>
                  </tr>

                  <tr bgcolor="#F8F8FF" height="50">
                    <td> 03/26/26</td>
                    <td></td>
                    <td><strong>Policy Optimization</strong>: Natural Policy Gradient and TRPO</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.10 & 11</a> </td>
                    <td><a href=""></a>, <br>
                        <a href=""></a>
                    </td>
                  </tr>

                  <tr height="50">
                    <td> 03/31/26</td>
                    <td></td>
                    <td><strong>Policy Optimization</strong>: Global optimality of PG and NPG </td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.10 & 11</a></td>
                    <td><a href=""></a>, <br>
                        <a href=""></a>
                  </tr>

                  <tr bgcolor="#F8F8FF" height="50">
                    <td>04/02/26</td>
                    <td></td>
                    <td><strong>Policy Optimization</strong>: Conservative Policy Iteration and Function Approximation</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.12</a></td>
                    <td><a href=""></a>, <br>
                        <a href=""></a></td>
                  </tr>
                  
                  <tr height="50">
                    <td>04/07/26</td>
                    <td></td>
                    <td><strong>Policy Optimization</strong>: NPG and Proximal Policy Optimization</td>
                    <td><a href="https://rltheorybook.github.io/rltheorybook_ABJKS.pdf">Ch.12</a></td>
                    <td><a href=""></a>, <br>
                        <a href=""></a>
                    </td>
                  </tr>

                <tr bgcolor="#F8F8FF" height="50">
                  <td>04/09/26</td>
                  <td></td>
                  <td><strong>(TBD) RLHF</strong>: Contextual Bandits and BT model and DPO and REBEL </td>
                  <td> <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/5df1d92b478716877f774b82943477b3-Paper-Conference.pdf">Paper 1</a>, 
                       <a href="https://arxiv.org/pdf/2305.18290">Paper 2</a>
                  </td>
                </tr>

                <tr height="50">
                  <td>04/14/26</td>
                  <td></td>
                  <td><strong>Guest Lecture Wen Sun RLVR</strong>: (TBD) </td>
                  <td> <a href=""></a>, 
                       <a href=""></a>
                  </td>
                  <td></td> 
                </tr>
        
                <tr bgcolor="#F8F8FF" height="50">
                  <td>04/16/26</td>
                  <td></td>
                  <td><strong>Guest Lecture Gabriel Poesia Reis e Silva </strong>: (TBD) </td>
                    <td></td>   
                    <td>
                    </td>
                </tr>

                <tr height="50">
                  <td> 04/21/26</td>
                  <td></td>
                  <td><strong>Student Project Presentations</strong></td>
                  <td></td>
                  <td></td>
                </tr>
                
                <tr bgcolor="#F8F8FF" height="50">
                  <td> 04/23/26</td>
                  <td></td>
                  <td><strong>Student Project Presentations</strong></td>
                  <td></td>
                  <td></td>
                </tr>

                <tr height="50">
                  <td> 04/28/26</td>
                  <td></td>
                  <td><strong>Student Project Presentations</strong></td>
                  <td></td>
                  <td></td>
                </tr>


               </tbody></td></tr>
              </table>



            </td>
          </tr>
        </table>


        
        </td>
    </tr>
  </table>
</body>

</html>

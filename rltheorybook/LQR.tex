\chapter{Linear Quadratic Regulators}
\label{chap:LQR}

\newcommand{\cD}{\mathcal{D}}
\newcommand{\Tr}{\mathrm{Trace}}

This chapter will introduce some of the fundamentals of optimal
control for the linear quadratic regulator model. This model is an
MDP, with continuous states and actions. While the model itself is
often inadequate as a global model, it can be quite effective as a
locally linear model (provided our system does not deviate away from the
regime where our linear model is reasonable approximation).

The basics of optimal control theory can be found in any number of
standards
text~\citet{Anderson:1990:OCL:79089,Evans2005,Bertsekas:DP}. The
treatment of Gauss-Newton and the NPG algorithm are due
to~\citet{ICML-2018-Fazel0KM}. 

\section{The LQR Model}
In the standard optimal control problem, a dynamical system is described as
\[
x_{t+1} = f_t(x_t,u_t,w_t) \, ,
\]
where $f_t$ maps a state $x_t\in \R^d$, a control (the action) $u_t \in \R^k$,
and a disturbance $w_t$, to the next state $x_{t+1}\in \R^d$, starting
from an initial state $x_0$. The objective is to find the control
policy $\pi$ which minimizes the long term cost,   
\begin{eqnarray*}
&\textrm{minimize} & \E_\pi \left[\sum_{t=0}^H c_t(x_t,u_t)\right]\\
&\textrm{such that} & x_{t+1} = f_t(x_t,u_t,w_t) \;\; t=0,\ldots,H.
\end{eqnarray*}
where $H$ is the time horizon (which can be finite or
infinite). 

In practice, this is often solved by considering the linearized
control (sub-)problem where the dynamics are approximated by 
\[
x_{t+1} = A_tx_t +B_tu_t +w_t,
\] 
with the matrices $A_t$ and $B_t$ are derivatives of the dynamics
$f$ and where the costs are approximated by a quadratic
function in $x_t$ and $u_t$.   

This chapter focuses on an important special case: finite and infinite
horizon problem referred to as the linear quadratic regulator (LQR)
problem. We can view this model as being an local approximation to
non-linear model. However, we will analyze these models under the
assumption that they are globally valid.

\iffalse
This chapter focuses on an important special case: the time
homogenous, finite and infinite horizon problem referred to as the
linear quadratic regulator (LQR) problem. 
\fi

%The results
%herein can also be extended to the finite horizon, time inhomogeneous
%setting, discussed in Section~\ref{section:discussion}.
%which is discussed in the Appendix.  


\paragraph{Finite Horizon LQRs.} The finite horizon LQR problem is given by
\begin{eqnarray*}
&\textrm{minimize} & \E\left[x_H^\top Q x_H+
\sum_{t=0}^{H-1} (x_t^\top Q x_t + u_t^\top R u_t )\right] \\
&\textrm{such that} & x_{t+1} = A_tx_t + B_t u_t +w_t\, , \quad x_0\sim \cD ,
\ w_t \sim N(0,\sigma^2 I)\, ,
\end{eqnarray*}
where initial state $x_0\sim \cD$ is assumed to be randomly distributed
according to distribution $\cD$;  the disturbance $w_t\in \R^d$
follows the law of a multi-variate normal with covariance $\sigma^2 I$; the matrices
$A_t\in \R^{d \times d}$ and $B_t\in \R^{d \times k}$ are referred to as
system (or transition) matrices; $Q\in \R^{d \times d}$ and
$R\in \R^{k \times k}$ are 
both positive definite matrices that 
parameterize the quadratic costs.  Note that this model is 
a finite horizon MDP, where the $\Scal = \R^d$ and $\Acal=\R^k$.

\paragraph{Infinite Horizon LQRs.} We also consider the infinite horizon LQR problem:
\begin{eqnarray*}
&\textrm{minimize} & \lim_{H\rightarrow\infty}\frac{1}{H}\E\left[\sum_{t=0}^H (x_t^\top Q x_t + u_t^\top R u_t )\right] \\
&\textrm{such that} & x_{t+1} = A x_t + B u_t+w_t \, , \quad 
x_0\sim \cD, \, \ w_t \sim N(0,\sigma^2 I) .
\end{eqnarray*}
Note that here we are assuming the dynamics are time
homogenous.
\iffalse
Note that here we are not considering a noise disturbance but only a
random initial state. Extending this model to the case with noise is
possible though note we would then have to consider a limiting cost
function; this can be done via subtracting off the average cost
(this is a special case an MDP with average reward
objective). \fi
We will assume that the optimal objective function (i.e. the optimal
average cost) is finite; this is referred to as the system being \emph{controllable}.
This is a special case of an MDP with an average reward objective.

Throughout this chapter, we assume $A$ and $B$ are such that the
optimal cost is finite.  Due to the geometric
nature of the system dynamics (say for a controller which takes
controls $u_t$ that are linear in the state $x_t$), there may exists
linear controllers with infinite costs. This instability of LQRs (at
least for some $A$ and $B$ and some controllers) leads to that the
theoretical analysis often makes various assumptions on $A$ and $B$ in
order to guarantee some notion of stability. In practice, the finite
horizon setting is more commonly used in practice, particularly due to that the
LQR model is only a good local approximation of the system dynamics, where
the infinite horizon model tends to be largely of theoretical
interest. See Section~\ref{bib:LQR}. 

\paragraph{The infinite horizon discounted case?} The infinite horizon
discounted case tends not to be studied for LQRs. This is largely due
to that, for the undiscounted
case (with the average cost objective), we have may infinte costs (due
to the aforementioned geometric nature of the system dynamics); in
such cases, discounting will
not necessarily make the average cost finite.


\section{Bellman Optimality: \\
Value Iteration \& The Algebraic Riccati Equations}

A standard result in optimal control theory shows that the optimal
control input can be written as a linear function in the state. As we
shall see, this is a consequence of the Bellman equations.

 %, the optimal control policy has the form:
%\[
%u_t = -K^* x_t %\, , \textrm{where} \, \, \, K^{*} = -(B^{T} P B + R)^{-1} B^{T} P A.
%\]
%where $K^* \in \R^{k\times d}$.  


%\begin{proof}
%{\bf To be added... \/}
%\end{proof}

\subsection{Planning and Finite Horizon LQRs}

Slightly abusing notation, it is convenient to define the value function and
state-action value function with respect to the costs as follows:
For a policy $\pi$, a state $x$, and $h \in \{0,\ldots H-1\}$, we define the value
function $V_h^\pi: \R^d \to \R$ as 
\[
V_h^\pi(x) = \Expe{x_H^\top Q x_H+
\sum_{t=h}^{H-1} (x_t^\top Q x_t + u_t^\top R u_t )
\cond \pi, x_h = x} ,
\]
where again expectation is with respect to the randomness of the
trajectory, that is, the randomness in state transitions.  Similarly,
the state-action value (or Q-value) function
$Q_h^\pi: \R^d \times \R^k \to \R$ is defined as
\[
Q_h^\pi(x,u) = \Expe{x_H^\top Q x_H+
\sum_{t=h}^{H-1} (x_t^\top Q x_t + u_t^\top R u_t )
\cond \pi, x_h = x, u_h = u}.
\]
We define $V^\star$ and $Q^\star$ analogously.

The following theorem provides a characterization of the optimal
policy, via the algebraic Riccati equations. These equations 
are simply the value iteration algorithm for the special case of LQRs.


\begin{theorem}\label{thm:riccati_discrete}
(Value Iteration and the Riccati Equations).  Suppose $R$ is positive definite. The
optimal policy is a linear controller specified by:
\[
\pi^\star(x_t) = -K^\star_t x_t
\]
where
\[
K^\star_t = (B_t^\top P_{t+1} B_t+R)^{-1}B_t^\top P_{t+1}A_t.
\]
Here, $P_t$ can be computed iteratively, in a backwards manner, using
the following algebraic Riccati equations, where for $t\in [H]$,
\begin{eqnarray*}
P_t &:= &A_t^\top P_{t+1} A_t +Q- A_t^\top P_{t+1} B_t (B_t^\top P_{t+1} B_t+R)^{-1}B_t^\top P_{t+1}A_t\\
&= &A_t^\top P_{t+1} A_t +Q- (K^\star_{t+1})^\top(B_t^\top P_{t+1} B_t+R)K^\star_{t+1}
%\label{eq:riccati_discrete}
\end{eqnarray*}
and where $P_H=Q$.
(The above equation is simply the value iteration algorithm).

Furthermore, for $t\in [H]$, we have that:
\[
V^\star_t(x) = x^\top P_t x + \sigma^2 \sum_{h=t+1}^{H} \mathrm{Trace}(P_h).
\]
\end{theorem}

We often refer to $K_t^\star$ as the \emph{optimal control gain} matrices.
It is straightforward to generalize the above when $\sigma\geq 0$.

We have assumed that $R$ is strictly positive definite, to avoid have
to working with the pseudo-inverse; the theorem is still true when
$R=0$, provided we use a pseudo-inverse.



\begin{proof}
By the Bellman optimality conditions
(Theorem~\ref{thm:Bellman_episodic} for episodic MDPs), we know the
optimal policy (among all possibly history dependent, non-stationary, and randomized policies),  is
given by a deterministic stationary policy which is only a function of
$x_t$ and $t$. We have that:
\begin{align*}
Q_{H-1}(x,u) &=  \E\big[(A_{H-1}x+B_{H-1}u +w_{H-1})^\top Q (A_{H-1}x+B_{H-1}u +w_{H-1})\big] 
+x^\top Q x+ u^\top R u\\
 &=  (A_{H-1}x+B_{H-1}u)^\top Q (A_{H-1}x+B_{H-1}u) + \sigma^2\textrm{Trace}(Q)+x^\top Q x
+ u^\top R u
\end{align*}
due to that $x_H = A_{H-1}x+B_{H-1}u+w_{H-1}$, and $\E[ w_{H-1}^{\top} Q w_{H-1} ] = \textrm{Trace}(\sigma^2 Q)$.
Due to that this is a quadratic function of
$u$, we can immediately derive that the optimal control is given by:
\[
\pi^\star_{H-1}(x)  = - (B_{H-1}^\top Q B_{H-1}+R)^{-1}B_{H-1}^\top QA_{H-1} x= -K^\star_{H-1}x,
\]
where the last step uses that $P_H:=Q$. 

For notational convenience, let $K=K^\star_{H-1}$, $A=A_{H-1}$, and
$B=B_{H-1}$. Using the optimal control at $x$, i.e. $u=-K^\star_{H-1} x$, we have:
\begin{align*}
V^\star_{H-1}(x)
&=Q_{H-1}(x,-K^\star_{H-1} x)  \\
&= x^\top\Big( (A-BK)^\top Q (A-BK)+ Q + K^\top R K \Big)x
+\sigma^2\textrm{Trace}(Q)\\
&= x^\top \Big(A Q A+ Q -2 K^\top B^\top QA
+ K^\top (B^\top QB+R) K \Big)x +\sigma^2\textrm{Trace}(Q)\\
&= x^\top \Big(A Q A+ Q -2 K^\top (B^\top QB+R) K
+ K^\top (B^\top QB+R) K \Big)x +\sigma^2\textrm{Trace}(Q)\\
&= x^\top \Big(A Q A+ Q - K^\top (B^\top QB+R)K\Big)x +\sigma^2\textrm{Trace}(Q)\\
&=x^\top P_{H-1}x +\sigma^2\textrm{Trace}(Q).
\end{align*}
where the fourth step uses our expression for
$K=K^\star_{H-1}$. This proves our claim for $t=H-1$.

This implies that:
\begin{align*}
Q^\star_{H-2}(x,u) &=  \E[V^\star_{H-1}(A_{H-2}x+B_{H-2}u+w_{H-2})]
+x^\top Q x+u^\top R u\\
&=  (A_{H-2}x+B_{H-2}u)^\top P_{H-1} (A_{H-2}x+B_{H-2}u)+
\sigma^2\textrm{Trace}(P_{H-1})+\sigma^2\textrm{Trace}(Q)
+x^\top Q x + u^\top R u.
\end{align*}
The remainder of the proof follows from a recursive argument, which
can be verified along identical lines to the $t=H-1$ case.
\end{proof}

\subsection{Planning and Infinite Horizon LQRs}

\begin{theorem}
Suppose that the optimal cost is finite and that $R$ is positive definite.
Let $P$ be a solution to the following algebraic Riccati equation:
\begin{equation}\label{eq:ARE}
P= A^{T} P A + Q - A^{T} PB(B^{T} PB+R)^{-1} B^{T} P A .
\end{equation}
(Note that $P$ is a positive definite matrix).
We have that the optimal policy is:
\[
\pi^\star(x)= -K^\star x
\]
where the optimal control gain is:
\begin{equation} \label{eq:K_from_P}
K^{*} = -(B^{T} P B + R)^{-1} B^{T} P A.
\end{equation}
We have that $P$ is unique and that the optimal average cost is
$\sigma^2\mathrm{Trace}(P)$. 
\end{theorem}

As before, $P$ parameterizes the 
optimal value function. We do not prove this theorem here though it
follows along similar lines to the previous proof, via a limiting argument.

To find $P$, we can again
run the recursion:
\[
P\leftarrow Q+ A^TP A - A^TP B (R+B^TP B)^{-1} B^TP A
\]
starting with $P=Q$, which can be shown to converge to the unique positive semidefinite solution 
of the Riccati equation (since one can show the fixed-point iteration
is contractive).  Again, this approach is simply value iteration. 


\section{Convex Programs to find $P$ and $K^\star$}

For the infinite horizon LQR problem, the optimization may be
formulated as a convex program. In particular, the LQR problem can also be expressed as a
semidefinite program (SDP) with variable $P$,
for the infinite horizon case. We now present this primal program
along with the dual program.

Note that these programs are the analogues of the linear programs from
Section~\ref{sect:LP} for MDPs. While specifying these linear programs for
an LQR (as an LQR is an MDP) would result in infinite dimensional
linear programs, the special structure of the LQR implies these
primal and dual programs have a more compact formulation when specified as an SDP.

\subsection{The Primal for Infinite Horizon LQR}


The primal optimization problem is given as:
\begin{equation*}%\label{eq:SDP}
\begin{array}{ll}
\mbox{maximize} & \sigma^2\textrm{Trace}(P) \\
\mbox{subject to} & 
   \left[ 
\begin{array}{ll} A^T PA+Q-P &  A^\top PB\\ 
B^T P A& B^\top P B + R \end{array}  
\right]  \succeq 0, \;\;\;\; P\succeq 0,
\end{array}
\end{equation*}
where the optimization variable is $P$. This SDP has a unique
solution, $P^\star$,  which satisfies the algebraic Riccati equations
(Equation~\ref{eq:ARE} ); the optimal average cost of the infinite
horizon LQR is $\sigma^2\textrm{Trace}(P^\star)$; and the optimal
policy is given by Equation~\ref{eq:K_from_P}.

The SDP can be derived by relaxing the equality in the
Riccati equation to an inequality, then using the Schur complement
lemma to rewrite the resulting Riccati inequality as linear matrix
inequality. In particular, we can consider the relaxation where $P$
must satisfy:
\[
A^{T} P A + Q - A^{T} PB(B^{T} PB+R)^{-1} B^{T} P A  \succeq P.
\]
That the solution to this relaxed optimization problem leads to the optimal
$P^\star$ is due to the Bellman optimality conditions.

Now the Schur complement lemma for positive semi-definiteness is
as follows: define
\[
X = \left[\begin{array}{ll} D & E\\ 
E^\top & F \end{array}\right].
\]
(for matrices $D$, $E$, and $F$ of appropriate size, with $D$ and $F$
being square symmetric matrices).  We have that $X$ is
PSD if and only if
\[
D - E^\top F^{-1} E \succeq 0.
\] 
This shows that the constraint set is equivalent to the above relaxation. 

\iffalse
following from the Riccati equation
(Equation~\ref{eq:ARE}), we have the relaxation that for all matrices
$K$, $P$ must satisfy
\[
P\succeq (A-BK)^{T} P (A-BK) + Q - K^\top R K .
\]
One way to see this is to consider the Bellman optimality constraint
for the case of linear policies. Alternatively, one can also 
consider the discrete time Belman
optimality equations and then take limits (where $P_{h-1}=P_h$ in
the limit).
\fi

\subsection{The Dual}

The dual optimization problem is given as:
\begin{equation*}%\label{eq:SDP}
\begin{array}{ll}
\mbox{minimize} & \textrm{Trace}\left( \Sigma \cdot \left[\begin{array}{ll} Q & 0\\ 
0& R \end{array}\right] \right) \vspace*{0.1cm}\\
\mbox{subject to} & 
\Sigma_{xx}=(A \ \ B)\Sigma (A \ \  B)^\top+\sigma^2 I, \;\;\;\; \Sigma\succeq 0,
\end{array}
\end{equation*}
where the optimization variable is a symmetric matrix $\Sigma$, which is a $(d+k)\times
(d+k)$ matrix with the block structure:
\[
\Sigma = \left[\begin{array}{ll} \Sigma_{xx} & \Sigma_{xu}\\ 
\Sigma_{ux}& \Sigma_{uu} \end{array}\right].
\]
The interpretation of $\Sigma$ is that it is the covariance matrix of the
stationary distribution. This analogous to the state-visitation
measure for an MDP.

This SDP has a unique solution, say $\Sigma^\star$. The optimal
gain matrix is then given by:
\[
K^\star = -\Sigma^\star_{ux} (\Sigma^\star_{xx})^{-1}.
\]


\section{Policy Iteration, Gauss Newton, and NPG}

Note that the noise variance $\sigma^2$ does not impact the optimal
policy, in either the discounted case or in the infinite horizon
case. 

Here, when we examine local search methods, it is more convenient to
work with case where $\sigma=0$.  In this case, we can work with
cumulative cost rather the average cost. Precisely, when $\sigma=0$,
the infinite horizon LQR problem takes the form: 
\begin{eqnarray*}
\min_K C(K), \textrm{ where }
C(K) = \E_{x_0\sim \cD}\left[\sum_{t=0}^H (x_t^\top Q x_t + u_t^\top R
                     u_t )\right] ,
\end{eqnarray*}
where they dynamics evolves as 
\[
x_{t+1} = (A-BK) x_t.
\]
Note that we have directly parameterized our policy as a linear
policy in terms of the gain matrix $K$, due to that we know the
optimal policy is linear in the state.  Again, we assume that
$C(K^\star)$ is finite; this assumption is referred to as the system being
\emph{controllable}.

We now examine local search based approaches, where we will see a
close connection to policy iteration. Again, we have a non-convex
optimization problem:

\begin{lemma}%\label{lem:main:nonconvex}
(Non-convexity) If $d\ge3$, there exists an LQR optimization problem,
$\min_{K} C(K)$,  which is not convex or quasi-convex.
\end{lemma}

Regardless, we will see that gradient based approaches are effective. 
For local search based approaches, the importance of (some)
randomization, either in $x_0$ or noise through having a disturbance,
is analogous to our use of having a wide-coverage distribution $\mu$
(for MDPs).

\subsection{Gradient Expressions}
Gradient descent on $C(K)$, with a fixed stepsize $\eta$,  follows the update rule:
\[
K \leftarrow K - \eta \nabla C(K)\, .
\]
It is helpful to explicitly write out the functional form of the
gradient. Define $P_K$ as the solution to: 
\[
P_K = Q + K^\top R K + (A-BK)^\top P_K  (A-BK) \, .
\]
and, under this definition, it follows that $C(K)$ can be written as:
\[
C(K) = \E_{x_0\sim \cD }  \, x_0^\top P_K x_0 \, .
\]
Also, define $\Sigma_K$ as the (un-normalized) state correlation matrix, i.e.
\[
\Sigma_K = \E_{x_0\sim \cD } \sum_{t=0}^\infty x_t x_t^\top \, .
\]

\begin{lemma}\label{lem:gradexpression}
(Policy Gradient Expression) 
The policy gradient is:
\[
\nabla C(K) = 2 \left( (R+ B^\top P_K B) K - B^\top P_K A\right)
\Sigma_K 
\]
For convenience, define $E_K$ to be
$$
E_K = \left( (R+ B^\top P_K B) K - B^\top P_K A\right),
$$
as a result the gradient can be written as $\nabla C(K) = 2E_K\Sigma_K$.
\end{lemma}

\begin{proof}
Observe:
\begin{align*}
C_K(x_0)  = x_0^\top P_K x_0
  &= x_0^\top\left(Q+K^\top R K\right) x_0
+ x_0^\top (A-BK)^\top P_K (A-BK) x_0 \\
&= x_0^\top\left(Q+K^\top R K\right) x_0
+ C_K((A-BK)x_0) \, .
\end{align*}
Let $\nabla$ denote the gradient with respect to $K$; note that
$\nabla C_K((A-BK)x_0)$ has two terms as function of $K$,  one with respect to $K$ in the
subscript and one with respect to the input $(A-BK)x_0$. This implies  
\begin{align*}
\nabla C_K(x_0)&=
 2 R K x_0 x_0^\top  -2 B^\top P_K (A-BK) x_0 x_0^\top
+  \nabla C_K(x_1)|_{x_1=(A-BK)x_0}\\ 
&= 
2 \left( (R+ B^\top P_K B) K - B^\top P_K A\right) \sum_{t=0}^\infty x_t x_t^\top  
\end{align*}
where we have used recursion and that $x_1 = (A-BK) x_0$. Taking expectations
completes the proof.
\end{proof}

{\bf The natural policy gradient.\/}
Let us now motivate a version of the natural gradient.
The natural policy gradient follows the update:
\begin{align*}
\theta \leftarrow \theta - \eta \, F(\theta)^{-1} \nabla C(\theta),
\  \textrm{where} \ \
F(\theta) = \E\left[\sum_{t=0}^\infty \nabla \log \pi_\theta(u_t|x_t)
  \nabla \log \pi_\theta(u_t|x_t)^\top \right] \, ,
\end{align*}
where $F(\theta)$ is the Fisher information matrix. 
A natural special case is using a linear policy with additive
Gaussian noise, i.e. 
\begin{equation}\label {eq:gauss_policy}
\pi_K(x,u) = \mathcal{N}(Kx,\sigma^2 I)
\end{equation}
where $K \in \R^{k\times d}$ and $\sigma^2$ is the noise variance. In
this case, the
natural policy gradient of $K$ (when $\sigma$ is considered fixed) takes the form:
\begin{equation}\label{eq:ng_def}
K \leftarrow K - \eta \nabla C(\pi_K) \Sigma_K^{-1}
\end{equation}
Note a subtlety here is that $C(\pi_K)$ is the randomized
policy. 

To see this, one can verify that the Fisher matrix of size $kd \times kd$, which is indexed as
$[G_K]_{(i,j),(i',j')}$ where $i,i' \in \{1,\ldots k\}$ and $j,j'\in
\{1,\ldots d\}$,  has a block diagonal form where the only non-zeros blocks are
$
[G_K]_{(i,\cdot),(i,\cdot)} = \Sigma_K
$
(this is the block corresponding to the $i$-th coordinate of the
action, as $i$ ranges from $1$ to $k$). This form holds more generally, for any
diagonal noise.

\subsection{Convergence Rates}

We consider three exact rules, where we assume access to having exact
gradients. As before, we can also estimate these gradients through
simulation. For gradient descent, the update is 
\begin{equation}
 K_{n+1} =  K_n - \eta \nabla C(K_n). \label{eq:exact_gd}
\end{equation}
For natural policy gradient descent, the direction is defined so that it is
consistent with the stochastic case, as per
Equation~\ref{eq:ng_def}, in the exact case the update is:
\begin{equation}
 K_{n+1} =  K_n - \eta \nabla C(K_n) \Sigma_{K_n}^{-1} \label{eq:exact_ngd}
 \end{equation}
One show that the Gauss-Newton update is:
\begin{equation}
 K_{n+1} =  K_n - 
\eta (R+ B^\top P_{K_n} B)^{-1}  \nabla C(K_n) \Sigma_{K_n}^{-1} \, .\label{eq:exact_gn}
\end{equation}
(Gauss-Newton is non-linear optimization approach which uses a certain
Hessian approximation. It can be show that this leads to the above
update rule.) Interestingly, for the case when $\eta =
1$ , the Gauss-Newton method  is equivalent to the policy iteration
algorithm, which optimizes a one-step deviation from the current policy.

The Gauss-Newton method requires the most complex oracle to implement: it requires
access to $\nabla C(K)$, $\Sigma_K$, and $R+ B^\top P_K B$; as we
shall see, it also
enjoys the strongest convergence rate guarantee. At the
other extreme, gradient descent requires oracle access to
only $\nabla C(K)$ and has the slowest convergence rate. The natural policy gradient sits in
between, requiring oracle access to $\nabla C(K)$ and
$\Sigma_K$, and having a convergence rate between the other two methods.

In this theorem, $\|M\|_2$ denotes the spectral norm of a matrix $M$.

\begin{theorem}\label{theorem:gd_exact}
(Global Convergence of Gradient Methods) Suppose $C(K_0)$ is
finite and, for $\mu$ defined as
\[
\mu := \sigma_{\textrm{min}}(\E_{x_0\sim \cD } x_0 x_0^\top),
\]
suppose $\mu>0$.
\begin{itemize}
\item Gauss-Newton case: Suppose $\eta=1$,  
the Gauss-Newton algorithm (Equation~\ref{eq:exact_gn}) enjoys the following performance bound:
\[
C(K_N) -C(K^*) \leq \eps, \ \textrm{ for } \ \ 
N \geq \frac{\|\Sigma_{K^*}\|_2}{\mu} \, \log \frac{C(K_0)
  -C(K^*)}{\eps} \, .
\]
\item Natural policy gradient case: For a stepsize 
$\eta = 1/(\|R\|_2 + \frac{\|B\|_2^2 C(K_0)}{\mu})$,
natural policy gradient descent (Equation~\ref{eq:exact_ngd}) enjoys the
following performance bound:
\[
C(K_N) -C(K^*) \leq \eps , \ \textrm{ for } \ \
N \geq \frac{\|\Sigma_{K^*}\|_2}{\mu} \,
\left(\frac{\|R\|_2}{\sigma_{\textrm{min}}(R)} + 
\frac{\|B\|_2^2 C(K_0)}{\mu \sigma_{\textrm{min}}(R)} \right) 
\log \frac{C(K_0) -C(K^*)}{\eps}. 
\]
\item Gradient descent case: For any starting policy $K_0$, there exists
  a (constant) stepsize $\eta$ (which could be a function of $K_0$), such that:
\[
C(K_N) \rightarrow C(K^*) , \ \textrm{ as } \ \ N\rightarrow\infty.
\]
\end{itemize}
\end{theorem}

\subsection{Gauss-Newton Analysis}

We only provide a proof for the Gauss-Newton case (see~\ref{bib:LQR}
for further readings).

We overload notation and let $K$ denote the policy $\pi(x)=Kx$. For
the infinite horizon cost function, define:
\begin{eqnarray*}
V_K(x) & := & 
\sum_{t=0}^\infty \left(x_t^\top Q x_t + u_t^\top R u_t \right) \\
& = & x^\top P_K x \, ,
\end{eqnarray*}
and
\[
Q_K(x,u) := x^\top Q x + u^\top R u + V_K(Ax+Bu) \, ,
\] 
and 
\[ 
A_K(x,u) = Q_K(x,u) - V_K(x) \, .
\]
The next lemma is identical to the performance difference lemma.

\begin{lemma}\label{lemma:helper}
(Cost difference lemma) Suppose $K$ and $K'$ have finite costs. Let
$\{x'_t\}$ and $\{u'_t\}$ be state and 
action sequences generated by $K'$, i.e. starting with $x'_0=x$ and
using $u'_t = -K' x'_t$. It holds that:
\[
V_{K'}(x) - V_K(x)  = \sum_t A_K(x'_t, u'_t) \, .
\]
Also, for any $x$, the advantage is:
\begin{equation}\label{eq:advantage}
A_K(x,K'x) =   2x^\top(K'-K)^\top E_K x
+
x^\top(K'-K)^\top (R + B^\top P_KB) (K'-K)x \, .
\end{equation}
\end{lemma}

\begin{proof}
Let  $c_t'$ be the cost sequence generated by $K'$. Telescoping the
sum appropriately:
\begin{align*}
V_{K'}(x) - V_K(x)  &=
\sum_{t=0} c_t' - V_K(x)  
=\sum_{t=0} (c_t'+ V_K(x'_t) - V_K(x'_t)) - V_K(x)  \\
&=\sum_{t=0} (c_t'+ V_K(x'_{t+1}) - V_K(x'_t))  
=\sum_{t=0} A_K(x'_t, u'_t)
\end{align*}
which completes the first claim (the third equality uses the fact that $x = x_0 = x'_0$). 

For the second claim, observe that:
\begin{align*}
V_K(x)   =   x^\top\left(Q+K^\top R K\right) x  + 
 x^\top (A-BK)^\top P_K (A-BK) x
\end{align*}
And, for $u=K'x$,
\begin{eqnarray*}
A_K(x,u) & = &  Q_K(x,u) -V_K(x)\\
 & = &  x^\top\left(Q+(K')^\top R K'\right) x  + 
 x^\top (A-BK')^\top P_K (A-BK') x -V_K(x)\\
 & = &  x^\top\left(Q+(K'-K+K)^\top R (K'-K+K)\right) x  + \\
&& x^\top (A-BK- B(K'-K))^\top P_K (A -BK-B(K'-K)) x -V_K(x)\\
 & = &  2x^\top(K'-K)^\top \left( (R+ B^\top P_K B) K - B^\top P_K A\right) x + \\
&&x^\top (K'-K)^\top (R + B^\top P_KB) (K'-K)) x \, ,
\end{eqnarray*}
which completes the proof.
\end{proof}

We have the following corollary, which can be viewed analogously to a
smoothness lemma.

\begin{corollary}\label{lemma:smoothness}
(``Almost'' smoothness) $C(K)$ satisfies:
\[
C(K')-C(K) 
 = 
-2\Tr(\Sigma_{K'}(K-K')^\top E_K) +
\Tr(\Sigma_{K'}(K-K')^\top (R + B^\top P_KB) (K-K'))
\]
\end{corollary}

To see why this is related to smoothness (recall the definition of a
smooth function in Equation~\ref{eq:def_smoothness}), suppose $K'$ is
sufficiently close to $K$ so that:
\begin{equation}\label{eq:taylor}
\Sigma_{K'} \approx \Sigma_K + O(\|K-K'\|)
\end{equation}
and the leading order term $2\Tr(\Sigma_{K'}(K'-K)^\top E_K)$ would
then behave as $\Tr((K'-K)^\top \nabla C(K))$.

\begin{proof}
The claim immediately results from Lemma~\ref{lemma:helper}, by using
Equation~\ref{eq:advantage} and taking an expectation.
\end{proof}

We now use this cost difference lemma to show that $C(K)$ is gradient dominated.

\begin{lemma}\label{lemma:domination}
(Gradient domination) Let $K^*$ be an optimal policy. Suppose $K$ has
finite cost and $\mu >0$. It holds that:
%$C(\cdot)$ is gradient dominated in the following sense:
\begin{align*}
C(K)-C(K^*)  
& \leq 
\|\Sigma_{K^*}\| \Tr(E_K^\top (R + B^\top P_KB)^{-1}E_K)\\
%&  \leq \frac{\|\Sigma_{K^*}\|}{\sigma_{\textrm{min}}(R)} \Tr(E_K^\top E_K)\\
%&  \leq
%\frac{\|\Sigma_{K^*}\|}{\sigma_{min}(\Sigma_K)^2
%\sigma_{\textrm{min}}(R)}  \Tr(\nabla C(K)^\top \nabla C(K)) \\
& \leq\frac{\|\Sigma_{K^*}\|}{\mu^2 \sigma_{\textrm{min}}(R)} \Tr(\nabla C(K)^\top \nabla C(K))
\end{align*}
\end{lemma}

\begin{proof}
From Equation~\ref{eq:advantage} and by completing the square,
\begin{align}
&A_K(x,K'x) 
=Q_K(x,K'x) - V_K(x) \nonumber \\
&\ \  =  2\Tr(xx^\top(K'-K)^\top E_K) +
                        \Tr(xx^\top(K'-K)^\top (R + B^\top P_KB) (K'-K)) \nonumber \\ 
&\ \ = 
\Tr(xx^\top\left(K'-K+(R + B^\top P_KB)^{-1}E_K \right)^\top (R + B^\top P_KB)
    \left(K'-K+(R + B^\top P_KB)^{-1}E_K\right)) \nonumber \\ 
&\ \  -\Tr(xx^\top E_K^\top(R + B^\top P_KB)^{-1}E_K) \nonumber \\ 
&\ \  \geq -\Tr(xx^\top E_K^\top (R + B^\top P_KB)^{-1} E_K) \label{eq:completed_square}
\end{align}
with equality when $K'=K-(R + B^\top P_KB)^{-1}E_K$.


Let $x^*_t$ and $u^*_t$ be the sequence generated under $K_*$. Using
this and Lemma~\ref{lemma:helper}, 
\begin{align*}
&C(K) -C(K^*)  =  -\E \sum_t A_K(x^*_t,u^*_t)
\leq \E \sum_t \Tr(x^*_t(x^*_t)^\top E_K^\top (R + B^\top P_KB)^{-1} E_K)\\
&\quad = \Tr(\Sigma_{K^*}E_K^\top (R + B^\top P_KB)^{-1} E_K)
\leq  \|\Sigma_{K^*}\| \Tr(E_K^\top (R + B^\top P_KB)^{-1}E_K)
\end{align*}
which completes the proof of first inequality. For the second
inequality, observe:
\begin{align*}
&\Tr(E_K^\top (R + B^\top P_KB)^{-1}E_K)
\leq \| (R + B^\top P_KB)^{-1}\| \,
\Tr(E_K^\top E_K)
\leq\frac{1}{\sigma_{\textrm{min}}(R)} \Tr(E_K^\top E_K)\\
&\quad =\frac{1}{ \sigma_{\textrm{min}}(R)}
    \Tr(\Sigma_K^{-1}\nabla C(K)^\top \nabla C(K) \Sigma_K^{-1})
\leq\frac{1}{\sigma_{min}(\Sigma_K)^2 \sigma_{\textrm{min}}(R)} \Tr(\nabla C(K)^\top \nabla C(K))\\
&\quad \leq\frac{1}{\mu^2 \sigma_{\textrm{min}}(R)} \Tr(\nabla C(K)^\top \nabla C(K))
\end{align*}
which completes the proof of the upper bound. Here the last step is because $\Sigma_K \succeq \E[x_0x_0^\top]$. 
\end{proof}


The next lemma bounds the one step progress of Gauss-Newton.

\begin{lemma}(Gauss-Newton Contraction)
Suppose that:
\[
K' = K - \eta (R+ B^\top P_K B)^{-1}  \nabla C(K) \Sigma_K^{-1}\, , .
\]
If $\eta \leq 1$, then
\[
C(K') -C(K^*) \leq \left(1- \frac{\eta \mu }{\|\Sigma_{K^*}\|}\right) (C(K)-C(K^*))
\]
\end{lemma}

\begin{proof}
Observe that for PSD matrices $A$ and $B$, we have that $\Tr(AB)\geq
\sigma_{\min}(A)\Tr(B)$. Also, observe $K' = K - \eta (R+ B^\top P_K B)^{-1} E_K$.
Using Lemma~\ref{lemma:smoothness} and the condition on $\eta$,
\begin{eqnarray*}
C(K')-C(K) 
&=&
-2\eta\Tr(\Sigma_{K'} E_K^\top (R+ B^\top P_K B)^{-1} E_K) +
\eta^2  \Tr(\Sigma_{K'} E_K^\top (R+ B^\top P_K B)^{-1} E_K)\\
&\leq&-\eta \Tr(\Sigma_{K'} E_K^\top (R+ B^\top P_K B)^{-1}
    E_K)\\
&\leq&-\eta \sigma_{\textrm{min}}(\Sigma_{K'})\Tr(E_K^\top (R+ B^\top P_K B)^{-1}
    E_K)\\
& \leq & - \eta \mu\Tr(E_K^\top (R+ B^\top P_K B)^{-1} E_K) \\
& \leq & - \eta \frac{\mu}{\|\Sigma_{K^*}\|} (C(K)-C(K^*)) \, ,
\end{eqnarray*}
where the last step uses Lemma~\ref{lemma:domination}.

\iffalse
Now, we will prove that $K'$ is always stabilizing for our choice of
$\eta$. We will use $K'(\eta)$ to denote the policy $K'$ when we
choose step size $\eta$. Assume towards contradiction that for some
$\eta \le 1$ $\Sigma_{K'}$ is not stabilizing. Let $\eta_0 =
\inf_{\eta} \rho(A-BK'(\eta)) \ge 1$ and $\eta_1 = \eta_0 - \epsilon$
for small enough $\epsilon$. By definition $\eta_1$ is still
stabilizing so we know $C(K'(\eta_1)) \le C(K)$, and also
$\|A-BK'(\eta_1)\| \le \|A-BK\| + \|(R+ B^\top P_K B)^{-1}  \nabla
C(K) \Sigma_K^{-1}\|$ is uniformly bounded for every $K'(\eta)$. By
Lemma~\ref{lemma:SigmaK_perturbation} we know there exists a
neighborhood of $K'(\eta_1)$ such that every policy in this
neighborhood is stabilizing. However, this contradicts with the
assumption that $K'(\eta_0)$ is not stabilizing when $\epsilon$ is
chosen to be small enough. 
\fi
\end{proof}

With this lemma, the proof of the convergence rate of the Gauss Newton
algorithm is immediate.

\begin{proof} (of Theorem~\ref{theorem:gd_exact}, Gauss-Newton case)
The theorem is due to that $\eta=1$ leads to a contraction of  $1- \frac{\mu
}{\|\Sigma_{K^*}\|}$ at every step.
\end{proof}


\section{System Level Synthesis for Linear Dynamical Systems }
\label{sec:convex_para_lqr}
We demonstrate another parameterization of controllers which admits convexity.  Specifically in this section, we consider finite horizon setting, i.e.,
\begin{align*}
&x_{t+1} = A x_t + B u_t + w_t,  \quad x_0\sim \Dcal,  w_t\sim \mathcal{N}(0, \sigma^2 I),\quad t \in [0,1,\dots, H-1].
\end{align*}
Instead of focusing on quadratic cost function, we consider general convex cost function $c(x_t,u_t)$, and the goal is to optimize:
\begin{align*}
\E_{\pi} \left[ \sum_{t=0}^{H-1} c_t(x_t, u_t)   \right],
\end{align*}  where $\pi$ is a time-dependent policy. 

While we relax the assumption on the cost function from quadratic cost to general convex cost, we still focus on linearly parameterized policy, i.e.,  we are still interested in searching for a sequence of time-dependent linear controllers $\{-K^\star_t\}_{t=0}^{H-1}$ with $u_t = -K^\star_t x_t$, such that it minimizes the above objective function.  We again assume the system is controllable, i.e., the expected total cost under $\{-K^\star_t\}_{t=0}^{H-1}$ is finite. 

Note that due to the fact that now the cost function is not quadratic anymore, the Riccati equation we studied before does not hold , and it is not necessarily true that the value function of any linear controllers will be a quadratic function. 


We present another parameterization of linear controllers which admit convexity in the objective function with respect to parameterization (recall that the objective function is non-convex with respect to linear controllers $-K_t$). With the new parameterization, we will see that due to the convexity, we can directly apply gradient descent to find the globally optimal solution. 

Consider any fixed time-dependent linear controllers $\{-K_t\}_{t=0}^{H-1}$. We start by rolling out the linear system under the controllers.  Note that during the rollout, once we observe $x_{t+1}$, we can compute $w_{t}$ as $w_{t} = x_{t+1} - A x_t - B u_t$.  With $x_0 \sim \mu$, we have that at time step $t$:
\begin{align*}
u_t = -K_t x_t & = - K_t ( A x_{t-1} - BK_{t-1} x_{t-1} + w_{t-1})  \\
& =  - K_t w_{t-1} - K_t ( A - B K_{t-1})x_{t-1} \\
& = - K_t w_{t-1} - K_t ( A - B K_{t-1}) ( A x_{t-2} - B K_{t-2} x_{t-2} + w_{t-2}) \\
& = \underbrace{- K_t}_{:= M_{t-1;t}} w_{t-1}  \underbrace{- K_t ( A - B K_{t-1})}_{:=M_{t-2;t}} w_{t-2}  \underbrace{- K_t ( A - B K_{t-1})(A - B K_{t-2})}_{:=M_{t-3;t}} x_{t-2}\\
%& = - K_t w_{t-1} - K_t ( A + B K_{t-1}) w_{t-2} - K_t ( A + B K_{t-1})(A + B K_{t-2}) w_{t-3} + K_t ( A + B K_{t-1})(A + B K_{t-2})(A + B K_{t-3}) w_{t-4}
& =  - K_t \left(\prod_{\tau = 1}^t (A - B K_{t-\tau})\right) x_0  -  \sum_{\tau = 0}^{t-1} K_t     \prod_{h=1}^{t-1-\tau}  (A- B K_{t -h })       w_{\tau}         
\end{align*}
Note that we can equivalently write $u_t$ using $x_0$ and the noises $w_{0},\dots, w_{t-1}$. Hence for time step $t$, let us do the following re-parameterization. Denote  
\begin{align*}
M_{t} := - K_t \left(\prod_{\tau = 1}^t (A - B K_{t-\tau})\right), \quad  M_{\tau; t} := -K_t     \prod_{h=1}^{t-1-\tau}  (A- B K_{t -h }), \text{ where } \tau  = [0, \dots, t-1].
\end{align*}
Now we can express the control $u_t$ using $M_{\tau; t}$ for $\tau\in [0,\dots, t-1] $ and $M_t$ as follows:
\begin{align*}
u_t = M_t x_0 + \sum_{\tau = 0}^{t-1} M_{\tau; t} w_{\tau},
\end{align*} which is equal to $u_t = -K_t  x_t$. 
Note that above we only reasoned time step $t$. We can repeat the same calculation for all $t = 0\to H-1$.

The above calculation basically proves the following claim. 
\begin{claim}For any linear controllers $\pi := \{-K_0,\dots, -K_{H-1}\}$, there exists a parameterization, \\$\widetilde\pi:= \left\{ \{M_t, M_{0;t},\dots M_{t-1;t }\} \right\}_{t=0}^{H-1}$, such that when execute $\pi$ and $\widetilde\pi$ under any initialization $x_0$, and any sequence of noises $\{w_t\}_{t=0}^{H-1}$, they generate exactly the same state-control trajectory.
\end{claim}

We can execute $\widetilde\pi:= \left\{ \{M_t, M_{0;t},\dots M_{t-1;t } \}\right\}_{t=0}^{H-1}$ in the following way. Given any $x_0$, we execute $u_0 = M_0 x_0$ and observe $x_1$; at time step t with the observed $x_t$, we calculate $w_{t-1} = x_{t} - A x_{t-1} - B u_{t-1}$, and execute the control $u_t  = M_t x_0 + \sum_{\tau = 0}^{t-1} M_{\tau;t} w_\tau $. We repeat until we execute the last control $u_{H-1}$ and reach $x_{H}$.



 What is the benefit of the above parameterization? Note that for any $t$, this is clearly over-parameterized: the simple linear controllers $-K_t$ has $d \times k$ many parameters, while the new controller $\{M_t; \{M_{\tau; t}\}_{\tau = 0}^{t-1}\}$ has $t \times d \times  k $ many parameters. The benefit of the above parameterization is that the objective function now is convex!
%We generalize the quadratic cost function with postive-semi-definite Q and R here to any general convex cost function, i.e., $c_t(x,u)$ is convex in both $x$ and $u$. 
The following claim formally shows the convexity. 
%shows that the objective function (expected total cost) is convex with respect to $\widetilde\pi:= \{ M_t, M_{0;t},\dots M_{t-1;t } \}_{t=0}^{H-1}$.

\begin{claim}
Given $\widetilde\pi:= \left\{ \{M_t, M_{0;t},\dots M_{t-1;t }\} \right\}_{t=0}^{H-1}$ and denote its expected total cost as $J(\widetilde\pi) := \mathbb{E}\left[ \sum_{t=0}^{H-1} c(x_t, u_t) \right]$, where the expectation is with respect to the noise $w_t\sim \mathcal{N}(0, \sigma^2 I)$ and the initial state $x_0\sim \mu$, and $c$ is any convex function with respect to $x,u$. We have that $J(\widetilde\pi)$ is convex with respect to parameters $\left\{\{ M_t, M_{0;t},\dots M_{t-1;t }\} \right\}_{t=0}^{H-1}$.
\end{claim}
\begin{proof}
We consider a fixed $x_0$ and a fixed sequence of noises $\{w_t\}_{t=0}^{H-1}$. Recall that for $u_t$, we can write it as:
\begin{align*}
&u_t = M_{t} x_0 + \sum_{\tau = 0}^{t-1} M_{\tau; t} w_{\tau}, 
%\\
% x_{t} & = A x_{t-1} + B M_{t-1} x_0 + \sum_{\tau = 0}^{t-2} B M_{\tau; t-1} w_{\tau} = A \left( A x_{t-2} + B u_{t-2} \right) + B M_{t-1} x_0 + \sum_{\tau = 0}^{t-2} B M_{\tau; t} w_{\tau} \\
%& = A^2 x_{t-2} + AB M_{t-2} x_0 + \sum_{\tau = 0}^{t-3} AB M_{\tau; t-2} w_{\tau} + B M_{t-1} x_0 + \sum_{\tau = 0}^{t-2} B M_{\tau; t} w_{\tau}
\end{align*} which is clearly linear with respect to the parameterization $\{M_{t}; M_{0;t},\dots M_{t-1;t}\}$. 

For $x_t$, we can show by induction that it is linear with respect to $\{M_{\tau}; M_{0;\tau},\dots M_{\tau-1;\tau}\}_{\tau = 0}^{t-1}$. Note that it is clearly true for $x_0$ which is independent of any parameterizations. Assume this claim holds for $x_t$ with $t \geq 0$, we now check $x_{t+1}$. We have:
\begin{align*}
x_{t+1} = A x_t + B u_t = A x_t + B M_t x_0 + \sum_{\tau = 0}^{t-1} BM_{\tau; t} w_{\tau}
\end{align*}
Note that by inductive hypothesis $x_t$ is linear with respect to $\{M_{\tau}; M_{0;\tau},\dots M_{\tau-1;\tau}\}_{\tau = 0}^{t-1}$, and the part $B M_t x_0 + \sum_{\tau = 0}^{t-1} BM_{\tau; t} w_{\tau}$ is clearly linear with respect to $\{ M_{t}; M_{0;t},\dots M_{t-1;t}\}$. Together, this implies that $x_{t+1}$ is linear with respect to $\{M_{\tau}; M_{0;\tau},\dots M_{\tau-1;\tau}\}_{\tau = 0}^{t}$, which concludes that for any $x_t$ with $t = 0, \dots, H-1$, we have $x_t$ is linear with respect to $\{M_{\tau}; M_{0;\tau},\dots M_{\tau-1;\tau}\}_{\tau =0}^{H-1}$.

Note that the trajectory total cost is $\sum_{t = 0}^{H-1} c(x_t, u_t)$.  Since $c_t$ is convex with respect to $x_t$ and $u_t$, and $x_t$ and $u_t$ are linear with respect to $\{M_{\tau}; M_{0;\tau},\dots M_{\tau-1;\tau}\}_{\tau = 0}^{H-1}$, we have that $\sum_{t = 0}^{H-1} c_t(x_t, u_t)$ is convex with respect to $\{M_{\tau}; M_{0;\tau},\dots M_{\tau-1;\tau}\}_{\tau = 0}^{H-1}$.

In last step we can simply take expectation with respect to $x_0$ and $w_1,\dots, w_{H-1}$. By linearity of expectation, we conclude the proof.
\end{proof}

The above immediately suggests that (sub) gradient-based algorithms such as projected gradient descent on parameters $\{ M_t, M_{0;t},\dots M_{t-1;t } \}_{t=0}^{H-1}$ can converge to the globally optimal solutions for any convex function $c_t$. Recall that the best linear controllers $\{-K^\star_t\}_{t=0}^{H-1}$ has its own corresponding parameterization $\{ M_t^\star, M_{0;t}^\star,\dots, M^\star_{t-1;t}\}_{t=0}^{H-1}$. Thus gradient-based optimization (with some care of the boundness of the parameters $\{ M_t^\star, M_{0;t}^\star,\dots, M^\star_{t-1;t}\}_{t=0}^{H-1}$) can find a solution that at least as good as the best linear controllers $\{-K^\star_t\}_{t=0}^{H-1}$.

\paragraph{Remark} The above claims easily extend to time-dependent transition and cost function, i.e., $A_t, B_t, c_t$ are time-dependent.  One can also extend it to episodic online control setting with adversarial noises $w_t$ and adversarial cost functions using no-regret online convex programming algorithms \cite{shalev2011online}. In episodic online control,  every episode $k$, an adversary determines a sequence of bounded noises $w^k_{0},\dots, w^k_{H-1}$, and cost function $c^k(x,u)$; the learner proposes a sequence of controllers $\widetilde\pi^k = \{ M_t^k, M_{0;t}^k,\dots, M^k_{t-1;t}\}_{t=0}^{H-1}$ and executes them (learner does not know the cost function $c^k$ until the end of the episode, and the noises are revealed in a way when she observes $x^k_t$ and calculates $w^k_{t-1}$ as $x^k_t - A x^k_{t-1} - B u^k_{t-1}$); at the end of the episode, learner observes $c^k$ and suffers total cost $\sum_{t=0}^{H-1} c^k(x^k_t,u^k_t)$.   The goal of the episodic online control is to be no-regret with respect to the best linear controllers in hindsight:
\begin{align*}
\sum_{k=0}^{K-1} \sum_{t=0}^{H-1} c^k(x^k_t, u^k_t) -  \min_{\{-K^\star_t\}_{t=0}^{H-1}} \sum_{k=0}^{K-1}  J^k( \{-K^\star_t\}_{t=0}^{H-1}) = o(K), 
\end{align*} where we denote $J^k( \{-K^\star_t\}_{t=0}^{H-1})$ as the total cost of executing $\{-K^\star_t\}_{t=0}^{H-1}$ in episodic $k$ under $c^k$ and noises $\{w^k_t\}$, i.e., $\sum_{t=0}^{H-1} c(x_t, u_t)$ with $u_t = -K^\star_t x_t$ and $x_{t+1} = A x_t + B u_t + w^k_t$, for all $t$. 


%for any differentiable convex cost functions $c_t$ for $t = 0,\dots, H-1$. 


\iffalse
We will focus on the following strong-stable linear policy $\pi = \{K_0, \dots, K_{H-1}\}$.  
\begin{definition}[$(\kappa,\gamma)$ stable controllers] Given a policy $\pi = \{K_0, \dots, K_{H-1}\}$, it is called $(\kappa, \gamma)$ stable controllers if for any $t$, we have that $(A + B K_t)$ can be written as $A+B K_t = H_t L_t$
\end{definition}
\fi




\section{Bibliographic Remarks and Further Readings}\label{bib:LQR}

The basics of optimal control theory can be found in any number of
standards text~\cite{Anderson:1990:OCL:79089,Evans2005,Bertsekas:DP}.
The primal and dual formulations for the continuous time LQR are
derived in~\citet{SDP-LQR}, while the dual formulation for
the discrete time LQR, that we us here, is
derived in~\citet{cohen2019learning}.

The treatment of Gauss-Newton, NPG, and PG algorithm are due
to~\citet{ICML-2018-Fazel0KM}. We have only provided the proof for the
Gauss-Newton case based; the proofs of convergence rates for NPG and
PG can be found in~\citet{ICML-2018-Fazel0KM}.

For many applications, the finite horizon LQR model is widely used as a model of locally linear
dynamics,
e.g. \cite{ahn2007iterative,Todorov04,tedrake2009lqr,perez2012lqr}. 
The issue of instabilities are largely due to model misspecification
and in the accuracy of the Taylor's expansion; it is less evident how
the infinite horizon LQR model captures these issues. In contrast,
for MDPs, practice tends to deal with stationary (and discounted)
MDPs, due that stationary policies are more convenient to represent
and learn; here, the non-stationary, finite horizon MDP model tends to be more of
theoretical interest, due to that it is straightforward (and often
more effective) to simply incorporate temporal information into the
state. Roughly, if our policy is parametric, then practical
representational constraints leads us to use stationary policies
(incorporating temporal information into the state), while if we tend
to use non-parametric policies (e.g. through some rollout based
procedure, say with ``on the fly'' computations like in model
predictive control, e.g. \cite{williams2017model}), then it is often more effective to work with
finite horizon, non-stationary models.

\paragraph{Sample Complexity and Regret for LQRs.} We have not treated the sample
complexity of learning in an LQR (see
~\cite{dean:2017,simchowitz2018learning,mania2019certainty} for rates). Here,
the basic analysis follows
from the online regression approach, which was developed in the study
of linear bandits~\cite{dani2008stochastic,abbasi2011improved}; in
particular, the self-normalized bound for vector-valued
martingales~\citep{abbasi2011improved} (see
Theorem~\ref{lemma:self_norm}) provides a direct means to obtain
sharp confidence intervals for estimating  the system matrices $A$ and
$B$ from data from a single trajectory (e.g. see~\cite{simchowitz2020naive}).
  
Another family of work provides regret analyses of online LQR
problems~\citep{abbasi2011regret,dean2018regret,mania2019certainty,cohen2019learning,simchowitz2020naive}. 
Here, naive random search suffices for sample efficient learning of
LQRs~\citep{simchowitz2020naive}. For the learning and control of more
complex nonlinear dynamical systems, one would expect this is
insufficient, where strategic
exploration is required for sample efficient learning; just as in 
the case for MDPs (e.g. the
UCB-VI algorithm).


\paragraph{Convex Parameterization of Linear Controllers}
The convex parameterization in Section~\ref{sec:convex_para_lqr} is based on \cite{agarwal2019online}  which is equivalent to the System Level Synthesis (SLS) parameterization \cite{wang2019system}. \citet{agarwal2019online} uses SLS parameterization in the infinite horizon online control setting and leverages a reduction to online learning with memory. Note that in episodic online control setting, we can just use classic no-regret online learner such as projected gradient descent \cite{zinkevich2003online}. For partial observable linear systems, the classic Youla  parameterization\cite{youla1976modern}  introduces a convex parameterization.  We also refer readers to \cite{simchowitz2020improper} for more detailed discussion about Youla parameterization and the generalization of Youla parameterization. Moreover, using the performance difference lemma, the exact optimal control policy for adversarial noise with full observations can be exactly characterized \citet{foster2020logarithmic,goel2020power} and yields a form reminiscent of the SLS parametrization \citet{wang2019system}.

%\citet{wang2019system} introduces the System Level Synthesis (SLS) which is equivalent to the parameterization used by \citet{agarwal2019online}. For partial observation setting, the Youla parameterization \citet{youla1976modern} yields a convex parameterization 

\chapter{Sample Complexity with a Generative Model}
\label{chap:generative}

This chapter begins our study of \emph{sample complexity} in tabular reinforcement learning.
We ask a minimax question: as a function of the problem dimensions $(|\Scal|,|\Acal|)$,
an accuracy target $\eps$, a failure probability $\delta$, and an effective horizon
(either $H$ in finite-horizon MDPs or $(1-\gamma)^{-1}$ in discounted MDPs),
how many transition samples are required to (i) estimate the optimal action-value function
$Q^\star$, or (ii) output a policy whose performance is $\eps$-close to optimal?

Throughout this chapter we work in the \emph{generative model} setting:
given any state--action pair $(s,a)$, we may query a simulator that returns an i.i.d.\
sample $s'\sim P(\cdot\mid s,a)$ together with the reward.  For clarity, we assume rewards
are deterministic; this assumption is often mild here because the main statistical
difficulty in the tabular setting is learning the transition model~$P$, and allowing
stochastic rewards only changes constants (and can be absorbed into the variance terms
that appear later).

Our first question is whether we must learn an accurate model of the world in order to act near-optimally.
The transition kernel has $|\Scal|^2|\Acal|$ parameters, so a naive ``learn-$P$-then-plan'' intuition
suggests that one might need on the order of $|\Scal|^2|\Acal|$ observed transitions.
One of the main takeaways of this chapter is that this intuition is overly pessimistic:
with a generative model, one can compute a near-optimal policy from a number of samples that is
\emph{sublinear} in the model size---scaling (up to horizon and logarithmic factors) like
$|\Scal||\Acal|$ rather than $|\Scal|^2|\Acal|$.

A second, equally important question is how to think about accuracy when the horizon is long.
In discounted problems, values naturally live on the scale $1/(1-\gamma)$, so an additive error
$\eps$ should be interpreted relative to this scale.  More importantly, small one-step errors can
compound over time: recall the matrix identity
\[
Q^\pi = (I-\gamma P^\pi)^{-1} r,
\]
which makes it clear that as $\gamma\uparrow 1$, the effective horizon grows and the mapping from
one-step quantities to long-run values becomes increasingly sensitive.
This raises a central question for the chapter: \emph{what is the correct dependence on the effective
horizon for obtaining a desired (relative) accuracy in $Q^\star$ or in the value of the learned policy?}
And can we avoid paying extra ``horizon amplification'' factors that arise from worst-case error
propagation arguments?

\section{Warmup: a naive model-based approach}

As a warmup, we analyze the most naive ``model-based'' strategy in the generative model setting.
We sample enough transitions to build an accurate empirical transition kernel $\widehat P$,
form the empirical MDP $\widehat M$ by replacing $P$ with $\widehat P$, and then plan in $\widehat M$.

Concretely, we query the simulator $N$ times for each state--action pair $(s,a)$.
Let $\mathrm{count}(s',s,a)$ be the number of times these $N$ samples transition to $s'$.
Define the empirical transition model
\[
\widehat P(s'\mid s,a) \;:=\; \frac{\mathrm{count}(s',s,a)}{N}.
\]
The total number of simulator calls is $|\Scal||\Acal|N$. As in Chapter~\ref{chap:prelims},
we will also view $P$ and $\widehat P$ as matrices of size $|\Scal||\Acal|\times|\Scal|$.

Since the transition kernel has $|\Scal|^2|\Acal|$ parameters, one might expect that
on the order of $|\Scal|^2|\Acal|$ observed transitions suffice to learn a globally accurate model.
The next proposition confirms this intuition, and shows that such global model accuracy implies
uniform accuracy of values (simultaneously for all policies), and hence near-optimal planning in $\widehat M$.

\begin{proposition}\label{prop:naive_model_based}
There exists an absolute constant $c$ such that the following holds.
Fix $\eps\in\big(0,\frac{1}{1-\gamma}\big)$ and $\delta\in(0,1)$, and suppose we collect
$N$ samples per state--action pair, with
\[
\textrm{\# samples from generative model } = |\Scal||\Acal|N
\;\ge\;
\frac{c}{(1-\gamma)^4}\cdot
\frac{|\Scal|^2|\Acal|\log\!\big(c|\Scal||\Acal|/\delta\big)}{\eps^2}.
\]
(Equivalently, $N \ge \frac{c|\Scal|\log(c|\Scal||\Acal|/\delta)}{(1-\gamma)^4\eps^2}$.)
Then, with probability at least $1-\delta$, the following hold:
\begin{itemize}
\item \textbf{(Model accuracy)} 
\[
\max_{s,a}\big\|P(\cdot\mid s,a)-\widehat P(\cdot\mid s,a)\big\|_1
\;\le\; (1-\gamma)^2\eps.
\]
\item \textbf{(Uniform value accuracy)} For all stationary policies $\pi$,
\[
\|Q^\pi-\widehat Q^\pi\|_\infty \;\le\; \eps.
\]
\item \textbf{(Near-optimal planning)} Let $\widehat\pi^\star$ be an optimal policy in $\widehat M$.
Then
\[
\|\widehat Q^\star-Q^\star\|_\infty \;\le\; \eps,
\qquad\text{and}\qquad
\|Q^{\widehat\pi^\star}-Q^\star\|_\infty \;\le\; 2\eps.
\]
\end{itemize}
\end{proposition}

Before proving Proposition~\ref{prop:naive_model_based}, we record two basic lemmas.

\begin{lemma}\label{lemma:simulation}
\textbf{(Simulation lemma)} For any stationary policy $\pi$,
\[
Q^\pi-\widehat Q^\pi
\;=\;
\gamma (I-\gamma \widehat P^\pi)^{-1}(P-\widehat P)\,V^\pi.
\]
\end{lemma}

\begin{proof}
Using $Q^\pi=(I-\gamma P^\pi)^{-1}r$ (Equation~\ref{eq:policy_value_matrix_form}) and the analogous identity
$\widehat Q^\pi=(I-\gamma \widehat P^\pi)^{-1}r$ in $\widehat M$, we have
\begin{align*}
Q^\pi-\widehat Q^\pi
&= (I-\gamma P^\pi)^{-1}r - (I-\gamma \widehat P^\pi)^{-1}r \\
&= (I-\gamma \widehat P^\pi)^{-1}\Big((I-\gamma \widehat P^\pi)-(I-\gamma P^\pi)\Big)(I-\gamma P^\pi)^{-1}r \\
&= \gamma (I-\gamma \widehat P^\pi)^{-1}(P^\pi-\widehat P^\pi)Q^\pi.
\end{align*}
Finally, $(P^\pi Q^\pi)(s,a)=\E_{s'\sim P(\cdot\mid s,a)}[V^\pi(s')]$ and similarly for $\widehat P$,
so $(P^\pi-\widehat P^\pi)Q^\pi=(P-\widehat P)V^\pi$, which completes the proof.
\end{proof}

\begin{lemma}\label{lemma:gamma_factor}
For any stationary policy $\pi$ and any vector $v\in\R^{|\Scal||\Acal|}$,
\[
\big\|(I-\gamma P^\pi)^{-1}v\big\|_\infty \;\le\; \frac{1}{1-\gamma}\|v\|_\infty.
\]
\end{lemma}

\begin{proof}
Let $w=(I-\gamma P^\pi)^{-1}v$, so $v=(I-\gamma P^\pi)w=w-\gamma P^\pi w$. Therefore,
\[
\|v\|_\infty \;=\; \|w-\gamma P^\pi w\|_\infty
\;\ge\; \|w\|_\infty - \gamma \|P^\pi w\|_\infty
\;\ge\; (1-\gamma)\|w\|_\infty,
\]
where $\|P^\pi w\|_\infty\le \|w\|_\infty$ uses that $P^\pi$ is row-stochastic.
Rearranging gives the claim.
\end{proof}

\begin{proof}(Proof of Proposition~\ref{prop:naive_model_based})
\paragraph{Step 1: model accuracy.}
Fix $(s,a)$. By Proposition~\ref{app:discrete} (applied with $d=|\Scal|$ and $N$ samples),
with probability at least $1-\delta'$, 
\[
\big\|P(\cdot\mid s,a)-\widehat P(\cdot\mid s,a)\big\|_1
\;\le\; c\sqrt{\frac{|\Scal|\log(c/\delta')}{N}}.
\]
Taking a union bound over all $|\Scal||\Acal|$ state--action pairs and setting $\delta'=\delta/(|\Scal||\Acal|)$ yields
\[
\max_{s,a}\big\|P(\cdot\mid s,a)-\widehat P(\cdot\mid s,a)\big\|_1
\;\le\; c\sqrt{\frac{|\Scal|\log(c|\Scal||\Acal|/\delta)}{N}}
\]
with probability at least $1-\delta$. Under the stated lower bound on $N$, the right-hand side is at most
$(1-\gamma)^2\eps$, proving the first bullet.

\paragraph{Step 2: uniform value accuracy.}
Fix any stationary policy $\pi$. By Lemma~\ref{lemma:simulation} and Lemma~\ref{lemma:gamma_factor},
\begin{align*}
\|Q^\pi-\widehat Q^\pi\|_\infty
&\le \gamma\big\|(I-\gamma \widehat P^\pi)^{-1}(P-\widehat P)V^\pi\big\|_\infty \\
&\le \frac{\gamma}{1-\gamma}\,\|(P-\widehat P)V^\pi\|_\infty.
\end{align*}
For each $(s,a)$, the quantity $\big[(P-\widehat P)V^\pi\big](s,a)$ is the difference of expectations of $V^\pi(s')$
under $P(\cdot\mid s,a)$ and $\widehat P(\cdot\mid s,a)$, hence
\[
\|(P-\widehat P)V^\pi\|_\infty
\le \Big(\max_{s,a}\|P(\cdot\mid s,a)-\widehat P(\cdot\mid s,a)\|_1\Big)\,\|V^\pi\|_\infty.
\]
Since rewards lie in $[0,1]$, we have $\|V^\pi\|_\infty\le \frac{1}{1-\gamma}$. Combining,
\[
\|Q^\pi-\widehat Q^\pi\|_\infty
\le \frac{\gamma}{(1-\gamma)^2}\max_{s,a}\|P(\cdot\mid s,a)-\widehat P(\cdot\mid s,a)\|_1
\le \frac{\gamma}{(1-\gamma)^2}\cdot (1-\gamma)^2\eps
\le \eps,
\]
which proves the second bullet (simultaneously for all $\pi$ on the event from Step 1).

\paragraph{Step 3: near-optimal planning.}
First, using $|\sup_x f(x)-\sup_x g(x)|\le \sup_x |f(x)-g(x)|$,
\[
|\widehat Q^\star(s,a)-Q^\star(s,a)|
=
\Big|\sup_\pi \widehat Q^\pi(s,a) - \sup_\pi Q^\pi(s,a)\Big|
\le \sup_\pi |\widehat Q^\pi(s,a)-Q^\pi(s,a)|
\le \eps.
\]
Taking the maximum over $(s,a)$ yields $\|\widehat Q^\star-Q^\star\|_\infty\le \eps$.

Finally, since $\widehat\pi^\star$ is optimal in $\widehat M$, we have $\widehat Q^{\widehat\pi^\star}=\widehat Q^\star$.
Therefore,
\[
\|Q^{\widehat\pi^\star}-Q^\star\|_\infty
\le \|Q^{\widehat\pi^\star}-\widehat Q^{\widehat\pi^\star}\|_\infty
+ \|\widehat Q^\star-Q^\star\|_\infty
\le \eps+\eps = 2\eps,
\]
where the first term used the uniform value accuracy bound from Step 2. This proves the third bullet.
\end{proof}

\paragraph{What our warmup does (and why it is not the full story).}
Proposition~\ref{prop:naive_model_based} formalizes a very strong guarantee:
by learning $\widehat P$ accurately everywhere, we obtain \emph{uniform} value accuracy,
meaning that $\widehat Q^\pi$ is close to $Q^\pi$ simultaneously for \emph{every} policy $\pi$.
This is more than is needed for planning---to act near-optimally, we only need to identify a single
near-optimal policy (or, equivalently, approximate $Q^\star$ well enough to recover one).
The uniform guarantee comes at a cost: it forces us to estimate each transition distribution in
$\ell_1$ to high accuracy, which leads to a sample requirement scaling like the number of parameters
in the transition model, $|\Scal|^2|\Acal|$.

The rest of the chapter asks what happens if we target only what planning requires.
With a generative model, it turns out that one can do substantially better:
the minimax number of samples needed to estimate $Q^\star$ (and to find a near-optimal policy)
scales, up to horizon and logarithmic factors, like $|\Scal||\Acal|$ rather than $|\Scal|^2|\Acal|$.
This is the first concrete sense in which, in reinforcement learning, we can sometimes ``act near-optimally
without learning the world model.''


\section{Sublinear Sample Complexity}

In the warmup, we obtained \emph{uniform} value accuracy for all policies, which effectively required
learning each transition distribution $P(\cdot\mid s,a)$ in $\ell_1$---hence the $|\Scal|^2|\Acal|$
scaling.  For planning, however, we do not need to approximate the value of \emph{every} policy.
Instead, it is natural to ask for guarantees tailored to the optimal value function $Q^\star$ (and to
a near-optimal policy).

Throughout this section we measure sample size in terms of
\[
\textrm{\# samples from generative model } = |\Scal||\Acal|\,N,
\]
where $N$ is the number of simulator calls per state--action pair.

We begin with a crude but instructive bound which already yields a \emph{sublinear} dependence on the
number of parameters in $P$.

\begin{proposition}\label{proposition:crude}
\textbf{(Crude value bounds)}
Fix $\delta\in(0,1)$. With probability at least $1-\delta$,
\begin{align*}
\|Q^\star-\widehat Q^\star\|_\infty &\le \Delta_{\delta,N},\\
\|Q^\star-\widehat Q^{\pi^\star}\|_\infty &\le \Delta_{\delta,N},
\end{align*}
where
\[
\Delta_{\delta,N}
:= \frac{\gamma}{(1-\gamma)^2}\sqrt{\frac{2\log(2|\Scal||\Acal|/\delta)}{N}}.
\]
\end{proposition}

The first inequality gives a sublinear rate for estimating $Q^\star$ itself.
To translate value-estimation error into a policy guarantee, a generic (and somewhat pessimistic)
route is to use the $Q$-error amplification lemma (Lemma~\ref{lemma:Q_to_policy}):
since $\widehat\pi^\star$ is greedy w.r.t.\ $\widehat Q^\star$ (ties broken deterministically),
\[
\|V^\star - V^{\widehat\pi^\star}\|_\infty
\;\le\; \frac{2}{1-\gamma}\,\|\widehat Q^\star - Q^\star\|_\infty
\;\le\; \frac{2}{1-\gamma}\,\Delta_{\delta,N}.
\]
Equivalently, using $Q^\pi(s,a)=r(s,a)+\gamma \E[V^\pi(s')\mid s,a]$, we also have
\[
\|Q^\star - Q^{\widehat\pi^\star}\|_\infty
\;\le\; \gamma \|V^\star - V^{\widehat\pi^\star}\|_\infty
\;\le\; \frac{2\gamma}{1-\gamma}\,\Delta_{\delta,N}.
\]

\paragraph{A note on ``horizon amplification'' from our second approach.}
The crude bound already shows sublinear dependence on the model size, but its dependence on the
effective horizon can be pessimistic.
From Proposition~\ref{proposition:crude}, achieving $\|Q^\star-\widehat Q^\star\|_\infty \le \eps$
requires
\[
N \;\gtrsim\; \frac{1}{(1-\gamma)^4}\cdot \frac{\log(|\Scal||\Acal|/\delta)}{\eps^2}.
\]
If we then translate estimation error into a policy guarantee using the generic $Q$-to-policy bound
(Lemma~\ref{lemma:Q_to_policy}), we lose an additional factor $1/(1-\gamma)$:
to make the value of $\widehat\pi^\star$ $\eps$-close to optimal, the same route suggests
\[
N \;\gtrsim\; \frac{1}{(1-\gamma)^6}\cdot \frac{\log(|\Scal||\Acal|/\delta)}{\eps^2}.
\]
This extra horizon cost is an artifact of worst-case error propagation, and one of the main goals of the sharper
analysis later in the chapter is to avoid it: value estimation and near-optimal planning end up having the same
effective-horizon dependence (up to constants and logarithmic factors).


We now provide the proof. We start with a planning-focused comparison
lemma, which will be useful throughout the chapter.

\begin{lemma}\label{lemma:component}
\textbf{(Component-wise bounds)}
Let $\pi^\star$ be an optimal policy in $M$, and let $\widehat\pi^\star$ be an optimal policy in $\widehat M$.
Then
\begin{align*}
Q^\star-\widehat Q^\star
&\le \gamma (I-\gamma \widehat P^{\pi^\star})^{-1}(P-\widehat P)\,V^\star,\\
Q^\star-\widehat Q^\star
&\ge \gamma (I-\gamma \widehat P^{\widehat\pi^\star})^{-1}(P-\widehat P)\,V^\star.
\end{align*}
\end{lemma}

\begin{proof}
Define the Bellman \emph{evaluation} operator in the empirical MDP $\widehat M$ for a deterministic
policy $\pi$ by
\[
(\widehat{\Tcal}^{\pi} Q)(s,a) := r(s,a) + \gamma \sum_{s'} \widehat P(s'\mid s,a)\, Q(s',\pi(s')).
\]
Its unique fixed point is $\widehat Q^\pi$.

Let us first prove the upper bound.
Since $\widehat\pi^\star$ is optimal in $\widehat M$, we have $\widehat Q^\star=\widehat Q^{\widehat\pi^\star}\ge \widehat Q^{\pi^\star}$,
and hence
\[
Q^\star-\widehat Q^\star \le Q^\star-\widehat Q^{\pi^\star}.
\]
Because $\widehat Q^{\pi^\star}$ is the fixed point of $\widehat{\Tcal}^{\pi^\star}$, we can write the exact identity
\[
Q^\star-\widehat Q^{\pi^\star}
= (I-\gamma \widehat P^{\pi^\star})^{-1}\big(Q^\star-\widehat{\Tcal}^{\pi^\star}Q^\star\big).
\]
Using $Q^\star=r+\gamma P V^\star$ and the fact that $\pi^\star$ is greedy w.r.t.\ $Q^\star$ (so $Q^\star(s,\pi^\star(s))=V^\star(s)$),
we have
\[
(\widehat{\Tcal}^{\pi^\star}Q^\star)(s,a)
= r(s,a) + \gamma \E_{s'\sim \widehat P(\cdot\mid s,a)}\!\left[Q^\star(s',\pi^\star(s'))\right]
= r(s,a) + \gamma \E_{s'\sim \widehat P(\cdot\mid s,a)}\!\left[V^\star(s')\right],
\]
and therefore
\[
Q^\star-\widehat{\Tcal}^{\pi^\star}Q^\star
= \gamma (P-\widehat P)\,V^\star.
\]
Combining yields the first inequality.

We now prove the lower bound.
Since $\widehat Q^\star=\widehat Q^{\widehat\pi^\star}$ is the fixed point of $\widehat{\Tcal}^{\widehat\pi^\star}$, we similarly have
\[
Q^\star-\widehat Q^\star
= (I-\gamma \widehat P^{\widehat\pi^\star})^{-1}\big(Q^\star-\widehat{\Tcal}^{\widehat\pi^\star}Q^\star\big).
\]
Now,
\begin{align*}
\big(Q^\star-\widehat{\Tcal}^{\widehat\pi^\star}Q^\star\big)(s,a)
&= \gamma \E_{s'\sim P(\cdot\mid s,a)}[V^\star(s')]
    - \gamma \E_{s'\sim \widehat P(\cdot\mid s,a)}[Q^\star(s',\widehat\pi^\star(s'))]\\
&\ge \gamma \E_{s'\sim P(\cdot\mid s,a)}[V^\star(s')]
    - \gamma \E_{s'\sim \widehat P(\cdot\mid s,a)}[V^\star(s')]\\
&= \gamma\big((P-\widehat P)V^\star\big)(s,a),
\end{align*}
where we used $Q^\star(s',\widehat\pi^\star(s'))\le \max_{a'}Q^\star(s',a')=V^\star(s')$.
Since $(I-\gamma \widehat P^{\widehat\pi^\star})^{-1}$ has nonnegative entries, multiplying preserves the inequality,
which gives the second claim.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{proposition:crude}]
First, by Lemma~\ref{lemma:simulation} (applied to $\pi^\star$) and Lemma~\ref{lemma:gamma_factor},
\[
\|Q^\star-\widehat Q^{\pi^\star}\|_\infty
= \|Q^{\pi^\star}-\widehat Q^{\pi^\star}\|_\infty
\le \frac{\gamma}{1-\gamma}\,\|(P-\widehat P)V^\star\|_\infty.
\]
Second, Lemma~\ref{lemma:component} implies that
\[
|Q^\star-\widehat Q^\star|
\le \gamma \max\!\left\{(I-\gamma \widehat P^{\pi^\star})^{-1},\,(I-\gamma \widehat P^{\widehat\pi^\star})^{-1}\right\}|(P-\widehat P)V^\star|,
\]
and therefore (using Lemma~\ref{lemma:gamma_factor} again),
\[
\|Q^\star-\widehat Q^\star\|_\infty
\le \frac{\gamma}{1-\gamma}\,\|(P-\widehat P)V^\star\|_\infty.
\]

It remains to bound $\|(P-\widehat P)V^\star\|_\infty$.
Fix $(s,a)$ and let $s'_1,\ldots,s'_N\sim P(\cdot\mid s,a)$ be the $N$ simulator samples used to form $\widehat P(\cdot\mid s,a)$.
Then $\E_{\widehat P}[V^\star(s')]=\frac1N\sum_{i=1}^N V^\star(s'_i)$ and $\E_{P}[V^\star(s')]$ is the true expectation.
Since $0\le V^\star\le \frac{1}{1-\gamma}$, Hoeffding's inequality gives
\[
\Pr\!\left(\left|\E_{\widehat P(\cdot\mid s,a)}[V^\star]-\E_{P(\cdot\mid s,a)}[V^\star]\right|
\ge \frac{1}{1-\gamma}\sqrt{\frac{2\log(2|\Scal||\Acal|/\delta)}{N}}\right)
\le \frac{\delta}{|\Scal||\Acal|}.
\]
Taking a union bound over all $(s,a)$ yields, with probability at least $1-\delta$,
\[
\|(P-\widehat P)V^\star\|_\infty
\le \frac{1}{1-\gamma}\sqrt{\frac{2\log(2|\Scal||\Acal|/\delta)}{N}}.
\]
Substituting into the previous displays proves both inequalities with the stated $\Delta_{\delta,N}$.
\end{proof}


\section{Minimax Optimal Sample Complexity (and the Model-Based Approach)}
\label{sec:minimax_generative}

We now state the minimax-optimal sample complexity (up to logarithmic factors) in the
generative model setting. The main message is that a simple
model-based approach --- estimate $\widehat P$
from samples and then plan in the empirical MDP --- achieves the optimal dependence on
$|\Scal||\Acal|$, $\eps$, and $(1-\gamma)$.

\subsection{The Discounted Case}

\paragraph{Upper bounds.}
The next theorem packages the main quantitative consequence of the refined analysis
from Section~\ref{sec:analysis} into a clean sample-complexity statement.

\begin{theorem}[Minimax upper bound: discounted case]
\label{thm:minimax_upper_discounted}
There exists an absolute constant $c$ such that the following holds.
Fix $\delta\in(0,1)$ and suppose we collect $N$ independent samples from the generative model
for each state--action pair, so that the total number of samples is $|\Scal||\Acal|N$.
Let $\widehat\pi^\star$ and $\widehat Q^\star$ denote an optimal policy and optimal $Q$-function
in the empirical MDP $\widehat M$.

\begin{itemize}
\item \textbf{(Value estimation)} If $\eps\le 1$ and
\[
|\Scal||\Acal|N \;\ge\; \frac{c\,|\Scal||\Acal|}{(1-\gamma)^3}\cdot
\frac{\log\!\big(c|\Scal||\Acal|/\delta\big)}{\eps^2},
\]
then with probability at least $1-\delta$,
\[
\|\widehat Q^\star - Q^\star\|_\infty \;\le\; \eps.
\]

\item \textbf{(Near-optimal planning)} If $\eps\le 1/\sqrt{1-\gamma}$ and
\[
|\Scal||\Acal|N \;\ge\; \frac{c\,|\Scal||\Acal|}{(1-\gamma)^3}\cdot
\frac{\log\!\big(c|\Scal||\Acal|/\delta\big)}{\eps^2},
\]
then with probability at least $1-\delta$,
\[
\|Q^{\widehat\pi^\star} - Q^\star\|_\infty \;\le\; \eps.
\]
\end{itemize}
\end{theorem}

We only prove the \emph{value estimation} guarantee (the first bullet) in
Theorem~\ref{thm:minimax_upper_discounted}; the \emph{planning} guarantee is more delicate.
A black-box route using Lemma~\ref{lemma:Q_to_policy} would yield an additional $1/(1-\gamma)$
loss (the ``horizon amplification'' phenomenon discussed earlier). The second bullet shows that,
with a sharper (variance-sensitive) argument, this amplification can be avoided.
See Section~\ref{bib:sample_complexity} for further discussion, including results that improve the
technical ``burn-in'' restriction on $\eps$ using different algorithms.

\paragraph{Lower bounds.}
We next state a matching minimax lower bound (again, up to logarithmic factors).
We say that an estimator $\mathcal{A}$ is \emph{$(\eps,\delta)$-good} on an MDP $M$ if,
given samples from the generative model, it outputs $\widehat Q^\star$ satisfying
$\|\widehat Q^\star - Q^\star\|_\infty \le \eps$ with probability at least $1-\delta$.

\begin{theorem}[Minimax lower bound: discounted case]
\label{thm:minimax_lower_discounted}
There exist absolute constants $\eps_0\in(0,1)$, $\delta_0\in(0,1)$, and $c>0$ and a class of MDPs
$\mathcal{M}$ such that for all $\eps\in(0,\eps_0)$ and $\delta\in(0,\delta_0)$, any algorithm
$\mathcal{A}$ that is $(\eps,\delta)$-good on every $M\in\mathcal{M}$ must use at least
\[
\textrm{\# samples from generative model}
\;\ge\;
\frac{c\,|\Scal||\Acal|}{(1-\gamma)^3}\cdot \frac{\log(c/\delta)}{\eps^2}.
\]
\end{theorem}

In other words, the model-based approach is minimax-optimal in its dependence on
$|\Scal||\Acal|$, $(1-\gamma)$, and $\eps$ (up to logarithmic factors).

\paragraph{Interpreting the horizon dependence.}
It is useful to calibrate the target accuracy in discounted problems.
Since rewards are bounded in $[0,1]$, we have $\|Q^\star\|_\infty \leq
\frac{1}{1-\gamma}$, so an additive error of size $\frac{\eps}{1-\gamma}$ corresponds to \emph{relative} accuracy on the scale of $\eps$.
Equivalently, if we define a normalized value function $\bar Q^\pi := (1-\gamma)Q^\pi$, then $\|\bar Q^\pi\|_\infty\le 1$ and asking for additive error $\eps$ in $\bar Q$ is the same as asking for additive error $\eps/(1-\gamma)$ in $Q$.

Plugging this normalization into Theorem~\ref{thm:minimax_upper_discounted} (and ignoring logarithmic factors and the mild technical restrictions on $\eps$), the minimax sample complexity for constant-accuracy estimation of $\bar Q^\star$ scales like
\[
\textrm{\# samples} \;\approx\; \frac{|\Scal||\Acal|}{1-\gamma}\cdot \frac{1}{\eps^2}.
\]
In this sense, after normalization, the dependence on the effective horizon is essentially linear: the extra powers of $(1-\gamma)^{-1}$ in the unnormalized bounds are largely accounting for the fact that $Q^\star$ itself grows like $(1-\gamma)^{-1}$.

\subsection{Finite-Horizon Setting}
\label{sec:finite_horizon_sc}

Recall the finite-horizon (time-dependent) MDP model from
Section~\ref{section:finite_horizon}, with horizon $H$ and transition kernels
$\{P_h\}_{h=0}^{H-1}$.
In the generative-model setting, we can query any triple $(s,a,h)$ and obtain an
independent sample $s' \sim P_h(\cdot \mid s,a)$ (and the reward $r_h(s,a)$, which we treat as deterministic for simplicity).

We consider the natural plug-in model-based estimator: for each $(s,a,h)$ we draw
$N$ independent samples and form the empirical transition kernel $\widehat P_h(\cdot\mid s,a)$.
The total number of samples is therefore
\[
\textrm{\# samples from generative model } \;=\; H\,|\Scal|\,|\Acal|\,N .
\]
We state the resulting performance guarantees without proof.

\paragraph{Upper bounds.}
The following theorem gives a high-probability error bound for planning in the empirical MDP.

\begin{theorem}[Model-based planning: finite horizon]
\label{thm:sample_complexity_finite}
There exists an absolute constant $c$ such that the following holds.
For any $\delta\in(0,1)$, with probability at least $1-\delta$,
\begin{itemize}
\item \textbf{(Value estimation)} 
\[
\|Q_0^\star-\widehat Q_0^\star\|_\infty \;\le\;
cH \sqrt{\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
\;+\;
cH\frac{\log(c|\Scal||\Acal|/\delta)}{N}\, .
\]
\item \textbf{(Sub-optimality)} letting $\widehat \pi^\star$ be an optimal policy in the empirical MDP,
\[
\|Q_0^\star-Q_0^{\widehat \pi^\star}\|_\infty \;\le\;
cH \sqrt{\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
\;+\;
cH\frac{\log(c|\Scal||\Acal|/\delta)}{N}\, .
\]
\end{itemize}
\end{theorem}

Equivalently, ignoring lower-order terms, achieving $\|Q_0^\star-\widehat Q_0^\star\|_\infty \le \eps$
(or $\|Q_0^\star-Q_0^{\widehat\pi^\star}\|_\infty\le \eps$) is guaranteed once
\[
N \;\gtrsim\; \frac{H^2}{\eps^2}\,\log\!\Big(\frac{|\Scal||\Acal|}{\delta}\Big),
\qquad\text{or equivalently}\qquad
\textrm{\# samples from generative model } \;\gtrsim\; \frac{H^3|\Scal||\Acal|}{\eps^2}\,\log\!\Big(\frac{|\Scal||\Acal|}{\delta}\Big).
\]
The dependence on $H$ looks different from the discounted case, but it is not directly comparable without specifying how one maps $\gamma$ to an effective horizon. One rough correspondence is $H \asymp \frac{1}{1-\gamma}$; under this heuristic, the leading-order scalings match up to constants and logarithmic factors.

\paragraph{Lower bounds.}
In the same minimax sense as Theorem~\ref{thm:lower_sample_complexity}, the above model-based rate is optimal (up to constant and logarithmic factors) for finite-horizon MDPs under a generative model. We omit the proof.


\section{Analysis}\label{sec:analysis}

We now prove the \emph{value-estimation} guarantee in Theorem~\ref{thm:sample_complexity}
(i.e.\ the first bullet). For convenience, we restate it here.

\begin{theorem}[Value estimation; restatement of Theorem~\ref{thm:sample_complexity}]
\label{thm:sample_complexity_value_est}
Fix $\delta\in(0,1)$. For an appropriately chosen absolute constant $c$, with probability at least $1-\delta$,
\[
\|Q^\star-\widehat Q^\star\|_\infty \leq
\gamma\sqrt{\frac{c}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
+\frac{c \gamma}{(1-\gamma)^3}
\frac{\log(c|\Scal||\Acal|/\delta)}{N}\, .
\]
\end{theorem}

\subsection{Variance lemmas}

The sharper dependence on $(1-\gamma)$ comes from replacing a worst-case
(Hoeffding-style) deviation bound by a variance-sensitive one (Bernstein),
and then exploiting a Bellman-type recursion for variances.

\paragraph{Variance notation.}
For a real-valued function $f$ and distribution $\mathcal{D}$, define
\[
\Var_{\mathcal{D}}(f) := \E_{x\sim \mathcal{D}}[f(x)^2] - \Big(\E_{x\sim \mathcal{D}}[f(x)]\Big)^2.
\]
For a vector $V\in \R^{|\Scal|}$, define $\Var_{P}(V)\in \R^{|\Scal||\Acal|}$ componentwise by
\[
\Var_{P}(V)(s,a) := \Var_{P(\cdot\mid s,a)}(V)
= \E_{s'\sim P(\cdot\mid s,a)}[V(s')^2] - \Big(\E_{s'\sim P(\cdot\mid s,a)}[V(s')]\Big)^2.
\]
Equivalently (with squares understood componentwise),
\[
\Var_{P}(V) = P(V^2) - (PV)^2.
\]

\paragraph{A Bernstein deviation bound.}
We will repeatedly apply Bernstein's inequality to the empirical estimate $\widehat P(\cdot\mid s,a)$.

\begin{lemma}\label{lemma:bernstein}
Let $\delta\in(0,1)$. With probability at least $1-\delta$, the following holds
\emph{simultaneously} for all $(s,a)$:
\[
\big|(P-\widehat P) V^\star\big|
\leq \sqrt{\frac{ 2\log(2|\Scal||\Acal|/\delta)}{N}} \;\sqrt{\Var_{P}(V^\star)}
+\frac{1}{1-\gamma}\frac{2\log(2|\Scal||\Acal|/\delta)}{3N}\,\mathds{1}\, .
\]
Here the absolute value and inequality are componentwise, and $\mathds{1}$ denotes the all-ones vector in $\R^{|\Scal||\Acal|}$.
\end{lemma}

\begin{proof}
Fix $(s,a)$. Apply Bernstein's inequality to the i.i.d.\ random variables
$V^\star(s'_1),\ldots,V^\star(s'_N)$ with $s'_i\sim P(\cdot\mid s,a)$.
Since $0\le V^\star\le \frac{1}{1-\gamma}$, the range is at most $\frac{1}{1-\gamma}$, and the variance term is exactly $\Var_{P(\cdot\mid s,a)}(V^\star)$.
A union bound over $|\Scal||\Acal|$ state-action pairs yields the stated form.
\end{proof}

The remaining work is to control how the (square-root) variance term propagates through the inverse Bellman operator, e.g.\
$\|(I-\gamma \widehat P^{\pi^\star})^{-1}\sqrt{\Var_{P}(V^\star)}\|_\infty$.

\paragraph{Variance of the discounted return.}
For any MDP $M$ and policy $\pi$, define $\Sigma^\pi_M(s,a)$ as the conditional variance of the
discounted return:
\[
\Sigma^\pi_M(s,a)
:= \E\!\left[\left(\sum_{t=0}^\infty \gamma^t r(s_t,a_t)-Q^\pi_M(s,a)\right)^2
\;\middle|\; s_0=s,\;a_0=a\right],
\]
where the expectation is over trajectories generated by $\pi$ in $M$.
Because we assume the one-step reward $r(s,a)$ is deterministic, all randomness in the return comes from future states, hence
\[
\|\Sigma^\pi_M\|_\infty \;\le\; \left(\frac{\gamma}{1-\gamma}\right)^2 \;=\; \frac{\gamma^2}{(1-\gamma)^2}.
\]

The next lemma is the variance analogue of Bellman consistency.

\begin{lemma}[Bellman consistency of $\Sigma$]\label{eq:bellman_var}
For any MDP $M$ and policy $\pi$,
\begin{equation}\label{eq:bellman_var}
\Sigma^\pi_M \;=\; \gamma^2 \Var_{P}\!\big(V^{\pi}_M\big) \;+\; \gamma^2 P^\pi \Sigma^\pi_M,
\end{equation}
where $P$ is the transition model of $M$.
\end{lemma}

\noindent The proof is left as an exercise to the reader (it follows by writing the return as
$r(s,a)+\gamma(\text{future return})$ and applying the law of total variance).

\paragraph{A weighted sum-of-deviations bound.}
We now prove a bound that turns square-root variances into the desired $(1-\gamma)^{-3/2}$ scaling.

\begin{lemma}\label{lemma:var_add}
(Weighted sum of deviations)
For any policy $\pi$ and MDP $M$,
\[
\Big\| (I-\gamma P^\pi)^{-1}\sqrt{\Var_{P}(V^{\pi}_M)}\Big\|_\infty
\;\leq\; \sqrt{\frac{2}{(1-\gamma)^3}}\, ,
\]
where $P$ is the transition model of $M$.
\end{lemma}

\begin{proof}
Let $v := \Var_{P}(V^{\pi}_M)\in\R^{|\Scal||\Acal|}$, which is entrywise nonnegative.
Observe that $(1-\gamma)(I-\gamma P^\pi)^{-1}$ has nonnegative entries and each row sums to $1$,
so each row is a probability distribution over next state-action pairs.

Applying Jensen's inequality row-by-row (using concavity of $\sqrt{\cdot}$) gives
\[
\|(I-\gamma P^\pi)^{-1}\sqrt{v}\|_\infty
= \frac{1}{1-\gamma}\|(1-\gamma)(I-\gamma P^\pi)^{-1}\sqrt{v}\|_\infty
\le \sqrt{\Big\|\frac{1}{1-\gamma}(I-\gamma P^\pi)^{-1}v\Big\|_\infty}.
\]

Next we relate $(I-\gamma P^\pi)^{-1}$ to $(I-\gamma^2 P^\pi)^{-1}$:
for any entrywise nonnegative $u$,
\[
\|(I-\gamma P^\pi)^{-1}u\|_\infty \;\le\; 2\|(I-\gamma^2 P^\pi)^{-1}u\|_\infty.
\]
(We verify this inequality at the end of the proof.)
Applying this with $u=v$ yields
\[
\Big\|\frac{1}{1-\gamma}(I-\gamma P^\pi)^{-1}v\Big\|_\infty
\le \Big\|\frac{2}{1-\gamma}(I-\gamma^2 P^\pi)^{-1}v\Big\|_\infty.
\]

Finally, using the Bellman consistency equation~\eqref{eq:bellman_var} we have
$\Sigma^\pi_M = \gamma^2 (I-\gamma^2 P^\pi)^{-1} v$, hence
\[
(I-\gamma^2 P^\pi)^{-1}v = \frac{1}{\gamma^2}\Sigma^\pi_M,
\qquad\text{so}\qquad
\Big\|\frac{2}{1-\gamma}(I-\gamma^2 P^\pi)^{-1}v\Big\|_\infty
\le \frac{2}{1-\gamma}\cdot \frac{1}{\gamma^2}\|\Sigma^\pi_M\|_\infty
\le \frac{2}{(1-\gamma)^3}.
\]
Taking square roots completes the proof.

\smallskip
\noindent\emph{Proof of $\|(I-\gamma P^\pi)^{-1}u\|_\infty \le 2\|(I-\gamma^2 P^\pi)^{-1}u\|_\infty$.}
Let $w=(I-\gamma^2 P^\pi)^{-1}u$. Then
\[
(I-\gamma P^\pi)^{-1}u = (I-\gamma P^\pi)^{-1}(I-\gamma^2 P^\pi)w
= (I-\gamma P^\pi)^{-1}\big((1-\gamma)I+\gamma(I-\gamma P^\pi)\big)w
= (1-\gamma)(I-\gamma P^\pi)^{-1}w + \gamma w.
\]
Taking $\|\cdot\|_\infty$ norms and using Lemma~\ref{lemma:gamma_factor} on the first term gives
\[
\|(I-\gamma P^\pi)^{-1}u\|_\infty \le (1-\gamma)\cdot \frac{1}{1-\gamma}\|w\|_\infty + \gamma\|w\|_\infty
\le 2\|w\|_\infty = 2\|(I-\gamma^2 P^\pi)^{-1}u\|_\infty,
\]
as claimed.
\end{proof}

\subsection{Completing the proof}

The next lemma relates the (unknown) one-step variance under $P$ to an
empirical variance under $\widehat P$, up to lower-order estimation
terms.

\begin{lemma}\label{lemma:var_bound}
Fix $\delta\in(0,1)$. With probability at least $1-\delta$, we have the componentwise bounds
\begin{align*}
\Var_{P}(V^\star)
&\leq 2\,\Var_{\widehat P}(\widehat V^{\pi^\star}) \;+\;
\Delta^\prime_{\delta,N}\,\mathds{1},\\
\Var_{P}(V^\star)
&\leq 2\,\Var_{\widehat P}(\widehat V^\star) \;+\;
\Delta^\prime_{\delta,N}\,\mathds{1},
\end{align*}
where
\[
\Delta^\prime_{\delta,N}
:= \frac{1}{(1-\gamma)^2}\sqrt{\frac{18\log(6|\Scal||\Acal|/\delta)}{N}}
+\frac{1}{(1-\gamma)^4}\frac{4\log(6|\Scal||\Acal|/\delta)}{N}\, .
\]
\end{lemma}

\begin{proof}
We work componentwise in $(s,a)$, and write $(PV^\star)(s,a)=\E_{s'\sim P(\cdot\mid s,a)}[V^\star(s')]$.
By definition,
\begin{align*}
\Var_P(V^\star)
&= P\!\big((V^\star)^2\big) - (PV^\star)^2\\
&= \widehat P\!\big((V^\star)^2\big) - (\widehat P V^\star)^2
\;+\; (P-\widehat P)\!\big((V^\star)^2\big) \;-\;\Big((PV^\star)^2-(\widehat P V^\star)^2\Big)\\
&= \Var_{\widehat P}(V^\star)
\;+\; (P-\widehat P)\!\big((V^\star)^2\big)
\;-\;\Big((PV^\star)^2-(\widehat P V^\star)^2\Big).
\end{align*}

We bound the last two terms via Hoeffding + a union bound over $(s,a)$.

We now handle term 1: $(P-\widehat P)\big((V^\star)^2\big)$.
Since $0\le V^\star\le \frac{1}{1-\gamma}$, we have $0\le (V^\star)^2\le \frac{1}{(1-\gamma)^2}$.
Thus with probability at least $1-\delta/3$,
\[
\big\|(P-\widehat P)\big((V^\star)^2\big)\big\|_\infty
\le \frac{1}{(1-\gamma)^2}\sqrt{\frac{2\log(6|\Scal||\Acal|/\delta)}{N}}.
\]

Next, we handle term 2: $(PV^\star)^2-(\widehat P V^\star)^2$.
Using $(x^2-y^2)=(x+y)(x-y)$ componentwise,
\begin{align*}
\big\|(PV^\star)^2-(\widehat P V^\star)^2\big\|_\infty
&\le \|PV^\star+\widehat P V^\star\|_\infty \cdot \|PV^\star-\widehat P V^\star\|_\infty\\
&\le \frac{2}{1-\gamma}\cdot \|(P-\widehat P)V^\star\|_\infty.
\end{align*}
Again $0\le V^\star\le \frac{1}{1-\gamma}$, so by Hoeffding + union bound, with probability at least $1-\delta/3$,
\[
\|(P-\widehat P)V^\star\|_\infty
\le \frac{1}{1-\gamma}\sqrt{\frac{2\log(6|\Scal||\Acal|/\delta)}{N}}.
\]
Combining gives, with probability at least $1-\delta/3$,
\[
\big\|(PV^\star)^2-(\widehat P V^\star)^2\big\|_\infty
\le \frac{2}{(1-\gamma)^2}\sqrt{\frac{2\log(6|\Scal||\Acal|/\delta)}{N}}.
\]

We now handle term 3: $\Var_{\widehat P}(V^\star)$.
We compare $\Var_{\widehat P}(V^\star)$ to an empirical variance of a nearby value function.
Using $\Var(X+Y)\le 2\Var(X)+2\Var(Y)$ and $\Var_{\widehat P}(f)\le \|f\|_\infty^2$,
\begin{align*}
\Var_{\widehat P}(V^\star)
&=\Var_{\widehat P}\!\big((V^\star-\widehat V^{\pi^\star})+\widehat V^{\pi^\star}\big)\\
&\le 2\,\Var_{\widehat P}(V^\star-\widehat V^{\pi^\star})
+2\,\Var_{\widehat P}(\widehat V^{\pi^\star})\\
&\le 2\|V^\star-\widehat V^{\pi^\star}\|_\infty^2
+2\,\Var_{\widehat P}(\widehat V^{\pi^\star}).
\end{align*}
Since $\pi^\star$ is optimal in $M$, we have $V^\star(s)=Q^\star(s,\pi^\star(s))$ and
$\widehat V^{\pi^\star}(s)=\widehat Q^{\pi^\star}(s,\pi^\star(s))$, hence
$\|V^\star-\widehat V^{\pi^\star}\|_\infty\le \|Q^\star-\widehat Q^{\pi^\star}\|_\infty$.
Applying Proposition~\ref{proposition:crude} (with failure probability $\delta/3$) gives
\[
\|V^\star-\widehat V^{\pi^\star}\|_\infty
\le \Delta_{\delta/3,N}
\quad\Rightarrow\quad
2\|V^\star-\widehat V^{\pi^\star}\|_\infty^2
\le \frac{4}{(1-\gamma)^4}\frac{\log(6|\Scal||\Acal|/\delta)}{N},
\]
where we used $\gamma\le 1$ and the definition of $\Delta_{\delta,N}$.

\paragraph{Putting it together.}
With probability at least $1-\delta$ (union bound over the three events),
\[
\Var_P(V^\star)
\le 2\,\Var_{\widehat P}(\widehat V^{\pi^\star})
+\left(\frac{1}{(1-\gamma)^2}\sqrt{\frac{18\log(6|\Scal||\Acal|/\delta)}{N}}
+\frac{1}{(1-\gamma)^4}\frac{4\log(6|\Scal||\Acal|/\delta)}{N}\right)\mathds{1},
\]
which is the first claim.

For the second claim, repeat the same argument but decompose
$\Var_{\widehat P}(V^\star)=\Var_{\widehat P}((V^\star-\widehat V^\star)+\widehat V^\star)$
and use $\|V^\star-\widehat V^\star\|_\infty\le \|Q^\star-\widehat Q^\star\|_\infty\le \Delta_{\delta/3,N}$.
\end{proof}

\medskip

Combining the Bernstein deviation bound with the variance comparison above yields a
variance-sensitive control of $(P-\widehat P)V^\star$.

\begin{corollary}\label{corollary:one}
Fix $\delta\in(0,1)$. With probability at least $1-\delta$, we have the componentwise bounds
\begin{align*}
|(P-\widehat P) V^\star|
&\leq c\sqrt{\frac{\Var_{\widehat P}(\widehat V^{\pi^\star})\log(c|\Scal||\Acal|/\delta)}{N}}
+ \Delta^{\prime\prime}_{\delta,N}\,\mathds{1},\\
|(P-\widehat P) V^\star|
&\leq c\sqrt{\frac{\Var_{\widehat P}(\widehat V^\star)\log(c|\Scal||\Acal|/\delta)}{N}}
+ \Delta^{\prime\prime}_{\delta,N}\,\mathds{1},
\end{align*}
where
\[
\Delta^{\prime\prime}_{\delta,N}
:=
c \frac{1}{1-\gamma}\left(\frac{\log(c|\Scal||\Acal|/\delta)}{N}\right)^{3/4}
+\frac{c}{(1-\gamma)^2}\frac{\log(c|\Scal||\Acal|/\delta)}{N},
\]
and $c$ is an absolute constant.
\end{corollary}

\begin{proof}
Apply Lemma~\ref{lemma:bernstein} and substitute the upper bound on $\Var_P(V^\star)$ from Lemma~\ref{lemma:var_bound}.
Then use $\sqrt{a+b}\le \sqrt a+\sqrt b$ componentwise to separate the empirical-variance term from $\Delta'_{\delta,N}$.
The term involving $\sqrt{\Delta'_{\delta,N}}$ contributes the $(\log/N)^{3/4}$ and $(\log/N)$ pieces, and the additive
Bernstein remainder $\frac{1}{1-\gamma}\frac{\log(\cdot)}{N}\mathds{1}$ is absorbed into the displayed $\Delta''_{\delta,N}$ by adjusting constants.
\end{proof}

\medskip

\begin{proof}[Proof of Theorem~\ref{thm:sample_complexity}]
We bound the two sides in Lemma~\ref{lemma:component}.

We first handle the upper deviation.
Using the first inequality in Lemma~\ref{lemma:component}, monotonicity of $(I-\gamma \widehat P^{\pi^\star})^{-1}$,
and Corollary~\ref{corollary:one} (first line), we obtain
\begin{align*}
\|Q^\star-\widehat Q^\star\|_\infty
&\le \gamma\Big\|(I-\gamma \widehat P^{\pi^\star})^{-1}\,\big|(P-\widehat P)V^\star\big|\Big\|_\infty\\
&\le c\gamma\sqrt{\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
\Big\|(I-\gamma \widehat P^{\pi^\star})^{-1}\sqrt{\Var_{\widehat P}(\widehat V^{\pi^\star})}\Big\|_\infty
\;+\;\gamma\,\Delta''_{\delta,N}\Big\|(I-\gamma \widehat P^{\pi^\star})^{-1}\mathds{1}\Big\|_\infty\\
&\le c\gamma\sqrt{\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
\Big\|(I-\gamma \widehat P^{\pi^\star})^{-1}\sqrt{\Var_{\widehat P}(\widehat V^{\pi^\star})}\Big\|_\infty
\;+\;\frac{c\gamma}{(1-\gamma)^2}\left(\frac{\log(c|\Scal||\Acal|/\delta)}{N}\right)^{3/4}
+\frac{c\gamma}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{N},
\end{align*}
where we used $\|(I-\gamma \widehat P^{\pi^\star})^{-1}\mathds{1}\|_\infty\le \frac{1}{1-\gamma}$.

Now apply Lemma~\ref{lemma:var_add} to $\widehat M$ and policy $\pi^\star$ (so $P=\widehat P$ and $V^\pi_M=\widehat V^{\pi^\star}$) to get
\[
\Big\|(I-\gamma \widehat P^{\pi^\star})^{-1}\sqrt{\Var_{\widehat P}(\widehat V^{\pi^\star})}\Big\|_\infty
\le \sqrt{\frac{2}{(1-\gamma)^3}}.
\]
Substituting yields
\begin{align*}
\|Q^\star-\widehat Q^\star\|_\infty
&\le
c\gamma\sqrt{\frac{2}{(1-\gamma)^3}}\sqrt{\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
+\frac{c\gamma}{(1-\gamma)^2}\left(\frac{\log(c|\Scal||\Acal|/\delta)}{N}\right)^{3/4}
+\frac{c\gamma}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{N}.
\end{align*}

The middle term can be absorbed into the other two terms using $2ab\le a^2+b^2$.
Indeed, letting $L:=\frac{\log(c|\Scal||\Acal|/\delta)}{N}$,
\[
\frac{1}{(1-\gamma)^2}L^{3/4}
=\left(\frac{1}{(1-\gamma)^{3/2}}L^{1/2}\right)\left(\frac{1}{(1-\gamma)^{1/2}}L^{1/4}\right)
\le \frac{1}{(1-\gamma)^3}L + \frac{1}{1-\gamma}L^{1/2}
\le \frac{1}{(1-\gamma)^3}L + \frac{1}{(1-\gamma)^{3/2}}L^{1/2},
\]
and adjusting constants gives the claimed form.

We now handle the lower deviation.
Repeat the same argument using the second inequality in Lemma~\ref{lemma:component}
(i.e.\ with $\widehat\pi^\star$ in place of $\pi^\star$), which yields an identical bound for
$\|\widehat Q^\star-Q^\star\|_\infty$.
Combining the two sides gives the stated $\ell_\infty$ error bound (with a possibly different absolute constant $c$).
\end{proof}




\section{Bibliographic Remarks and Further Readings}\label{bib:sample_complexity}

The notion of a generative model was first introduced
in~\cite{kearns1999finite}, which made the argument that, up to
horizon factors and logarithmic factors, both model based methods and
model free methods are comparable. \cite{kakade2003sample} gave an improved version of this rate
(analogous to the crude bounds seen here). 

The first claim in Theorem~\ref{thm:sample_complexity} is due to ~\cite{azar2013minimax},
and the proof in this section largely follows this work. Improvements
are possible with regards to bounding the quality of  $\widehat
\pi^\star$; here, Theorem~\ref{thm:sample_complexity} shows that 
the model based approach is near optimal even for policy itself;
showing that the quality of $\widehat
\pi^\star$ does suffer any amplification factor of
$1/(1-\gamma)$.  \cite{sidford2018vnear} provides the first proof of
this improvement using a
variance reduction algorithm with value iteration. The second claim in Theorem~\ref{thm:sample_complexity} is due
to~\cite{pmlr-v125-agarwal20b}, which shows that 
the naive model based approach is sufficient. The lower bound in
Theorem~\ref{thm:lower_sample_complexity} is due to ~\cite{azar2013minimax}.

We also remark that we may hope for the sub-optimality bounds (on the value
of the argmax policy) to hold up to for ``large'' $\eps$, i.e. up to
$\eps\leq 1/(1-\gamma)$ (see the second claim in Theorem~\ref{thm:sample_complexity}).
 Here, the work in
~\cite{li2020breaking} shows this limit is achievable, albeit
with a slightly different algorithm where they introduce
perturbations. It is currently an open question if the naive model based
approach also achieves the non-asymptotic statistical limit.

This chapter also provided results, without proof, on the optimal sample complexity in
the finite horizon setting (see
Section~\ref{sec:finite_horizon_sc}). The proof of this claim
would also follow from the line of reasoning  in~\cite{azar2013minimax}, with the added
simplification that the sub-optimality analysis is simpler in the
finite-horizon setting with time-dependent transition matrices
(e.g. see~\cite{Yin2021NearOP}).


\newpage
\newpage
\newpage
\newpage
\newpage
\newpage
\newpage

\section{Minmax Optimal Sample Complexity (and the Model Based Approach)}

We now see that the model based approach is minmax optimal, for both
the discounted case and the finite horizon setting. 

\subsection{The Discounted Case}

\paragraph{Upper bounds.} The following theorem refines our crude bound on $\widehat Q^\star$.

\begin{theorem}\label{thm:sample_complexity}
For $\delta\geq 0$ and for an appropriately chosen absolute constant  $c$, we have that:
\begin{itemize}
\item (Value estimation) With probability greater than $1-\delta$,
\[
\|Q^\star-\widehat Q^\star\|_\infty \leq
\gamma\sqrt{\frac{c}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
+\frac{c \gamma}{(1-\gamma)^3}
\frac{\log(c|\Scal||\Acal|/\delta)}{N}\, .
\]
\item (Sub-optimality) If $N\geq \frac{1}{(1-\gamma)^2}$,
then with probability greater than $1-\delta$, 
\[
\|Q^\star-Q^{\widehat \pi^\star}\|_\infty \leq
\gamma\sqrt{\frac{c}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{N}}.
\]
\end{itemize}
\end{theorem}

This immediately provides the following corollary.
\begin{corollary}\label{corollary:eps}
Provided that $\eps\leq 1$ and that
\[
%N\geq \frac{c}{(1-\gamma)^3}\frac{|\Scal||\Acal|\log(c|\Scal||\Acal|/\delta)}{\eps^2} ,
%N\geq \frac{c}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{\eps^2} ,
\textrm{\# samples from generative model }  = |\Scal||\Acal| N\geq
\frac{c |\Scal||\Acal|}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{\eps^2} ,
\]
then with probability greater than $1-\delta$, 
\[
\|Q^\star-\widehat Q^\star\|_\infty \leq \eps.
\]
%Note that this implies $\|Q^\star-Q^{\widehat \pi^\star}\|_\infty
%\leq \eps/(1-\gamma).$ 
Furthermore, 
provided that $\eps\leq \sqrt{\frac{1}{1-\gamma}}$ and that
\[
%N\geq \frac{c}{(1-\gamma)^3}\frac{|\Scal||\Acal|\log(c|\Scal||\Acal|/\delta)}{\eps^2} ,
%N\geq \frac{c}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{\eps^2} ,
\textrm{\# samples from generative model }  = |\Scal||\Acal| N\geq
\frac{c |\Scal||\Acal|}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{\eps^2} ,
\]
then with probability greater than $1-\delta$, 
\[
\|Q^\star-Q^{\widehat \pi^\star}\|_\infty \leq \eps.
\]
\end{corollary}

\iffalse
Ultimately, we are interested in the value $V^{\widehat \pi^\star}$
when we execute $\widehat \pi^\star$, not just an estimate
$\widehat Q^\star$ of $Q^\star$. The above corollary is not
sharp with regards to finding a near optimal policy. The following
Theorem shows that in fact both value estimation and policy estimation
have the same rate.

\begin{theorem}\label{thm:policy}
Provided that $\eps\leq \sqrt{\frac{1}{1-\gamma}}$,  we have that if
\[
%N\geq \frac{c}{(1-\gamma)^3}\frac{|\Scal||\Acal|\log(c|\Scal||\Acal|/\delta)}{\eps^2} ,
N\geq \frac{c}{(1-\gamma)^3}\frac{\log(c|\Scal||\Acal|/\delta)}{\eps^2} ,
\]
then with probability greater than $1-\delta$, then
\[
\|Q^\star-Q^{\widehat \pi^\star}\|_\infty \leq \eps, \textrm{ and } \|Q^\star-\widehat Q^\star\|_\infty \leq \eps.
\]
\end{theorem}


We state this improved theorem without proof due to it being more
involved, and only prove Theorem~\ref{thm:sample_complexity}. See
Section~\ref{bib:sample_complexity} for further discussion. 
\fi

We only prove the first claim in Theorem~\ref{thm:sample_complexity}
on the estimation accuracy. With regards to the sub-optimality, note
that Theorem~\ref {lemma:Q_to_policy} already implies a sub-optimality
gap, though with an amplification of the estimation error by
$2/(1-\gamma)$. The argument for the improvement provided in the
second claim is more involved (See Section~\ref{bib:sample_complexity}
for further discussion).



\iffalse
As we now discuss, this corollary does not achieve the minmax rate
for the value of the policy.
\paragraph{Improvements (Non-asymptotic rates).} 
This bound on quality of  $\widehat \pi^\star$ is not sharp and can be improved upon.  With a more careful argument, the
$(1-\gamma)^5$ can be reduced to only $(1-\gamma)^3$.  Also, even for
estimating the value $Q^\star$, using $\widehat Q^\star$,  the bounds
presented are only valid when $\eps\leq 1$, and we may hope to have a
sharp rate even when $\eps\leq 1/(1-\gamma)$. See Section~\ref{bib:sample_complexity} for further discussion.
\fi



\paragraph{Lower Bounds.}

Let us say that an estimation algorithm $\mathcal{A}$, which is a map
from samples to an estimate $\widehat Q^\star$, is $(\eps,\delta)$-good on MDP
$M$  if $\|Q^\star-\widehat Q^\star\|_\infty \leq \eps$ holds with
probability greater than $1-\delta$.

\begin{theorem}~\label{thm:lower_sample_complexity}
There exists $\eps_0,\delta_0,c$ and a set
of MDPs $\mathcal{M}$  such that
for $\eps \in (0,\eps_0)$ and $\delta \in (0,\delta_0)$
if algorithm $\mathcal{A}$ is $(\eps,\delta)$-good on all $M\in \mathcal{M}$, then
$\mathcal{A}$ must use a number of samples that is lower bounded as follows
\[
\textrm{\# samples from generative model } \geq \frac{c}{(1-\gamma)^3}\frac{|\Scal||\Acal|\log(c|\Scal||\Acal|/\delta)}{\eps^2} \, .
\]
\end{theorem}

In other words, this theorem shows that the model based approach
minmax optimal.
%\subsection{What about the Value of the Policy $\widehat \pi^\star$?}

\iffalse
\citet{azar2013minimax} shows that for
sufficiently small $\eps$ --- for $\eps \leq c' (1-\gamma)/|\Scal|$
(for an absolute constant $c'$) --- the additional $1/(1-\gamma)$ factor
can be removed, where it becomes a lower order effect; this is an
extremely stringent condition in that this amplification only becomes
lower order when $\eps$ depends on the size of the state space.
Furthermore, \citet{sidford2018vnear} provide a
different algorithm, based on variance reduction, which removes the
factor all together.
\fi

\subsection{Finite Horizon Setting}
\label{sec:finite_horizon_sc}

Recall the setting of finite horizon MDPs defined in
Section~\ref{section:finite_horizon}. 
Again, we can consider the most naive approach to learning (when
we have access to a generative model):
suppose we call our simulator $N$ times for every
$(s,a,h)\in\Scal\times\Acal\times[H]$, i.e. 
we obtain $N$ i.i.d. samples where $s'\sim P_h(\cdot|s, a)$, for every
$(s,a,h)\in\Scal\times\Acal\times[H]$.  
Note that the total number of observed transitions is $H|\Scal||\Acal|N$.


\paragraph{Upper bounds.} The following theorem provides an upper
bound on the model based approach.

\begin{theorem}\label{thm:sample_complexity_finite}
For $\delta\geq 0$ and
with probability greater than $1-\delta$, we have that:
\begin{itemize}
\item (Value estimation) 
\[
\|Q_0^\star-\widehat Q_0^\star\|_\infty \leq
cH \sqrt{\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
+cH\frac{\log(c|\Scal||\Acal|/\delta)}{N}\, ,
\]
\item (Sub-optimality) 
\[
\|Q_0^\star-Q_0^{\widehat \pi^\star}\|_\infty \leq
cH \sqrt{\frac{\log(c|\Scal||\Acal|/\delta)}{N}}
+cH\frac{\log(c|\Scal||\Acal|/\delta)}{N}\, ,
\]
\end{itemize}
where $c$ is an absolute constant.
\end{theorem}

Note that the above bound requires $N$ to be $O(H^2)$ in order to achieve an
$\eps$-optimal policy, while in the discounted case, we require $N$ to
be $O(1/(1-\gamma)^3)$ for the same guarantee. While this may seem like an improvement by a
horizon factor, recall that for the finite horizon case, $N$
corresponds to observing $O(H)$ more transitions than in the
discounted case.

\paragraph{Lower Bounds.}

In the minmax sense of Theorem~\ref{thm:lower_sample_complexity}, the
previous upper bound provided by the model based approach for the finite horizon
setting achieves the minmax optimal  sample complexity.

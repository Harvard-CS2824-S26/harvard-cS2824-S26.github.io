\chapter{RL with Linear Features: When Does It Work, and When Does It Fail?}
\label{chap:linear_features}

Up to now we have focused on tabular Markov decision processes, where both
sample complexity and computation scale polynomially with $|\Scal|$ and $|\Acal|$.
In many problems of interest, however, the state and action spaces are so large
that tabular methods are not viable. A standard response is to introduce a
feature map
\[
\phi:\Scal\times\Acal \to \R^d
\]
and to approximate value functions using linear predictors
$f_\theta(s,a) = \theta^\top \phi(s,a)$.

A natural hope is that the relevant complexity parameter becomes $d$
rather than $|\Scal||\Acal|$.
This chapter asks a single organizing question:

\begin{quote}
\emph{When do linear features actually buy you $\poly(d)$ sample
  complexity --- and when do they fundamentally not?} 
\end{quote}

The answer is subtle.
On the positive side, there are structural conditions under which linear methods
reduce reinforcement learning to a sequence of well-conditioned regression
problems, yielding $\poly(d,H,1/\eps)$ guarantees without explicit dependence on
$|\Scal|$ or $|\Acal|$.
On the negative side, several seemingly natural ``linear realizability''
assumptions are \emph{not} enough: even with excellent coverage, and even when
an algorithm is given the feature map as input, the sample complexity can be
exponential in the horizon.

The chapter is organized around a short ladder of assumptions.
We first introduce these assumptions and their relationships at a high level.
We then present a sharp positive result under a completeness condition
(Least-Squares Value Iteration with a well-conditioned design), followed by
information-theoretic lower bounds showing that weaker notions of realizability
do not suffice.

\section{Setup and an Assumption Ladder}
\label{sec:assumption_ladder}

We work with a fixed feature map $\phi:\Scal\times\Acal\to\R^d$.
For a parameter vector $\theta\in\R^d$, define the associated linear function
$f_\theta(s,a)=\theta^\top\phi(s,a)$, and let
\[
\mathcal{F} := \{ f_\theta : \theta\in\R^d \}
\]
denote the corresponding linear function class.

There are multiple distinct ways that ``linear features'' can enter reinforcement learning.
The following assumptions form a useful ladder: as we move down the list, the
assumptions become stronger and increasingly algorithmically useful.

\paragraph{(A) Agnostic linear approximation (no realizability).}
The weakest perspective is purely approximation-based: we do \emph{not} assume that any
relevant value function is linear. Instead, we measure the best approximation error
of $\mathcal{F}$ to some target (e.g.\ $Q^\star$, or the iterates of a fitted value
iteration procedure) under a chosen norm or data distribution.
This is the analogue of agnostic supervised learning.
We defer this viewpoint---and its attendant distribution-shift / concentrability
issues---to the chapter on fitted value iteration. However, this
chapter alone shows how the agnostic viewpoint alone can lead to
subexponential results (in the relevant parameters).

\paragraph{(B) Linear $Q^\star$ realizability.}
A common structural assumption is that the \emph{optimal} action-value function is linear:
for each stage $h$ there exists $\theta_h^\star\in\R^d$ such that
\[
Q_h^\star(s,a) = (\theta_h^\star)^\top \phi(s,a)\qquad \forall (s,a).
\]
This is the most direct analogue of realizable linear regression.
A tempting intuition is that this should allow $\poly(d)$ learning.
We will see that this intuition is false in a strong information-theoretic sense.

\paragraph{(C) All-policies linear realizability.}
A stronger assumption requires linear realizability not just for $Q^\star$, but for
\emph{every} policy:
for each $\pi$ and each stage $h$, there exists $\theta_h^\pi$ such that
\[
Q_h^\pi(s,a) = (\theta_h^\pi)^\top \phi(s,a)\qquad \forall (s,a).
\]
This assumption is substantially stronger than (B).
Nevertheless, it still does \emph{not} guarantee sample-efficient offline policy
evaluation: even with extremely strong feature coverage, any estimator can require
exponentially many samples.

\paragraph{(D) Linear Bellman completeness (a closure property).}
The most algorithmically useful assumption in this chapter is a \emph{closure}
condition: roughly, applying the Bellman optimality operator to a linear function
yields another linear function (with respect to the same feature map).
One convenient way to state this is: for each stage $h$ and each $\theta\in\R^d$,
there exists $w\in\R^d$ such that the function
\[
(s,a)\ \mapsto\ r_h(s,a) + \E_{s'\sim P_h(\cdot\mid s,a)}\Big[\max_{a'} \theta^\top\phi(s',a')\Big]
\]
is exactly $w^\top\phi(s,a)$ for all $(s,a)$.
Equivalently, letting $\Fcal := \{(s,a)\mapsto w^\top\phi(s,a)\mid w\in\R^d\}$ denote the
class of functions linear in $\phi$, the Bellman completeness property can be written as
\begin{equation}\label{def:bc}
f\in \Fcal \quad\Longrightarrow\quad \Tcal_h f \in \Fcal \qquad \text{for each } h\in[H],
\end{equation}
where $\Tcal_h$ is the stage-$h$ Bellman optimality operator.
This assumption turns dynamic programming into a sequence of regression problems, and it
is what underlies the positive results in this chapter.

\paragraph{Relationships.}
A few simple implications are worth keeping in mind.
First, linear Bellman completeness immediately implies that rewards are linear in $\phi$
(take $\theta=0$ above). It also implies linear $Q^\star$ realizability (by backward
induction / value iteration carried out within $\Fcal$).
In contrast, (C) and (D) are not comparable in general: (C) asserts realizability for
\emph{all} policies but does not impose closure under the \emph{optimality} operator,
while (D) enforces closure under the optimality operator but does not a priori assert
that $Q^\pi$ is linear for every policy $\pi$.

\paragraph{A remark on strength.}
Bellman completeness should be read as a structural condition on the MDP itself (via the
transition kernel and reward), not merely a property of $Q^\star$.
We will give concrete examples of model classes satisfying Bellman completeness
(e.g.\ linear MDPs and related low-complexity transition models) later in
Chapters~\ref{chap:linear_MDPs} and~\ref{chap:Bellman_rank}, once our algorithmic
guarantees are on the table.

With our ladder in place, the remainder of the chapter answers our organizing question
by pairing a sharp positive result under (D) with lower bounds showing that (B) and (C)
can fail dramatically.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Positive results under Bellman completeness
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Positive results under Bellman completeness}
\label{sec:BC_positive}

This section gives the affirmative path for linear features: under the right
structural condition, dynamic programming reduces to a sequence of (supervised)
regression problems, yielding sample complexity polynomial in the feature
dimension. We state the algorithm in a way that will be reused in both the
generative-model and offline settings; the difference will be only in how the
stage-wise datasets are obtained.

\subsection{Setup and data model}
\label{subsec:BC_setup}

Throughout this section we assume the Bellman completeness condition stated earlier
(Section~\ref{subsec:BC_assumption}).

We work in the finite-horizon setting with stages $h\in[H] := \{0,1,\dots,H-1\}$.
An MDP is specified by a state space $\Scal$, an action space $\Acal$, an initial
distribution $\mu$ over $\Scal$, stage-dependent transition kernels
$\{P_h(\cdot\mid s,a)\}_{h=0}^{H-1}$, and stage-dependent rewards
$\{r_h(s,a)\}_{h=0}^{H-1}$. Throughout we assume rewards are bounded,
$r_h(s,a)\in[0,1]$.\footnote{Allowing stochastic rewards with bounded support is
routine; to keep notation light we write rewards as deterministic.}

We are given a feature map $\phi:\Scal\times\Acal\to\R^d$, and we consider the
linear function class
\[
\Fcal \;:=\; \bigl\{\, f_w:\Scal\times\Acal\to\R \ \big|\ f_w(s,a)=w^\top\phi(s,a),\ w\in\R^d \,\bigr\}.
\]
(As usual, one can rescale features so that $\sup_{s,a}\|\phi(s,a)\|_2\le 1$; we
will be explicit about boundedness assumptions when they matter.)

\paragraph{Stage-wise datasets (offline view).}
To keep the algorithm reusable, we phrase everything in terms of per-stage
datasets. For each $h\in[H]$, let
\[
D_h = \{(s_i,a_i,r_i,s_i')\}_{i=1}^N
\]
be a dataset of $N$ transitions at stage $h$, where each
$s_i'\sim P_h(\cdot\mid s_i,a_i)$ is sampled independently conditional on
$(s_i,a_i)$ and $r_i=r_h(s_i,a_i)$.
In the generative-model setting we will \emph{construct} these datasets; in the
offline setting the datasets are given and we will impose an explicit coverage
condition.

\paragraph{Bellman operators.}
Recall from
Section~\ref{section:finite_horizon}, that
for a function $f:\Scal\times\Acal\to\R$, we have that our stage-$h$ \emph{optimal}
Bellman operator $\Tcal_h$ is defined by
\[
(\Tcal_h f)(s,a)
\;:=\;
r_h(s,a) + \E_{s'\sim P_h(\cdot\mid s,a)}\Big[\,\max_{a'\in\Acal} f(s',a')\,\Big].
\]
In the finite-horizon problem, the optimal $Q$-functions satisfy the recursion
$Q^\star_H \equiv 0$ and $Q^\star_h = \Tcal_h Q^\star_{h+1}$ for $h=H-1,\dots,0$.

The remainder of Section~\ref{sec:BC_positive} shows that, under Bellman
completeness, ordinary least squares can drive Bellman residuals uniformly small,
which in turn yields a near-optimal greedy policy.

\subsection{Algorithm: Least-Squares Value Iteration (LSVI)}
\label{subsec:BC_lsvi_alg}

LSVI runs a backward dynamic program, where each Bellman backup is implemented by
a least-squares regression onto the feature class $\Fcal$.

\begin{algorithm}[t]
\begin{algorithmic}[1]
\State \textbf{Input:} datasets $D_0,\dots,D_{H-1}$, feature map $\phi$
\State Initialize $\widehat V_{H}(s) \gets 0$ for all $s\in\Scal$
\For{$h = H-1, H-2, \dots, 0$}
    \State $\displaystyle
    \widehat\theta_{h}
    \in \arg\min_{\theta\in\R^d}
    \sum_{(s,a,r,s')\in D_h}
    \Big(\theta^\top\phi(s,a) - r - \widehat V_{h+1}(s')\Big)^2$
    \State Define $\widehat Q_h(s,a) \gets \widehat\theta_h^\top\phi(s,a)$ for all $(s,a)$
    \State Define $\widehat V_h(s) \gets \max_{a\in\Acal}\widehat Q_h(s,a)$ for all $s$
\EndFor
\State \textbf{Return:} greedy policy $\widehat\pi=\{\widehat\pi_h\}_{h=0}^{H-1}$ where
$\widehat\pi_h(s)\in\arg\max_{a\in\Acal}\widehat Q_h(s,a)$
\end{algorithmic}
\caption{Least-Squares Value Iteration (LSVI)}
\label{alg:lsvi}
\end{algorithm}

\paragraph{Remark.}
If the empirical covariance at a stage is singular, one can take a minimum-norm
solution (or add a small ridge). In the settings we analyze (D-optimal sampling
from a generative model, or explicit coverage assumptions offline), the design
is well-conditioned and this technicality is inessential.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main Guarantee: LSVI with a Generative Model
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Main guarantee: LSVI with a generative model}
\label{subsec:BC_generative_main}

We now prove the main positive result under Bellman completeness in the
generative-model setting. The proof uses three ingredients:
(i) a geometric sampling lemma (D-optimal design),
(ii) a uniform generalization bound for ordinary least squares (Appendix),
and (iii) a standard approximate dynamic programming lemma converting small
Bellman residuals into near-optimality.

\subsubsection{Tool lemma: D-optimal design}
\label{subsubsec:BC_dopt}

Let $\Phi := \{\phi(s,a): (s,a)\in\Scal\times\Acal\}\subset\R^d$ denote the set of
feature vectors. We assume $\Phi$ is bounded and full-dimensional (otherwise we
may restrict to $\mathrm{span}(\Phi)$).

\begin{lemma}[D-optimal design]\label{lem:d_opt_design}
There exists a distribution $\rho$ supported on at most $d(d+1)/2$ state--action
pairs such that, letting
\[
\Sigma := \E_{(s,a)\sim\rho}\big[\phi(s,a)\phi(s,a)^\top\big],
\]
we have $\Sigma \succ 0$ and
\[
\sup_{(s,a)\in\Scal\times\Acal} \; \phi(s,a)^\top \Sigma^{-1} \phi(s,a) \;\le\; d .
\]
\end{lemma}

%\paragraph{Remark (John / Kiefer--Wolfowitz).}
One convenient characterization of $\rho$ is as a maximizer of a log-determinant
objective:
\begin{align}
\rho \in \argmax_{\nu \in \Delta(\Scal\times\Acal)}
\ln\det\!\left(\E_{(s,a)\sim\nu}\big[\phi(s,a)\phi(s,a)^\top\big]\right),
\label{eq:d_design}
\end{align}
(which is the Kiefer--Wolfowitz Theorem).
Equivalently, the ellipsoid $\mathcal{E}=\{v:\|v\|_{\Sigma^{-1}}^2\le d\}$ is the
unique minimum-volume centered ellipsoid containing $\Phi$, which
John's Theorem. We do not provide a
proof of either claim here (both of which lead to the D-optimal design
lemma).  See Section~\ref{chapterBC_bib} for references and further discussion.

\subsubsection{Main theorem}
\label{subsubsec:BC_main_thm}

We assume a generative model that can be queried at any stage $h$ and any
$(s,a)$ to obtain an independent sample $s'\sim P_h(\cdot\mid s,a)$ (along with
the reward $r_h(s,a)$).

\begin{theorem}[LSVI under Bellman completeness; generative model]
\label{thm:lsvi_bc_generative}
Assume Bellman completeness (Section~\ref{subsec:BC_assumption}) and bounded rewards
$r_h(s,a)\in[0,1]$. Let $\rho$ be a D-optimal design as in
Lemma~\ref{lem:d_opt_design}. Fix $\epsilon\in(0,1)$ and $\delta\in(0,1)$, and set
\[
N \;:=\; \left\lceil \frac{c\,H^6 d^2 \log(H/\delta)}{\epsilon^2}\right\rceil,
\]
for a sufficiently large absolute constant $c$.

Construct datasets $\{D_h\}_{h=0}^{H-1}$ as follows: for each $h$ and each support
point $(s,a)\in\mathrm{supp}(\rho)$, query the generative model
$n_{s,a}=\lceil N\rho(s,a)\rceil$ times to obtain i.i.d.\ next-state samples, and
include the corresponding tuples in $D_h$.

Then, with probability at least $1-\delta$, running LSVI
(Algorithm~\ref{alg:lsvi}) on $\{D_h\}_{h=0}^{H-1}$ returns a greedy policy
$\widehat\pi$ satisfying
\[
\E_{s\sim\mu}\big[V_0^\star(s) - V_0^{\widehat\pi}(s)\big] \;\le\; \epsilon.
\]
Moreover, the total number of samples is at most
\[
\sum_{h=0}^{H-1} |D_h|
\;\le\;
H\Bigl(N + \tfrac{d(d+1)}{2}\Bigr)
\;=\;
O\!\left(Hd^2 + \frac{H^7 d^2 \log(H/\delta)}{\epsilon^2}\right).
\]
\end{theorem}

\subsubsection{Proof of Theorem~\ref{thm:lsvi_bc_generative}}
\label{subsubsec:BC_main_proof}

We begin with a standard lemma: small Bellman residuals imply near-optimality of
the greedy policy.

For any sequence of functions $\{\widehat Q_h\}_{h=0}^{H}$ with $\widehat Q_H\equiv 0$,
define $\widehat V_h(s):=\max_{a\in\Acal}\widehat Q_h(s,a)$ and the greedy policy
$\widehat\pi_h(s)\in\arg\max_a \widehat Q_h(s,a)$. Define the stage-$h$ Bellman
residual
\[
\mathrm{Res}_h \;:=\; \big\|\widehat Q_h - \Tcal_h \widehat Q_{h+1}\big\|_\infty.
\]

\begin{lemma}[Small residual $\Rightarrow$ good policy]
\label{lem:residual_to_policy}
Assume $\widehat Q_H\equiv 0$ and that $\mathrm{Res}_h \le \eta$ for all $h\in[H]$.
Then:
\begin{enumerate}
\item For all $h\in[H]$, $\|\widehat Q_h - Q_h^\star\|_\infty \le (H-h)\eta$.
\item For the greedy policy $\widehat\pi$, for all $s\in\Scal$,
\[
V_0^\star(s) - V^{\widehat\pi}_0(s) \;\le\; 2H^2\eta.
\]
\end{enumerate}
\end{lemma}

\begin{proof}
For (1), we proceed by backward induction. Since $Q_H^\star\equiv 0=\widehat Q_H$,
the claim holds for $h=H$. Suppose it holds for $h+1$. Then for any $(s,a)$,
\begin{align*}
|\widehat Q_h(s,a) - Q_h^\star(s,a)|
&\le |\widehat Q_h(s,a) - (\Tcal_h\widehat Q_{h+1})(s,a)|
     + |(\Tcal_h\widehat Q_{h+1})(s,a) - (\Tcal_h Q_{h+1}^\star)(s,a)| \\
&\le \eta + \E_{s'}\big[\max_{a'}|\widehat Q_{h+1}(s',a')-Q_{h+1}^\star(s',a')|\big] \\
&\le \eta + \|\widehat Q_{h+1}-Q_{h+1}^\star\|_\infty
\le (H-h)\eta,
\end{align*}
using that $\max$ is 1-Lipschitz in $\ell_\infty$.

For (2), fix $h$ and $s$. Let $a^\star\in\arg\max_a Q_h^\star(s,a)$ and
$\widehat a\in\arg\max_a \widehat Q_h(s,a)$. Then
\[
Q_h^\star(s,a^\star) - Q_h^\star(s,\widehat a)
\le
\bigl(Q_h^\star(s,a^\star)-\widehat Q_h(s,a^\star)\bigr)
+
\bigl(\widehat Q_h(s,\widehat a)-Q_h^\star(s,\widehat a)\bigr)
\le 2\|\widehat Q_h-Q_h^\star\|_\infty
\le 2(H-h)\eta.
\]
Unrolling along the trajectory of $\widehat\pi$ gives
\[
V_0^\star(s) - V_0^{\widehat\pi}(s)
\le \sum_{h=0}^{H-1} 2(H-h)\eta
\le 2H^2\eta.
\]
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:lsvi_bc_generative}]
We show that, with high probability, each stage regression in LSVI has small
\emph{uniform} prediction error, which implies small Bellman residuals; then we
invoke Lemma~\ref{lem:residual_to_policy}.

\paragraph{Step 1: one stage as fixed-design regression.}
Fix a stage $h\in[H]$ and condition on $\widehat Q_{h+1}$ (equivalently,
$\widehat V_{h+1}$), which is measurable with respect to data from stages
$h+1,\dots,H-1$.

Each sample has covariate $x_i := \phi(s_i,a_i)$ and response
$y_i := r_h(s_i,a_i) + \widehat V_{h+1}(s_i')$.
(Optionally clip $\widehat V_{h+1}$ to $[0,H]$ to strengthen boundedness.)

By Bellman completeness, there exists $\theta_h^\star\in\R^d$ such that for all
$(s,a)$,
\[
(\theta_h^\star)^\top \phi(s,a)
=
r_h(s,a) + \E_{s'\sim P_h(\cdot\mid s,a)}\big[\widehat V_{h+1}(s')\big]
=
(\Tcal_h \widehat Q_{h+1})(s,a).
\]
Hence $y_i = (\theta_h^\star)^\top x_i + \xi_i$ where $\E[\xi_i\mid x_i]=0$ and
$\{\xi_i\}$ are independent across samples. Moreover $|\xi_i|\lesssim H$, so
$\xi_i$ is $O(H)$-sub-Gaussian.

Let $\Lambda_h := \sum_{(s,a,r,s')\in D_h} \phi(s,a)\phi(s,a)^\top$ denote the
(stage-$h$) design matrix.

\paragraph{Step 2: leverage control from D-optimal sampling.}
Let $\Sigma=\E_{(s,a)\sim\rho}[\phi(s,a)\phi(s,a)^\top]$. By construction,
\[
\Lambda_h
=
\sum_{(s,a)\in\mathrm{supp}(\rho)} \lceil N\rho(s,a)\rceil\, \phi(s,a)\phi(s,a)^\top
\succeq
\sum_{(s,a)\in\mathrm{supp}(\rho)} N\rho(s,a)\, \phi(s,a)\phi(s,a)^\top
=
N\Sigma,
\]
so $\Lambda_h^{-1}\preceq (N\Sigma)^{-1}$ and Lemma~\ref{lem:d_opt_design} implies
\[
\sup_{(s,a)} \phi(s,a)^\top \Lambda_h^{-1}\phi(s,a) \;\le\; \frac{d}{N}.
\]

\paragraph{Step 3: OLS generalizes uniformly.}
Apply the fixed-design OLS bound from the Appendix (Theorem~\ref{thm:ols_fixed_design})
with failure probability $\delta' := \delta/(2H)$ and noise scale $O(H)$ to obtain,
with probability at least $1-\delta'$,
\[
\|\widehat\theta_h - \theta_h^\star\|_{\Lambda_h}
\;\le\;
c_1 H \sqrt{ d\log(H/\delta) } ,
\]
for an absolute constant $c_1$. Therefore, for any $(s,a)$,
\[
\big|(\widehat\theta_h-\theta_h^\star)^\top \phi(s,a)\big|
\le
\|\widehat\theta_h-\theta_h^\star\|_{\Lambda_h}
\sqrt{\phi(s,a)^\top \Lambda_h^{-1}\phi(s,a)}
\le
c_1 H d \sqrt{\frac{\log(H/\delta)}{N}}.
\]
Equivalently,
\[
\big\|\widehat Q_h - \Tcal_h \widehat Q_{h+1}\big\|_\infty
\le
c_1 H d \sqrt{\frac{\log(H/\delta)}{N}}.
\]

\paragraph{Step 4: union bound and conclude.}
Apply the bound for all $h\in[H]$ and take a union bound: with probability at
least $1-\delta/2$,
\[
\max_{h\in[H]}\big\|\widehat Q_h - \Tcal_h \widehat Q_{h+1}\big\|_\infty
\le
c_1 H d \sqrt{\frac{\log(H/\delta)}{N}}.
\]
With our choice of $N$ (and $c$ large enough), the right-hand side is at most
$\epsilon/(2H^2)$. Lemma~\ref{lem:residual_to_policy} then yields, for all $s$,
$V_0^\star(s) - V_0^{\widehat\pi}(s)\le \epsilon$, and hence also
$\E_{s\sim\mu}[V_0^\star(s)-V_0^{\widehat\pi}(s)]\le\epsilon$.

Finally, for each $h$,
\[
|D_h|
=
\sum_{(s,a)\in\mathrm{supp}(\rho)} \lceil N\rho(s,a)\rceil
\le
N + |\mathrm{supp}(\rho)|
\le
N + \frac{d(d+1)}{2},
\]
and summing over $h$ gives the stated total sample bound.
\end{proof}




\newpage
\section{Positive results under Bellman completeness}
\label{sec:BC_positive}

This part gives the affirmative path for linear features: under the right
structural condition, dynamic programming really does reduce to a sequence of
(supervised) regression problems, and this yields a sample complexity that is
polynomial in the feature dimension. The goal is to make explicit what the
assumption buys us, and to state the algorithms in a way that we can reuse in
both the generative-model and offline settings.

\subsection{Setup}
\label{subsec:BC_setup}

We work in the finite-horizon setting with stages $h\in[H] := \{0,1,\dots,H-1\}$.
An MDP is specified by a state space $\Scal$, an action space $\Acal$, an initial
distribution $\mu$ over $\Scal$, stage-dependent transition kernels
$\{P_h(\cdot\mid s,a)\}_{h=0}^{H-1}$, and stage-dependent rewards
$\{r_h(s,a)\}_{h=0}^{H-1}$. Throughout we assume rewards are bounded, say
$r_h(s,a)\in[0,1]$.\footnote{Allowing stochastic rewards with bounded support is
also routine; to keep notation light we write rewards as deterministic.}

We are given a feature map $\phi:\Scal\times\Acal\to\R^d$, and we consider the
linear function class
\[
\Fcal \;:=\; \bigl\{\, f_w:\Scal\times\Acal\to\R \ \big|\ f_w(s,a)=w^\top\phi(s,a),\ w\in\R^d \,\bigr\}.
\]
(As usual, one can rescale features so that $\sup_{s,a}\|\phi(s,a)\|_2\le 1$; we
will be explicit about boundedness assumptions when they matter.)


\paragraph{Data model (offline view).}
To keep the algorithms reusable, we phrase everything in terms of per-stage
datasets. For each $h\in[H]$, let
\[
D_h = \{(s_i,a_i,r_i,s_i')\}_{i=1}^N
\]
be a dataset of $N$ transitions at stage $h$, where each $s_i'\sim
P_h(\cdot\mid s_i,a_i)$ is sampled independently conditional on $(s_i,a_i)$ and
$r_i=r_h(s_i,a_i)$. In the generative-model setting we will \emph{construct}
these datasets; in the offline setting the datasets are given and we will impose
a suitable coverage condition.

\paragraph{Bellman completeness short hand notation.}
When we want to emphasize the induced map on parameters, we write
$w=\Tcal_h(\theta)$ for a choice of $w$ satisfying
$\Tcal_h f_\theta = f_w$, where $f_\theta(s,a)=\theta^\top\phi(s,a)$.

With the Bellman completeness assumption in place, the rest of Section~\ref{sec:BC_positive} shows that
we can drive the Bellman residuals small via ordinary least squares, which in
turn yields a near-optimal greedy policy.

\subsection{Algorithms}
\label{subsec:BC_algorithms}

We now state the two basic algorithms we will analyze. Importantly, both are
\emph{data-driven}: they take as input stage-wise datasets $\{D_h\}_{h=0}^{H-1}$.
This lets us reuse the same templates in the generative-model and offline
settings (the only difference is how the datasets are obtained).

\subsubsection{Least-Squares Value Iteration (LSVI)}

LSVI runs a backward dynamic program, where each Bellman backup is implemented by
a least-squares regression onto the feature class $\Fcal$.

\begin{algorithm}[t]
\begin{algorithmic}[1]
\State \textbf{Input:} datasets $D_0,\dots,D_{H-1}$, feature map $\phi$
\State Initialize $\widehat V_{H}(s) \gets 0$ for all $s\in\Scal$
\For{$h = H-1, H-2, \dots, 0$}
    \State $\displaystyle
    \widehat\theta_{h}
    \in \arg\min_{\theta\in\R^d}
    \sum_{(s,a,r,s')\in D_h}
    \Big(\theta^\top\phi(s,a) - r - \widehat V_{h+1}(s')\Big)^2$
    \State Define $\widehat Q_h(s,a) \gets \widehat\theta_h^\top\phi(s,a)$ for all $(s,a)$
    \State Define $\widehat V_h(s) \gets \max_{a\in\Acal}\widehat Q_h(s,a)$ for all $s$
\EndFor
\State \textbf{Return:} greedy policy $\widehat\pi=\{\widehat\pi_h\}_{h=0}^{H-1}$ where
$\widehat\pi_h(s)\in\arg\max_{a\in\Acal}\widehat Q_h(s,a)$
\end{algorithmic}
\caption{Least-Squares Value Iteration (LSVI)}
\label{alg:lsvi}
\end{algorithm}

\paragraph{Remark.}
If the empirical covariance in a stage is singular, one can take a minimum-norm
solution (or add a small ridge). In the settings we care about (D-optimal
sampling from a generative model, or coverage assumptions offline), the design
will be well-conditioned and this technicality does not play a major role.

\subsubsection{Least-Squares Policy Evaluation (LSPE)}

LSPE is the evaluation analogue of LSVI: the only change is that we replace the
optimality backup $\max_{a'}$ by the backup induced by a target policy $\pi$.
We allow a non-stationary policy $\pi=\{\pi_h\}_{h=0}^{H-1}$, where each
$\pi_h(s)\in\Acal$ is known (we assume the policy is deterministic for simplicity).

Given a parameter $\theta_h$, define the induced value estimate
\[
\widehat V_h^{\pi}(s)
\;:=\;
\widehat\theta_h^\top\phi(s, \pi_h(s)).
\]

\begin{algorithm}[t]
\begin{algorithmic}[1]
\State \textbf{Input:} policy $\pi=\{\pi_h\}_{h=0}^{H-1}$, datasets $D_0,\dots,D_{H-1}$, feature map $\phi$
\State Initialize $\widehat V^{\pi}_{H}(s) \gets 0$ for all $s\in\Scal$
\For{$h = H-1, H-2, \dots, 0$}
    \State $\displaystyle
    \widehat\theta_{h}
    \in \arg\min_{\theta\in\R^d}
    \sum_{(s,a,r,s')\in D_h}
    \Big(\theta^\top\phi(s,a) - r - \widehat V^{\pi}_{h+1}(s')\Big)^2$
    \State Define $\widehat V^{\pi}_h(s) \gets \widehat\theta_h^\top\phi(s, \pi_h(s))$ for all $s$
\EndFor
\State \textbf{Return:} parameters $\{\widehat\theta_h\}_{h=0}^{H-1}$ (equivalently, value estimates $\{\widehat V_h^{\pi}\}_{h=0}^{H}$)
\end{algorithmic}
\caption{Least-Squares Policy Evaluation (LSPE)}
\label{alg:lspe}
\end{algorithm}


\subsection{Main Guarantee: LSVI with a Generative Model}
\label{subsec:BC_generative_main}

This section proves the main positive result under Bellman completeness in the
generative-model setting. The proof has three moving parts:
(i) a geometric sampling lemma (D-optimal design), (ii) a uniform generalization
bound for ordinary least squares (from the Appendix), and (iii) a standard
approximate dynamic programming lemma converting small Bellman residuals into
near-optimality.

\subsubsection{Detour: D-optimal design}
\label{subsubsec:BC_dopt}

Let $\Phi := \{\phi(s,a): (s,a)\in\Scal\times\Acal\}\subset\R^d$ denote the set of
feature vectors. We assume $\Phi$ is bounded and full-dimensional (otherwise we
may restrict to $\mathrm{span}(\Phi)$).

\begin{lemma}[D-optimal design]\label{lem:d_opt_design}
There exists a distribution $\rho$ supported on at most $d(d+1)/2$ points in
$\Phi$ such that, letting
\[
\Sigma := \E_{x\sim\rho}[xx^\top],
\]
we have $\Sigma \succ 0$ and
\[
\sup_{x\in\Phi} \; x^\top \Sigma^{-1} x \;\le\; d .
\]
\end{lemma}

The distribution $\rho$ can be defined as a maximizer of a log-determinant
objective:
\begin{align}
\rho \in \argmax_{\nu \in \Delta(\Phi)} \ln\det\!\left(\E_{x\sim\nu}[xx^\top]\right).
\label{eq:d_design}
\end{align}
It also admits a geometric interpretation: the ellipsoid
$\mathcal{E}=\{v:\|v\|_{\Sigma^{-1}}^2\le d\}$ is the unique minimum-volume
centered ellipsoid containing $\Phi$. We do not provide a proof here; see
Section~\ref{chapterBC_bib} for references and further discussion.

In what follows, we will use the D-optimal design on $\Phi$ to specify a sampling
distribution over state--action pairs.

\subsubsection{Main theorem: LSVI under Bellman completeness}
\label{subsubsec:BC_main_thm}

We assume the Bellman completeness condition from Section~\ref{subsec:BC_setup},
and we work with a generative model that can be queried at any stage $h$ and any
$(s,a)$ to obtain an independent sample $s'\sim P_h(\cdot\mid s,a)$ (along with
the reward $r_h(s,a)$).

\begin{theorem}[LSVI under Bellman completeness; generative model]
\label{thm:lsvi_bc_generative}
Assume Bellman completeness (Section~\ref{subsec:BC_setup}) and bounded rewards
$r_h(s,a)\in[0,1]$. Let $\rho$ be a D-optimal design for $\Phi$ as in
Lemma~\ref{lem:d_opt_design}. Fix $\epsilon\in(0,1)$ and $\delta\in(0,1)$, and set
\[
N \;:=\; \left\lceil \frac{c\,H^6 d^2 \log(H/\delta)}{\epsilon^2}\right\rceil,
\]
for a sufficiently large absolute constant $c$.

Construct datasets $\{D_h\}_{h=0}^{H-1}$ as follows: for each $h$ and each support
point $(s,a)\in\mathrm{supp}(\rho)$, query the generative model
$n_{s,a}=\lceil N\rho(s,a)\rceil$ times to obtain i.i.d.\ next-state samples, and
include the corresponding transition tuples in $D_h$.

Then, with probability at least $1-\delta$, running LSVI
(Algorithm~\ref{alg:lsvi}) on $\{D_h\}_{h=0}^{H-1}$ returns a greedy policy
$\widehat\pi$ satisfying
\[
\E_{s\sim\mu}\big[V_0^\star(s) - V_0^{\widehat\pi}(s)\big] \;\le\; \epsilon.
\]
Moreover, the total number of samples is at most
\[
\sum_{h=0}^{H-1} |D_h|
\;\le\;
H\Bigl(N + \tfrac{d(d+1)}{2}\Bigr)
\;=\;
O\!\left(Hd^2 + \frac{H^7 d^2 \log(H/\delta)}{\epsilon^2}\right).
\]
\end{theorem}


\subsection{Proof of Theorem~\ref{thm:lsvi_bc_generative}}
\label{subsec:BC_main_proof}

We begin with a standard lemma: small Bellman residuals imply near-optimality of
the greedy policy.

For any sequence of functions $\{\widehat Q_h\}_{h=0}^{H}$ with $\widehat Q_H\equiv 0$,
define $\widehat V_h(s):=\max_{a\in\Acal}\widehat Q_h(s,a)$ and the greedy policy
$\widehat\pi_h(s)\in\arg\max_a \widehat Q_h(s,a)$. Define the stage-$h$ Bellman
residual
\[
\mathrm{Res}_h \;:=\; \big\|\widehat Q_h - \Tcal_h \widehat Q_{h+1}\big\|_\infty.
\]

\begin{lemma}[Small residual $\Rightarrow$ good policy]
\label{lem:residual_to_policy}
Assume $\widehat Q_H\equiv 0$ and that $\mathrm{Res}_h \le \eta$ for all $h\in[H]$.
Then:
\begin{enumerate}
\item For all $h\in[H]$, $\|\widehat Q_h - Q_h^\star\|_\infty \le (H-h)\eta$.
\item For the greedy policy $\widehat\pi$, for all $s\in\Scal$,
\[
V_0^\star(s) - V^{\widehat\pi}_0(s) \;\le\; 2H^2\eta.
\]
\end{enumerate}
\end{lemma}

\begin{proof}
For (1), we proceed by backward induction. Since $Q_H^\star\equiv 0=\widehat Q_H$,
the claim holds for $h=H$. Suppose it holds for $h+1$. Then for any $(s,a)$,
\begin{align*}
|\widehat Q_h(s,a) - Q_h^\star(s,a)|
&\le |\widehat Q_h(s,a) - (\Tcal_h\widehat Q_{h+1})(s,a)|
     + |(\Tcal_h\widehat Q_{h+1})(s,a) - (\Tcal_h Q_{h+1}^\star)(s,a)| \\
&\le \eta + \E_{s'}\big[\max_{a'}|\widehat Q_{h+1}(s',a')-Q_{h+1}^\star(s',a')|\big] \\
&\le \eta + \|\widehat Q_{h+1}-Q_{h+1}^\star\|_\infty
\le (H-h)\eta,
\end{align*}
using that $\max$ is 1-Lipschitz in $\ell_\infty$.

For (2), fix $h$ and $s$. Let $a^\star\in\arg\max_a Q_h^\star(s,a)$ and
$\widehat a\in\arg\max_a \widehat Q_h(s,a)$. Then
\[
Q_h^\star(s,a^\star) - Q_h^\star(s,\widehat a)
\le
\bigl(Q_h^\star(s,a^\star)-\widehat Q_h(s,a^\star)\bigr)
+
\bigl(\widehat Q_h(s,\widehat a)-Q_h^\star(s,\widehat a)\bigr)
\le 2\|\widehat Q_h-Q_h^\star\|_\infty
\le 2(H-h)\eta.
\]
Unrolling the Bellman equations along the trajectory of $\widehat\pi$ yields
\[
V_0^\star(s) - V_0^{\widehat\pi}(s)
\le \sum_{h=0}^{H-1} 2(H-h)\eta
\le 2H^2\eta.
\]
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:lsvi_bc_generative}]
We show that, with high probability, each stage regression in LSVI has small
\emph{uniform} prediction error, implying small Bellman residuals; then we invoke
Lemma~\ref{lem:residual_to_policy}.

\paragraph{Step 1: one stage as fixed-design linear regression.}
Fix a stage $h\in[H]$ and condition on $\widehat Q_{h+1}$ (equivalently,
$\widehat V_{h+1}$), which is measurable with respect to data from stages
$h+1,\dots,H-1$.

In the stage-$h$ regression, each sample has covariate
\[
x_i := \phi(s_i,a_i),
\]
and response
\[
y_i := r_h(s_i,a_i) + \widehat V_{h+1}(s_i').
\]
(If desired, one may clip $\widehat V_{h+1}$ to $[0,H]$ in the regression target;
this only strengthens boundedness and does not change the argument.)

By Bellman completeness, there exists $\theta_h^\star\in\R^d$ such that for all
$(s,a)$,
\[
(\theta_h^\star)^\top \phi(s,a)
=
r_h(s,a) + \E_{s'\sim P_h(\cdot\mid s,a)}\big[\widehat V_{h+1}(s')\big]
=
(\Tcal_h \widehat Q_{h+1})(s,a).
\]
Hence $y_i = (\theta_h^\star)^\top x_i + \xi_i$ where
$\E[\xi_i\mid x_i]=0$ and $\{\xi_i\}$ are independent across samples (because the
$s_i'$ are sampled independently conditional on $(s_i,a_i)$). Moreover,
$|\xi_i| \lesssim H$, so $\xi_i$ is $O(H)$-sub-Gaussian.

Let
\[
\Lambda_h := \sum_{(s,a,r,s')\in D_h} \phi(s,a)\phi(s,a)^\top
\]
denote the (stage-$h$) design matrix.

\paragraph{Step 2: leverage control from D-optimal sampling (one-line bound).}
Let $\Sigma=\E_{x\sim\rho}[xx^\top]$. By construction of $D_h$,
\[
\Lambda_h
=
\sum_{x\in\mathrm{supp}(\rho)} \lceil N\rho(x)\rceil\, xx^\top
\succeq
\sum_{x\in\mathrm{supp}(\rho)} N\rho(x)\, xx^\top
=
N\Sigma.
\]
Therefore $\Lambda_h^{-1}\preceq (N\Sigma)^{-1}$ and by Lemma~\ref{lem:d_opt_design},
\[
\sup_{(s,a)} \phi(s,a)^\top \Lambda_h^{-1}\phi(s,a)
\le
\frac{1}{N}\sup_{x\in\Phi} x^\top\Sigma^{-1}x
\le \frac{d}{N}.
\]

\paragraph{Step 3: OLS generalizes uniformly.}
Apply the fixed-design OLS bound from the Appendix (Theorem~\ref{thm:ols_fixed_design})
with failure probability $\delta' := \delta/(2H)$ and noise scale $O(H)$ to obtain,
with probability at least $1-\delta'$,
\[
\|\widehat\theta_h - \theta_h^\star\|_{\Lambda_h}
\;\le\;
c_1 H \sqrt{ d\log(H/\delta) },
\]
for an absolute constant $c_1$. Hence for any $(s,a)$,
\begin{align*}
\big|(\widehat\theta_h-\theta_h^\star)^\top \phi(s,a)\big|
&\le
\|\widehat\theta_h-\theta_h^\star\|_{\Lambda_h}\cdot
\sqrt{\phi(s,a)^\top \Lambda_h^{-1}\phi(s,a)} \\
&\le
c_1 H \sqrt{ d\log(H/\delta)}\cdot \sqrt{\frac{d}{N}}
=
c_1 H d \sqrt{\frac{\log(H/\delta)}{N}}.
\end{align*}
Equivalently,
\[
\big\|\widehat Q_h - \Tcal_h \widehat Q_{h+1}\big\|_\infty
\;\le\;
c_1 H d \sqrt{\frac{\log(H/\delta)}{N}}.
\]

\paragraph{Step 4: union bound over stages and conclude.}
Apply the above bound for all $h\in[H]$ and take a union bound: with probability
at least $1-\delta/2$,
\[
\max_{h\in[H]}\big\|\widehat Q_h - \Tcal_h \widehat Q_{h+1}\big\|_\infty
\;\le\;
c_1 H d \sqrt{\frac{\log(H/\delta)}{N}}.
\]
With our choice of $N$ (and $c$ large enough), the right-hand side is at most
$\epsilon/(2H^2)$. Lemma~\ref{lem:residual_to_policy} then yields, for all $s$,
\[
V_0^\star(s) - V_0^{\widehat\pi}(s)
\le 2H^2\cdot \frac{\epsilon}{2H^2}
=\epsilon,
\]
and hence also $\E_{s\sim\mu}[V_0^\star(s)-V_0^{\widehat\pi}(s)]\le\epsilon$.

Finally, the sample bound follows because for each $h$,
\[
|D_h|
=
\sum_{x\in\mathrm{supp}(\rho)} \lceil N\rho(x)\rceil
\le
N + |\mathrm{supp}(\rho)|
\le
N + \frac{d(d+1)}{2},
\]
and summing over $h$ gives the stated total.
\end{proof}


\newpage

\section{Offline Extensions with BC (or something like that)}

One notable observation about LSVI is that the algorithm is
non-adaptive, in the sense that it works using a fixed set of observed
transitions and rewards. Hence, it is applicable to the offline
setting discussed in Chapter~\ref{chap:prelims}.

We now discuss two offline objectives: that of learning a near optimal
policy and that of policy evaluation (i.e. evaluating the quality of a
given policy).

We consider the setting where have a $H$ datasets of the form 
$D_h = \{(s_i,a_i,s_i',r(s_i,a_i))\}_{i=1}^N$, where we assume for each $i$
and $h$, that we have independent samples $s'_i \sim P_h(\cdot|s_i,a_i)$. In other words, $D_h$ is
a dataset of observed transitions corresponding to stage $h$, where
each next state has been sampled independently. Note that here we have
not made explicit distributional assumption about how the
$(s_i,a_i)$'s have been generated. The results we present can also be
extended to where we have a fixed data generation distribution over
these quadruples.

It is straight forward to extend the LSVI guarantees to these setting
provided our dataset has coverage in the following sense:

\begin{assumption}\label{assumption:coverage}
(Coverage)
Suppose that for each $h\in [H]$, we have that:
\[
\frac{1}{N} \sum_{(s_i,a_i) \in D_h} \phi(s_i,a_i)
\phi(s_i,a_i)^{\top} \succeq \frac{1}{\kappa}\Sigma
\]
where $\Sigma$ is the D-optimal design covariance (see Equation~\ref{eq:d_design}).
\end{assumption}

\subsection{Offline Learning}

A minor modification of the proof in
Theorem~\ref{them:main_lsvi_bell_complete} leads to the following guarantee:

\begin{theorem} [Sample Complexity of LSVI] 
Suppose that Assumption~\ref{assumption:coverage} holds and that our
features satisfy the linear Bellman completion property. 
Fix $\delta \in (0, 1)$
  and $\epsilon \in (0,1)$.  Set parameter  $N := \left\lceil \frac{ c
      \kappa H^6 d^2 {\ln(1/\delta)}}{\epsilon^2} \right\rceil$, where
  $c$ is an appropriately chosen absolute constant. With probability
  at least $1-\delta$, Algorithm~\ref{alg:lsvi} outputs  $\widehat\pi$
  such that: 
\begin{align*}
\mathbb{E}_{s\sim \mu} [V^\star_0(s)] - \mathbb{E}_{s\sim \mu} [V^{\widehat\pi}_0(s)] \leq  \epsilon.
\end{align*}
\end{theorem}

\subsection{Offline Policy Evaluation}

Here we are interested in question of evaluating some given policy
$\pi$ using the offline data.  For this, we will make a completeness
assumption with respect to $\pi$ as follows:

\begin{algorithm}[t]
\begin{algorithmic}[1]
%\State Compute the D-optimal design $\rho$ (Eq.~\ref{eq:d_design})
\State \textbf{Input}: $\pi$, $\Dcal_0,\dots, \Dcal_{H-1}$ %Set $V_{H}(s) = 0$ for all $s\in\Scal$
\State Set $V_{H}(s) = 0$ for all $s\in \Scal$
\For{$h = H-1 \to 0$}
    %\State  $\forall (s,a) \in \text{support}(\rho)$, i.i.d sample $\lceil N\rho(s,a) \rceil$ many next states $\Dcal_{s,a} : = \{{s'}^i\}_{i=1}^{\lceil \rho(s,a)N \rceil}$ with ${s'}^i \sim P_h(\cdot | s,a)$
    \State Solve least squares 
    \begin{align}
    \widehat{\theta}_{h} = \arg\min_{\theta }  \sum_{s,a,r, s'\in \Dcal_{h}} \left( \theta^{\top} \phi(s,a)  - r - V_{h+1}({s'}) \right)^2 %\label{eq:ls_bc}
    \end{align}  %\label{line:lsvi_regression}
    \State Set $V_{h}(s) = \widehat{\theta}_h^{\top} \phi(s,\pi(s)), \forall s$
\EndFor
\State \textbf{Return:} $\{\widehat{\theta}_h\}_{h=0}^{H-1}$.
\end{algorithmic}
\caption{Least Squares Policy Evaluation}
\label{alg:lspe}
\end{algorithm}

\begin{definition}[Linear Completeness for $\pi$] 
We say the
features $\phi$ satisfy the \emph{policy completeness property for $\pi$}
if for all $\theta \in \mathbb{R}^d$ and
  $(s,a,h) \in \Scal\times\Acal\times[H]$, there exists
  $ w\in\mathbb{R}^d$ such that:
%linear function $f(s,a) :=
%\theta^{\top} \phi(s,a)$ for some $\theta \in \mathbb{R}^d$, we have: 
\begin{align*}
%\forall h\in [H]:  \exists w\in\mathbb{R}^d, \text{s.t., }  
w^{\top} \phi(s,a) = r(s,a) + \mathbb{E}_{s'\sim P_h(s,a)} \theta^{\top} \phi(s',\pi(s')).
\end{align*} 
%As $w$ depends on $\theta$, we use the notation $\Tcal_h: \mathbb{R}^d\mapsto \mathbb{R}^d$ to
%represent such a $w$, i.e., $w := \Tcal_h( \theta )$ in the above equation.  
\end{definition} 

For this case, Algorithm~\ref{alg:lspe}, Least Squares Policy
Evaluation (LSPE), is a modified version of
LSVI for the purposes of estimation; note the algorithm no longer
estimates $V_{h}(s)$ using the greedy policy.
Again, a nearly identical argument to the proof in
Theorem~\ref{them:main_lsvi_bell_complete} leads to the following
guarantee. Here, we set:
\[
\widehat{V} (\mu) = \sum_{s_i \in D_0} \theta_0 \cdot \phi(s_i, \pi(s_i))
\]
where $\theta_0$ is the parameter returned by LSVI.

\begin{theorem} [Sample Complexity of LSPE] 
Suppose that Assumption~\ref{assumption:coverage} holds and that our
features satisfy the linear policy completion property (with respect
to $\pi$). 
Fix $\delta \in (0, 1)$
  and $\epsilon \in (0,1)$.  Set parameter  $N := \left\lceil \frac{ c
      \kappa H^4 d^2 {\ln(1/\delta)}}{\epsilon^2} \right\rceil$, where
  $c$ is an appropriately chosen absolute constant. With probability
  at least $1-\delta$, Algorithm~\ref{alg:lspe} outputs  $\widehat{\theta}_0$
  such that for all $s$,
\[
|V^\pi_0(s) - \widehat{\theta}^\top_0 \phi(s,\pi(s)) | \leq  \epsilon.
\]
\end{theorem}

Note that the above theorem has a sample complexity improvement by a
factor of $H$. This is due to that the analysis is improvable, as we only care about
value accuracy (the first claim in Lemma~\ref{lem:perturbation_Q},
rather than the second, is what is relevant here). We should note that
the sample size bounds presented in this chapter have not been
optimized with regards to their $H$ dependencies.

\subsubsection{Least-Squares Policy Evaluation (LSPE)}

LSPE is the evaluation analogue of LSVI: the only change is that we replace the
optimality backup $\max_{a'}$ by the backup induced by a target policy $\pi$.
We allow a non-stationary policy $\pi=\{\pi_h\}_{h=0}^{H-1}$, where each
$\pi_h(s)\in\Acal$ is known (we assume the policy is deterministic for simplicity).

Given a parameter $\theta_h$, define the induced value estimate
\[
\widehat V_h^{\pi}(s)
\;:=\;
\widehat\theta_h^\top\phi(s, \pi_h(s)).
\]

\begin{algorithm}[t]
\begin{algorithmic}[1]
\State \textbf{Input:} policy $\pi=\{\pi_h\}_{h=0}^{H-1}$, datasets $D_0,\dots,D_{H-1}$, feature map $\phi$
\State Initialize $\widehat V^{\pi}_{H}(s) \gets 0$ for all $s\in\Scal$
\For{$h = H-1, H-2, \dots, 0$}
    \State $\displaystyle
    \widehat\theta_{h}
    \in \arg\min_{\theta\in\R^d}
    \sum_{(s,a,r,s')\in D_h}
    \Big(\theta^\top\phi(s,a) - r - \widehat V^{\pi}_{h+1}(s')\Big)^2$
    \State Define $\widehat V^{\pi}_h(s) \gets \widehat\theta_h^\top\phi(s, \pi_h(s))$ for all $s$
\EndFor
\State \textbf{Return:} parameters $\{\widehat\theta_h\}_{h=0}^{H-1}$ (equivalently, value estimates $\{\widehat V_h^{\pi}\}_{h=0}^{H}$)
\end{algorithmic}
\caption{Least-Squares Policy Evaluation (LSPE)}
\label{alg:lspe}
\end{algorithm}

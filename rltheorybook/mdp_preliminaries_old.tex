\chapter{Markov Decision Processes}
% \\ and Computational Complexity}
\label{chap:prelims}

\section{Discounted (Infinite-Horizon) Markov Decision Processes}
In reinforcement learning, the interactions between the agent and the
environment are often described by an infinite-horizon, discounted Markov Decision Process (MDP)
$M = (\Scal, \Acal, P, r, \gamma, \mu)$, specified by:
\begin{itemize}
\item A state space $\Scal$, which may be finite or infinite.
For mathematical convenience, we will assume that $\Scal$ is
finite or countably infinite.
\item An action space $\Acal$, which also may be discrete or infinite.
For mathematical convenience, we will assume that $\Acal$ is
finite.
\item A transition function $P: \Scal\times \Acal \to \Delta(\Scal)$,
  where $\Delta(\Scal)$ is the space of probability distributions over
  $\Scal$ (i.e., the probability simplex). $P(s' | s, a)$ is the
  probability of transitioning into state $s'$ upon taking action $a$
  in state $s$. We use $P_{s,a}$ to denote the vector $P(\cdot \cond s,a)$.
  % For finite $\Scal$, $P(s' \cond s,a)$ is the
                 %probability mass; for continuous $\Scal$, $P(s'
                 %\cond s,a)$ is the probability density.
\item A reward function $r: \Scal\times \Acal \to [0,1]$. $r(s,a)$ is
  the immediate reward associated with taking action $a$ in state
  $s$. More generally, the $r(s,a)$ could be a random variable (where
  the distribution depends on $s,a$). While we largely focus on the
  case where $r(s,a)$ is deterministic, the extension to methods with
  stochastic rewards are often straightforward.
\item A discount factor $\gamma \in [0, 1)$, which defines a horizon for
  the problem.
\item An initial state distribution $\mu \in \Delta(\Scal)$, which
  specifies how the initial state $s_0$ is generated.
\end{itemize}


In many cases, we will assume that the initial state is fixed at
$s_0$, i.e. $\mu$ is a distribution supported only on $s_0$.

\iffalse
In some situations, it is necessary to specify how the initial state
$s_0$ is generated. We consider $s_0$ sampled from an initial
distribution $\mu \in \Delta(\Scal)$. When $\mu$ is of importance to
the discussion, we include it as part of the MDP definition, and write
$M = (\Scal, \Acal, P, r, \gamma, \mu)$.
\fi

%\subsection{Interaction protocol}
%\label{sec:mdp_eval_protocol}


\subsection{The objective, policies, and values}
\label{sec:background_trajectory}

\paragraph{Policies.} In a given MDP $M = (\Scal, \Acal, P, r, \gamma,\mu)$, the agent interacts
with the environment according to the following protocol: the agent
starts at some state $s_0\sim \mu$; at each time step $t=0,1, 2, \ldots$, the
agent takes an action $a_t \in \Acal$, obtains the immediate reward
$r_t = r(s_t, a_t)$, and observes the next state $s_{t+1}$ sampled
according to $s_{t+1} \sim P(\cdot|s_t, a_t)$. The interaction
record at time $t$,
$$
\tau_t = (s_0, a_0, r_0, s_1, \ldots,s_t,a_t, r_t),
$$
is called a \emph{trajectory}, which includes the observed state at time $t$.

In the most general setting, a policy specifies a decision-making
strategy in which the agent chooses 
actions adaptively based on the history of observations; precisely, a
policy is a (possibly
randomized) mapping from a trajectory to an action, i.e. $\pi: \Hcal \to \Delta(\Acal)$ where $\Hcal$ is the set of
all possible trajectories (of all lengths) and
$\Delta(\Acal)$ is the space of probability distributions over
  $\Acal$.
A \emph{stationary} policy $\pi: \Scal \to \Delta(\Acal)$
specifies a decision-making strategy in which the agent chooses
actions based only on the current state, i.e. $a_t \sim \pi(\cdot|s_t)$.
A deterministic, stationary policy is of the form $\pi: \Scal \to
\Acal$.

\paragraph{Values.} We now define values for (general) policies. For a
fixed policy and a starting state $s_0=s$, we define the value
function $V_M^\pi: \Scal \to \RR$ as the discounted sum of future
rewards
\[
V_M^\pi(s) = \Expe{\sum_{t=0}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi, s_0 = s} \, .
\]
where expectation is with respect to the randomness of the trajectory,
that is, the randomness in state transitions and the stochasticity of
$\pi$.  Here,
since $r(s,a)$ is bounded between $0$ and $1$, we have  $0\leq
V_M^\pi(s) \leq 1/(1-\gamma)$.

Similarly, the action-value (or Q-value) function $Q_M^\pi: \Scal
\times \Acal \to \RR$ is defined as
\[
Q_M^\pi(s,a) = \Expe{\sum_{t=0}^\infty \gamma^t  r(s_t, a_t) \cond \pi, s_0 = s, a_0 = a}.
\]
and $Q_M^\pi(s,a)$ is also bounded by $1/(1-\gamma)$.

\paragraph{Goal.} Given a state $s$, the goal of the agent is to find a policy $\pi$ that maximizes the value,
i.e. the optimization problem the agent seeks to solve is:
\begin{align} \label{eq:expected_return}
\max_\pi V_M^\pi(s)
\end{align}
where the max is over all (possibly non-stationary and randomized)
policies. As we shall see, there exists a deterministic and
stationary policy which is simultaneously  optimal for all starting
states $s$.
%This fact will be important when we later analyze the
%error propagation of planning and learning algorithms.

We drop the dependence on $M$ and write $V^\pi$ when it is clear from context.

\begin{example}[Navigation]
  Navigation is perhaps the simplest to see example of RL. The state
  of the agent is their current location. The four actions might be
  moving 1 step along each of east, west, north or south. The
  transitions in the simplest setting are deterministic. Taking the
  north action moves the agent one step north of their location,
  assuming that the size of a step is standardized. The agent might
  have a goal state $g$ they are trying to reach, and the reward is 0
  until the agent reaches the goal, and 1 upon reaching the goal
  state. Since the discount factor $\gamma < 1$, there is incentive to
  reach the goal state earlier in the trajectory. As a result, the
  optimal behavior in this setting corresponds to finding the shortest
  path from the initial to the goal state, and the value function of a
  state, given a policy is $\gamma^d$, where $d$ is the
  number of steps required by the policy to reach the goal state.
\end{example}


\begin{example}[Conversational agent]
  This is another fairly natural RL problem. The state of an agent can
  be the current transcript of the conversation so far, along with any
  additional information about the world, such as the context for the
  conversation, characteristics of the other agents or humans in the
  conversation etc. Actions depend on the domain. In the most basic form,
  we can think of it as the next statement to make in the
  conversation. Sometimes, conversational agents are designed for task
  completion, such as travel assistant or tech support or a virtual
  office receptionist. In these cases, there might be a predefined set
  of \emph{slots} which the agent needs to fill before they can find a
  good solution. For instance, in the travel agent case, these might
  correspond to the dates, source, destination and mode of travel. The
  actions might correspond to natural language queries to fill these
  slots.

  In task completion settings, reward is naturally defined as a binary
  outcome on whether the task was completed or not, such as whether
  the travel was successfully booked or not. Depending on the domain,
  we could further refine it based on the quality or the price of the
  travel package found. In more generic conversational settings, the
  ultimate reward is whether the conversation was satisfactory to the
  other agents or humans, or not.
\end{example}

\begin{example}[Strategic games]
  This is a popular category of RL applications, where
  RL has been successful in achieving human level performance in
  Backgammon, Go, Chess, and various
  forms of Poker. The usual setting consists of the
  state being the current game board, actions being the potential next
  moves and reward being the eventual win/loss outcome or a more
  detailed score when it is defined in the game. Technically, these
  are multi-agent RL settings, and, yet, the algorithms used are often non-multi-agent
  RL algorithms.
\end{example}

\subsection{Bellman Consistency Equations for Stationary Policies}
\label{sec:background_policyvalue}

Stationary policies satisfy the following consistency conditions:

\begin{lemma}\label{lemma:BEPE} Suppose that $\pi$ is a stationary policy. Then $V^\pi$ and $Q^\pi$
satisfy the following \emph{Bellman consistency equations}: for all $s \in \Scal, a\in\Acal$,
\begin{align*}
\begin{split} 
V^\pi(s) = &~ Q^\pi(s, \pi(s)). \\
Q^\pi(s,a) = &~ r(s,a) + \gamma \EE_{a\sim \pi(\cdot|s), s' \sim P(\cdot|s, a)}\big[
V^\pi(s') \big] \, .
\end{split}
\end{align*}
%where we are treating $\pi$ as a deterministic policy.
\end{lemma}
%For stochastic policies, this shorthand should be interpreted as $\EE_{a\sim \pi(s)} [Q^\pi(s, a)]$.

We leave the proof as an exercise to the reader.

It is helpful to view $V^\pi$ as vector of length $|\Scal|$ and $Q^\pi$
and $r$ as vectors of length $|\Scal|\cdot |\Acal|$.
We overload notation and let $P$ also refer to a matrix
  of size $(|\Scal|\cdot |\Acal|)\times |\Scal|$ where the entry
  $P_{(s,a),s'}$ is equal to $P(s'|s,a)$.

We also will define $P^\pi$ to be the transition matrix on state-action pairs induced by a
  stationary policy $\pi$, specifically:
%For a stationary policy, define have
\[
P^\pi_{(s,a),(s',a')}:=P(s'|s,a)\pi(a'|s').
\]
In particular, for deterministic policies we have:
\[
 P^\pi_{(s,a),(s',a')} :=      \left\{ \begin{array}{rr}
         P(s'|s,a) & \mbox{if } a'=\pi(s')
\\
0 & \mbox{if } a'\neq\pi(s')
                \end{array}\right.
\]

With this notation,
it is straightforward to verify:
\begin{align*}
  Q^\pi & = r+ \gamma PV^\pi \\
  Q^\pi &= r+ \gamma P^\pi Q^\pi \,.
\end{align*}

\begin{corollary}
\label{corollary:policy_value_matrix_form}
Suppose that $\pi$ is a stationary policy. We have that:
\begin{equation}\label{eq:policy_value_matrix_form}
  Q^\pi = (I-\gamma P^\pi)^{-1} r
\end{equation}
where $I$ is the identity matrix.
\end{corollary}

\begin{proof}
To see that the $I - \gamma P^\pi$ is invertible,
observe that  for any non-zero vector $x \in \RR^{|\Scal||\Acal|}$,
\begin{align*}
\maxnorm{(I - \gamma P^\pi) x }
= &~\|x - \gamma P^\pi x \|_\infty \\
\ge &~ \|x\|_\infty - \gamma \|P^\pi x \|_\infty  \tag{triangule inequality for norms} \\
\ge &~ \|x\|_\infty - \gamma \|x \|_\infty \tag{each element of $P^\pi
      x$ is an average of $x$} \\
= &~ (1-\gamma) \|x\|_\infty > 0 \tag{$\gamma < 1$, $x \ne 0$}
\end{align*}
which implies $I - \gamma P^\pi$ is full rank.
\end{proof} 

The following is also a helpful lemma:

\begin{lemma}\label{eq:sa_measure}
We have that:
\[
    [(1-\gamma) (I-\gamma P^\pi)^{-1}]_{(s,a), (s',a')} = (1-\gamma) \sum_{t=0}^\infty \gamma^t \mathbb{P}^{\pi}(s_t=s',a_t=a' | s_0=s,a_0=a)
\]
so we can view the $(s,a)$-th row of this matrix as an induced distribution over states and actions
when following $\pi$ after starting with $s_0=s$ and $a_0=a$.
\end{lemma}

We leave the proof as an exercise to the reader. 


\subsection{Bellman Optimality Equations}

A remarkable and convenient property of MDPs is that
there exists a stationary and deterministic policy that
simultaneously  maximizes $V^{\pi}(s)$ for all $s\in\Scal$.
This is formalized in the following
theorem:

\iffalse
and
maximizes $Q^\pi(s,a)$ for all $s\in\Scal, a\in\Acal$; we denote this \emph{optimal policy} as
$\pi_M^\star$ (or $\pi^\star$).  
\fi

\begin{theorem}
\label{thm:bellman-stat}
Let $\Pi$ be the set of all non-stationary and randomized policies.
Define:
\begin{align*}
V^\star(s)&:=\sup_{\pi \in \Pi} V^{\pi}(s)\\
Q^\star(s,a)&:=\sup_{\pi \in \Pi} Q^{\pi}(s,a).
\end{align*}
which is finite since $V^{\pi}(s)$ and $Q^{\pi}(s,a)$ are bounded between $0$
and $1/(1-\gamma)$.

There exists a stationary and deterministic policy $\pi$ such
that for all $s \in \Scal$ and $a\in\Acal$,
\begin{align*}
V^{\pi}(s) &=V^\star(s)\\
Q^{\pi}(s,a)&=Q^\star(s,a).
\end{align*}
We refer to such a $\pi$ as an optimal policy.
\end{theorem}

\begin{proof}
  For any $\pi \in \Pi$ and for any time $t$, $\pi$ specifies a
  distribution over actions conditioned on the history of
  observations; here, we write
  $\pi(A_t = a| S_0 = s_0, A_0=a_0, R_0=r_0, \ldots S_{t-1} = s_{t-1},
  A_{t-1}=a_{t-1}, R_{t-1}=r_{t-1}, S_t = s_t)$
  as the probability that $\pi$ selects action $a$ at time $t$ given
  an observed history
  $s_0, a_0, r_0, \ldots s_{t-1}, a_{t-1}, r_{t-1}, s_t$. For the
  purposes of this proof, it is helpful to formally let $S_t$, $A_t$
  and $R_t$ denote random variables, which will distinguish them from
  outcomes, which is denoted by lower case variables.  First, let us show that
  conditioned on $(S_0,A_0,R_0,S_1)=(s,a,r,s')$, the maximum future
  discounted value, from time $1$ onwards, is not a function of
  $s,a,r$. More precisely, we seek to show that:
\begin{equation}\label{eq:to_show_1step}
\sup_{\pi \in \Pi}\Expe{\sum_{t=1}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi, (S_0,A_0,R_0,S_1)=(s,a,r,s') } 
=\gamma V^\star(s')
\end{equation}
%To see this, first recall that a policy is (randomized) mapping from
%a trajectory $(s_0, a_0, r_1, s_1,\ldots,s_t)$ to an action. 
For any policy $\pi$, define an ``offset'' policy $\pi_{(s,a,r)}$, which is the policy that 
chooses actions on a trajectory $\tau$ according to the same
distribution that $\pi$ chooses actions on the trajectory
 $(s,a,r,\tau)$. Precisely, for all $t$, define 
\begin{align*}
&\pi_{(s,a,r)}(A_t = a|S_0 = s_0, A_0=a_0, R_0=r_0, \ldots S_t = s_t)\\
& \quad :=
\pi(A_{t+1} = a| S_0 = s, A_0=a, R_0=r, S_1 = s_0, A_1=a_0, R_1=r_0, \ldots S_{t+1} = s_t).
\end{align*}
%For example, $\pi_{(s,a,r)}(a_0=a'|s_0=s')$ is equal
% to the probability $\pi(a_1=a'|(s_0,a_0,r_0,s_1)=(s,a,r,s'))$. 
By the
 Markov property, we have that: 
\[
\Expe{\sum_{t=1}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi, (S_0,A_0,R_0,S_1)=(s,a,r,s') } = 
\gamma \Expe{\sum_{t=0}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi_{(s,a,r)}, S_0=s' } = 
\gamma V^{\pi_{(s,a,r)} }(s'),
\]
where the first equality follows from a change of variables on the
time index, along with the definition of the
policy $\pi_{(s,a,r)}$.
Also, we have that, for all
$(s,a,r)$, that the set
$\{\pi_{(s,a,r)} | \pi\in\Pi\} $ is equal to $\Pi$ itself, by
the definition of $\Pi$ and $\pi_{(s,a,r)}$.
This implies:
\[
\sup_{\pi \in \Pi}\Expe{\sum_{t=1}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi, (S_0,A_0,R_0,S_1)=(s,a,r,s') } = 
\gamma \cdot \sup_{\pi \in \Pi} V^{\pi_{(s,a,r)} }(s')= \gamma \cdot \sup_{\pi\in \Pi} V^{\pi}(s')
=\gamma V^\star(s'),
\]
thus proving Equation~\ref{eq:to_show_1step}.

We now show the deterministic and stationary policy 
\[
\widetilde\pi(s)=\sup_{a \in \Acal}\Expe{r(s,a) +\gamma V^\star(s_1)
  \cond (S_0,A_0)=(s,a)} 
\]
is optimal, i.e. that $V^{\widetilde\pi}(s) = V^\star(s)$.
%$\widetilde\pi(s) :=\argmax_{a \in \Acal} \sup_{\pi\in \Pi}Q^{\pi}(s, a)$ 
%satisfies $V^{\widetilde\pi}(s) = \sup_{\pi \in \Pi} V^{\pi}(s)$.
For this, we have that: 
\begin{align*}
V^\star(s_0) &= \sup_{\pi \in \Pi}\Expe{r(s_0,a_0) +\sum_{t=1}^\infty \gamma^t r(s_t,  a_t) } \\
&\stackrel{(a)}{=}   
\sup_{\pi \in \Pi}\Expe{r(s_0,a_0) +\Expe{\sum_{t=1}^\infty \gamma^t r(s_t,  a_t) \cond \pi, (S_0,A_0,R_0,S_1) =(s_0,a_0,r_0,s_1) }} \\
&\leq   
\sup_{\pi \in \Pi}\Expe{r(s_0,a_0) +\sup_{\pi' \in \Pi}\Expe{\sum_{t=1}^\infty \gamma^t r(s_t,  a_t) \cond \pi', (S_0,A_0,R_0,S_1) =(s_0,a_0,r_0,s_1) }} \\
&\stackrel{(b)}{=}  
\sup_{\pi \in \Pi}\Expe{r(s_0,a_0) +\gamma V^\star(s_1)} \\
&=  
\sup_{a_0 \in \Acal}\Expe{r(s_0,a_0) +\gamma V^\star(s_1)} \\
&\stackrel{(c)}{=}  \Expe{r(s_0,a_0) +\gamma V^\star(s_1)\cond \widetilde\pi}.
\end{align*}
where step $(a)$ uses the law of iterated 
expectations;  step $(b)$ uses
Equation~\ref{eq:to_show_1step}; and step $(c)$ follows from
the definition of $\widetilde\pi$.
Applying the same argument recursively leads to: 
\[
V^\star(s_0) \leq \Expe{r(s_0,a_0) +\gamma V^\star(s_1) \cond \widetilde\pi}
\leq \Expe{r(s_0,a_0) +\gamma r(s_1,a_1)+\gamma^2 V^\star(s_2) \cond \widetilde\pi}
\leq \ldots \leq V^{\widetilde\pi}(s_0).
\]
Since $V^{\widetilde\pi}(s) \leq \sup_{\pi\in \Pi} V^{\pi}(s)=
V^\star(s)$ for all $s$, we have that
$V^{\widetilde\pi}=V^\star$, which completes the proof of the first claim.

For the same policy $\widetilde\pi$, an analogous argument can be used prove the second claim.
\end{proof}

\iffalse
\begin{proof}
First, let us show that conditioned on $(s_0,a_0,r_0,s_1)=(s,a,r,s')$,
the maximum future discounted value, from time $1$ onwards, is not a function of
$s,a,r$. Specifically, 
\[
\sup_{\pi \in \Pi}\Expe{\sum_{t=1}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi, (s_0,a_0,r_0,s_1)=(s,a,r,s') } 
=\gamma V^\star(s')
\]
%To see this, first recall that a policy is (randomized) mapping from
%a trajectory $(s_0, a_0, r_1, s_1,\ldots,s_t)$ to an action. 
For any policy $\pi$, define an ``offset'' policy $\pi_{(s,a,r)}$, which is the policy that 
chooses actions on a trajectory $\tau$ according to the same
distribution that $\pi$ chooses actions on the trajectory
 $(s,a,r,\tau)$. For example, $\pi_{(s,a,r)}(a_0=a'|s_0=s')$ is equal
 to the probability $\pi(a_1=a'|(s_0,a_0,r_0,s_1)=(s,a,r,s'))$. By the
 Markov property, we have that: 
\[
\Expe{\sum_{t=1}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi, (s_0,a_0,r_0,s_1)=(s,a,r,s') } = 
\gamma \Expe{\sum_{t=0}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi_{(s,a,r)}, s_0=s' } = 
\gamma V^{\pi_{(s,a,r)} }(s').
\]
Hence, due to that $V^{\pi}(s')$ is not a function of
$(s,a,r) $, we have
\[
\sup_{\pi \in \Pi}\Expe{\sum_{t=1}^\infty \gamma^t  r(s_t, a_t)
  \cond \pi, (s_0,a_0,r_0,s_1)=(s,a,r,s') } = 
\gamma \cdot \sup_{\pi \in \Pi} V^{\pi_{(s,a,r)} }(s')= \gamma \cdot \sup_{\pi\in \Pi} V^{\pi}(s')
=\gamma V^\star(s'),
\]
thus proving the claim.

We now show the deterministic and stationary policy $\pi(s) =\argmax_{a \in \Acal}
\sup_{\pi' \in \Pi}Q^{\pi'}(s, a)$ satisfies $V^\pi(s) =
\sup_{\pi' \in \Pi} V^{\pi'}(s)$.
For this, we have that: 
\begin{align*}
V^\star(s_0) &= \sup_{\pi \in \Pi}\Expe{r(s_0,a_0) +\sum_{t=1}^\infty \gamma^t r(s_t,  a_t) } \\
&=   
\sup_{\pi \in \Pi}\Expe{r(s_0,a_0) +\Expe{\sum_{t=1}^\infty \gamma^t r(s_t,  a_t) \cond \pi, (s_0,a_0,r_0,s_1) }} \\
&\leq   
\sup_{\pi \in \Pi}\Expe{r(s_0,a_0) +\sup_{\pi' \in \Pi}\Expe{\sum_{t=1}^\infty \gamma^t r(s_t,  a_t) \cond \pi', (s_0,a_0,r_0,s_1) }} \\
&=  
\sup_{\pi \in \Pi}\Expe{r(s_0,a_0) +\gamma V^\star(s_1)} \\
&=  \Expe{r(s_0,a_0) +\gamma V^\star(s_1)\cond \pi}.
\end{align*}
where the second equality is by the  tower property of conditional
expectations, and the last equality follows from the definition of $\pi$.
Now, by recursion, 
\[
V^\star(s_0) \leq \Expe{r(s_0,a_0) +\gamma V^\star(s_1) \cond \pi}
\leq \Expe{r(s_0,a_0) +\gamma r(s_1,a_1)+\gamma^2 V^\star(s_2) \cond \pi}
\leq \ldots \leq V^\pi(s_0).
\]
Since $V^{\pi}(s) \leq \sup_{\pi' \in \Pi} V^{\pi'}(s)= V^\star(s)$, we have that
$V^\pi=V^\star$, which completes the proof of the first claim.

For the same policy $\pi$, an analogous argument can be used prove the second claim.
\end{proof}
\fi

This shows that we may restrict ourselves to using stationary and
deterministic policies without any loss in performance.  The following
theorem, also due to \cite{bellman1956dynamic}, gives a precise
characterization of the optimal value function. 

\begin{theorem}[Bellman optimality equations]
\label{thm:bellman-opt}
We say that a
vector $Q \in \R^{|\Scal||\Acal|}$ satisfies the \emph{Bellman
  optimality equations} if:
\[
Q(s,a) =  r(s,a) + \gamma \EE_{s' \sim P(\cdot|s, a)}\left[ \max_{a'\in\Acal} Q(s',a') \right].
\]
For any $Q \in \R^{|\Scal||\Acal|}$, we have that $Q=Q^\star$ if and
only if $Q$ satisfies the Bellman optimality equations. Furthermore,
the deterministic policy defined by
$\pi(s)\in \argmax_{a\in\Acal}Q^\star(s,a)$ is an optimal policy (where
ties are broken in some arbitrary manner).
\end{theorem}

Before we prove this claim, we will provide a few definitions.
%We use $V^\star$ and $Q^\star$ as a shorthand for $V^{\pi^\star}$
%and $Q^{\pi^\star}$, respectively. We follow this convention throughout
%this book. 
Let $\pi_{Q}$ denote the greedy policy with
respect to a vector $Q \in \RR^{|\Scal||\Acal|}$, i.e
\[
\pi_Q(s) := \argmax_{a \in \Acal} Q(s, a) \, .
\]
where ties are broken in some arbitrary manner.
With this notation, by the above theorem, the optimal policy
$\pi^\star$ is given by:
\[
\pi^\star = \pi_{Q^{\star}} \, .
\]

Let us also use the following notation to turn a vector $Q \in \RR^{|\Scal||\Acal|}$
into a vector of length $|\Scal|$.
\[
V_Q(s) := \max_{a\in\Acal} Q(s,a).
\]
The \emph{Bellman optimality operator} $\Tcal_M: \RR^{|\Scal||\Acal|} \to
\RR^{|\Scal||\Acal|}$ is defined as: 
\begin{align} \label{eq:bellman_update}
\Tcal Q := r + \gamma P V_Q \, .
\end{align}

This allows us to rewrite the Bellman optimality equation in the concise form:
\[
Q = \Tcal Q,
\]
and, so, the previous theorem states that $Q=Q^\star$ if and only if
$Q$ is a fixed point of the operator $\Tcal$.

\begin{proof}
Let us begin by showing that:
\begin{equation}\label{eq:step_bellman}
V^\star(s) = \max_a Q^\star(s,a).
\end{equation}
Let $\pi^\star$ be an
optimal stationary and deterministic policy, which exists by
Theorem~\ref{thm:bellman-stat}. 
Consider the policy which takes action $a$ and then follows
$\pi^\star$. Due to that $V^\star(s)$ is the maximum
value over all non-stationary policies (as shown in Theorem~\ref{thm:bellman-stat}),
\[
V^\star(s) \geq Q^{\pi^\star}(s,a) 
= Q^{\star}(s,a),
\]
which shows that $V^\star(s) \geq  \max_a Q^{\star}(s,a)$ since $a$
is arbitrary in the above. Also, by Lemma~\ref{lemma:BEPE} and
Theorem~\ref{thm:bellman-stat}, 
\[
V^\star(s)=V^{\pi^\star}(s)= Q^{\pi^\star}(s,\pi^\star(s))
\leq \max_a Q^{\pi^\star}(s,a)
= \max_a Q^\star(s,a) ,
\]
which proves our claim since we have upper and lower bounded
$V^\star(s)$ by $\max_a Q^\star(s,a)$.

\iffalse
By Lemma~\ref{lemma:BEPE} and Theorem~\ref{thm:bellman-stat}, we have that $V^\star(s) = V^{\pi^\star}(s) = Q^{\pi^\star}(s,\pi^\star(a))
=Q^\star(s,\pi^\star(a))$. Thus, $\max_a Q^\star(s,a) \geq Q^\star(s,\pi^\star(a)) =
V^\star(s)$. Also, by Theorem~\ref{thm:bellman-stat}, 
$\max_{\pi\in \Pi_{\textrm{det}} Q^\pi(s,a) = Q^{\star}(s,a)$,
  where$\Pi_{\textrm{det}}$ is the set of all
deterministic, stationary policies. This implies
\[
%\max_a Q^\star(s,a) \geq Q^\star(s,\pi^\star(a)) =
V^\star(s) \geq \max_a \max_{\pi\in\Pi_{\textrm{det}}} Q^\pi(s,a) 
%\geq
%\max_a Q^{\pi^\star}(s,a)
=\max_a Q^{\star}(s,a), 
\]
where the first inequality is due to that $V^\star(s)$ is the maximum
value over all non-stationary policies
which proves the claim, due to that we have shown that $V^\star(s)$ is
bounded above and below by $\max_a Q^\star(s,a)$.
\fi

We first show sufficiency, i.e. that $Q^\star$ (the state-action value of an optimal
policy) satisfies $Q^\star = \Tcal Q^\star$. 
Now for all actions $a\in \Acal$, we have:
\begin{align*}
Q^\star(s,a) &= \max_{\pi} Q^\pi(s,a)
= r(s,a) + \gamma \max_\pi \E_{s'\sim P(\cdot\given s,a)}[V^\pi(s')]\\
%&= (1-\gamma) r(s,a) + \max_\pi \E_{s'\sim P(\cdot\given s,a)}[V^\pi(s')]\\
&= r(s,a) + \gamma \E_{s'\sim P(\cdot\given s,a)}[V^\star(s')]\\
&= r(s,a) + \gamma \E_{s'\sim P(\cdot\given s,a)}[\max_{a'} Q^\star(s',a')].
\end{align*}
Here, the second equality follows from Theorem~\ref{thm:bellman-stat}
and the final equality follows from
Equation~\ref{eq:step_bellman}. This proves sufficiency.
%due to that there exists a policy that is optimal for every starting state.
%Thus $Q^\star$ satisfies the Bellman optimality equations.

For the converse, suppose $Q = \Tcal
Q$ for some $Q$. We now show that $Q=Q^\star$.
Let $\pi=\pi_Q$. That $Q = \Tcal
Q$ implies that $Q =r+
\gamma P^{\pi} Q$, and so:
\[
Q= (I-\gamma P^{\pi} )^{-1} r =Q^{\pi}
\]
using Corollary~\ref{corollary:policy_value_matrix_form} in the last step.
In other words, $Q$ is the action value of the policy $\pi_Q$.
Also, let us show that $ (P^\pi -P^{\pi'})
Q^\pi \geq 0$. To see this, observe that:
\[
  [(P^\pi -P^{\pi'}) Q^\pi]_{s,a} = \E_{s'\sim P(\cdot|s,a)}[Q^\pi(s',\pi(s'))-Q^\pi(s',\pi'(s'))]\geq 0
\]
where the last step uses that $\pi=\pi_Q$.
Now observe for any other deterministic and stationary policy $\pi'$:
\begin{eqnarray*}
   Q - Q^{\pi'}&= &  Q^\pi -Q^{\pi'} \\
&=& Q^\pi- (I-\gamma P^{\pi'})^{-1} r  \\
&= & (I-\gamma P^{\pi'})^{-1}((I-\gamma P^{\pi'}) -(I-\gamma P^\pi)) Q^\pi\\
&= & \gamma (I-\gamma P^{\pi'})^{-1}( P^\pi-P^{\pi'})
     Q^\pi\\
&\geq & 0,
\end{eqnarray*}
where the last step follows since 
we have shown $ (P^\pi -P^{\pi'})
Q^\pi \geq 0$ and since $(1-\gamma)
(I-\gamma P^{\pi'})^{-1}$ is a matrix with all positive
entries (see Lemma~\ref{eq:sa_measure}). Thus, $Q^\pi=Q \geq
Q^{\pi'}$ for all deterministic and stationary $\pi'$, which shows that
$\pi$ is an optimal policy. Thus, $Q=Q^\pi=Q^\star$, using Theorem~\ref{thm:bellman-stat}.
This completes the proof.
\end{proof}


\section{Finite-Horizon Markov Decision Processes}
\label{section:finite_horizon}

In some cases, it is natural to work with finite-horizon (and
time-dependent) Markov Decision Processes (see
the discussion below).  Here,
a finite horizon, time-dependent Markov
Decision Process (MDP) $M = (\Scal, \Acal, \{P\}_h, \{r\}_h, H, \mu)$ is
specified as follows:
\begin{itemize}
\item A state space $\Scal$, which may be finite or infinite.
\item An action space $\Acal$, which also may be discrete or infinite.
\item A time-dependent transition function
  $P_h: \Scal\times \Acal \to \Delta(\Scal)$, where $\Delta(\Scal)$ is
  the space of probability distributions over $\Scal$ (i.e., the
  probability simplex). $P_h(s' | s, a)$ is the probability of
  transitioning into state $s'$ upon taking action $a$ in state $s$ at
  time step $h$. Note that the time-dependent setting generalizes the
  stationary setting where all steps share the same transition.
\item A time-dependent reward function $r_h: \Scal\times \Acal \to [0,1]$. $r_h(s,a)$ is
  the immediate reward associated with taking action $a$ in state $s$ at time step $h$.
\item A integer $H$ which defines the horizon of
  the problem.
\item An initial state distribution $\mu \in \Delta(\Scal)$, which
  species how the initial state $s_0$ is generated.
\end{itemize}

Here, for a policy $\pi$, a state $s$, and $h \in \{0,\ldots H-1\}$, we define the value
function $V_h^\pi: \Scal \to \RR$ as 
\[
V_h^\pi(s) = \Expe{\sum_{t=h}^{H-1}  r_h(s_t, a_t)
  \cond \pi, s_h = s}  ,
\]
where again the expectation is with respect to the randomness of the trajectory,
that is, the randomness in state transitions and the stochasticity of
$\pi$.   Similarly, the state-action value (or Q-value) function $Q_h^\pi: \Scal
\times \Acal \to \RR$ is defined as
\[
Q_h^\pi(s,a) = \Expe{\sum_{t=h}^{H-1}   r_h(s_t, a_t) \cond \pi, s_h = s, a_h = a}.
\]
We also use the notation $V^\pi(s)=V_0^\pi(s)$.

Again, given a state $s$, the goal of the agent is to find a policy $\pi$ that maximizes the value,
i.e. the optimization problem the agent seeks to solve is:
\begin{align} \label{eq:expected_return}
\max_\pi V^\pi(s)
\end{align}
where recall that $V^\pi(s)=V_0^\pi(s)$.

\begin{theorem}\label{thm:Bellman_finite}
(Bellman optimality equations) Define
\[
Q^\star_h(s,a) = \sup_{\pi\in\Pi} Q_h^\pi(s,a)
\]
where the $\sup$ is over all non-stationary and randomized
policies. Suppose that $Q_{H}=0$.  We have that $Q_h=Q_h^\star$ for
all $h\in [H]$ if and only if for all $h\in [H]$,
\begin{equation}\label{eqn:Beq_finite}
Q_h(s,a) =  r_h(s,a) + \EE_{s' \sim P_h(\cdot|s, a)}\left[ \max_{a'\in\Acal} Q_{h+1} (s',a') \right].
\end{equation}
Furthermore, $\pi(s,h) = \argmax_{a\in \Acal} Q_h^\star(s,a)$ is an optimal policy.
\end{theorem}

We leave the proof as an exercise to the reader.

\paragraph{Discussion: 
Stationary MDPs vs Time-Dependent MDPs}
For the purposes of this book, it is natural for us to study both of
these models, where we typically assume stationary dynamics in the
infinite horizon setting and time-dependent dynamics in the
finite-horizon setting. From a theoretical perspective, the finite horizon, 
\emph{time-dependent} setting is often more amenable to analysis, where
optimal statistical rates often require simpler arguments. 
However, we should note that from a practical perspective,
time-dependent MDPs are rarely utilized because they lead to
policies and value functions that are $O(H)$ larger (to store in
memory) than those in
the stationary setting. In practice, we often incorporate
temporal information directly into the definition of the state, which
leads to more compact value functions and policies (when coupled with
function approximation methods, which attempt to represent both the values and
policies in a more compact form).


\section{Computational Complexity} \label{sec:complexity}

This section will be concerned with computing an
optimal policy, when the MDP $M = (\Scal, \Acal, P,
r, \gamma)$ is known; this can be thought of as the solving the
\emph{planning} problem.  While much of this book is concerned with statistical
limits, understanding the computational limits can be informative.
We will consider algorithms which give both exact and
approximately optimal policies. In particular, we will be interested
in polynomial time (and strongly polynomial time) algorithms.

Suppose that $(P, r, \gamma)$ in our MDP $M$ is specified  with
rational entries. Let $L(P,r,\gamma)$ denote the total bit-size
required to specify $M$, and assume that basic arithmetic operations $+,-,\times,\div$
take unit time. Here, we may hope for an algorithm which (exactly) returns an
optimal policy whose runtime is polynomial in  $L(P,r,\gamma)$ and the
number of states and actions.

More generally, it may also be helpful to understand which algorithms
are \emph{strongly} polynomial. Here, we do not want to explicitly restrict $(P, r,
\gamma)$ to be specified by rationals. An algorithm is said to be
strongly polynomial if it returns an optimal policy with runtime that
is polynomial in only the number of states and actions (with no
dependence on $L(P,r,\gamma)$).

\begin{table*}[t!]
\centering
%\aboverulesep=0ex
% \belowrulesep=0ex
%\ra{2}
\begin{tabular}{ |c|c|c|c| }
%{|>{\centering\arraybackslash}m{9.5cm}|>{\centering\arraybackslash}m{2.8cm}|>{\centering\arraybackslash}m{3cm}|}
%\toprule
\hline
&Value Iteration & Policy Iteration & LP-Algorithms\\ \hline
Poly?  & $|\Scal|^2 |\Acal|\frac{ L(P,r,\gamma) \log \frac{1}{1-\gamma}}{1-\gamma}$
& $(|\Scal|^3 +|\Scal|^2|\Acal|)\frac{ L(P,r,\gamma) \log \frac{1}{1-\gamma}}{1-\gamma}$
& $|\Scal|^3 |\Acal| L(P,r,\gamma)$\\ \hline
Strongly Poly?   & \xmark
&
$(|\Scal|^3 +|\Scal|^2|\Acal|)\cdot \min\left\{ \frac{|\Acal|^{|\Scal|}}{|\Scal|},
\frac{|\Scal|^2 |\Acal| \log\frac{|\Scal|^2 }{1-\gamma}}{1-\gamma}
\right\}$
& $|\Scal|^4 |\Acal|^4 \log\frac{|\Scal| }{1-\gamma}$ \\ \hline
\end{tabular}
\caption{Computational complexities of various approaches (we drop
  universal constants).
  Polynomial time algorithms depend on the bit complexity,
  $L(P,r,\gamma)$, while strongly polynomial algorithms do not.
  Note that only for a fixed value of $\gamma$ are value and policy iteration
  polynomial time algorithms; otherwise, they are not polynomial time algorithms.
  Similarly, only for a fixed value of $\gamma$ is policy iteration a strongly
  polynomial time algorithm. In contrast, the LP-approach leads to both
  polynomial time and strongly polynomial time algorithms; for the
  latter, the approach is an interior point algorithm.  See text for
  further discussion, and  Section~\ref{ch1:bib} for
  references. Here, $|\Scal|^2 |\Acal|$ is the assumed runtime per
  iteration of value iteration, and $|\Scal|^3 +|\Scal|^2|\Acal|$ is the
  assumed runtime per iteration of policy iteration (note that for this
  complexity we would directly update the values $V$ rather than $Q$
  values, as described in the text); these runtimes are consistent with
  assuming cubic complexity for linear system solving.
}
\label{table:computation}
\end{table*}


The first two subsections will cover classical \emph{iterative}
algorithms that compute $Q^\star$, and then we cover the linear
programming approach.

\iffalse
\section{Iterative Methods} \label{sec:mdpplanning}
\emph{Planning} refers to the problem of computing $\pi_M^\star$ given
the MDP specification $M = (\Scal, \Acal, P, r, \gamma)$. This section
reviews classical planning algorithms that compute $Q^\star$.
\fi

\subsection{Value Iteration}
Perhaps the simplest algorithm for discounted MDPs is to iteratively apply the fixed point
mapping: starting at some $Q$, we iteratively apply $\Tcal$:
\begin{eqnarray*}
Q \leftarrow \Tcal Q \, ,
\end{eqnarray*}
This is algorithm is referred to as \emph{$Q$-value iteration}.

\begin{lemma}(contraction)\label{lemma:contraction}
For any two vectors $Q,Q' \in \RR^{|\Scal||\Acal|}$,
\[
  \| \Tcal Q - \Tcal Q'\|_\infty \leq \gamma \|Q-Q'\|_\infty
\]
\label{lemma:contraction}
\end{lemma}
\begin{proof}
First, let us show that for all $s\in\Scal$, $|V_Q(s) - V_{Q'}(s)| \leq \max_{a\in\Acal} |Q(s,a) - Q'(s,a)|$.
Assume $V_Q(s) > V_{Q'}(s)$ (the other direction is symmetric), and let $a$ be the greedy action for $Q$ at $s$. Then
\begin{align*}
 |V_Q(s) - V_{Q'}(s)| = Q(s, a) - \max_{a'\in\Acal}Q'(s,a')
\le  Q(s, a) - Q'(s, a)
\le \max_{a\in\Acal} |Q(s,a) - Q'(s,a)|.
\end{align*}
Using this,
\begin{eqnarray*}
  \| \Tcal Q - \Tcal Q'\|_\infty &=& \gamma \|P V_Q-P V_{Q'}\|_\infty\\
&=& \gamma \|P (V_Q- V_{Q'})\|_\infty\\
&\leq& \gamma \|V_Q-V_{Q'}\|_\infty\\
&=& \gamma \max_s|V_Q(s)-V_{Q'}(s)|\\
&\leq& \gamma \max_s \max_a|Q(s,a)-Q'(s,a)|\\
&=& \gamma \|Q-Q'\|_\infty
\end{eqnarray*}
where the first inequality uses that each element of $P (V_Q- V_{Q'})$ is a
convex average of $V_Q- V_{Q'}$ and the second inequality uses our
claim above.
\end{proof}

The following result bounds the sub-optimality of the greedy policy
itself, based on the error in $Q$-value function.
\begin{lemma}\label{lemma:Q_to_policy}
($Q$-Error Amplification) For any vector $Q \in \RR^{|\Scal||\Acal|}$,
\[
V^{\pi_{Q}} \geq V^{\star}
-\frac{2 \|Q - Q^\star\|_\infty}{1-\gamma} \mathds{1},
\]
where $\mathds{1}$ denotes the vector of all ones.
\end{lemma}

\begin{proof}
Fix state $s$ and let $a=\pi_Q(s)$. We have:
\begin{eqnarray*}
V^\star(s) - V^{\pi_Q}(s)
& = & Q^\star(s, \pi^\star(s)) -
    Q^{\pi_Q}(s, a) \\
 & = & Q^\star(s, \pi^\star(s)) - Q^\star(s, a) + Q^\star(s, a) - Q^{\pi_Q}(s, a) \\
&= & Q^\star(s, \pi^\star(s)) - Q^\star(s, a)
+ \gamma \EE_{s' \sim P(\cdot | s, a)} [V^\star(s') -V^{\pi_Q}(s')]\\
&\le &Q^\star(s, \pi^\star(s)) - Q(s, \pi^\star(s)) + Q(s, a) - Q^\star(s, a)  \\
&& + \gamma \EE_{s' \sim P(s, a)} [V^\star(s') - V^{\pi_Q}(s')]\\
&\le & 2 \|Q - Q^\star\|_\infty + \gamma \|V^\star - V^{\pi_Q}\|_\infty.
\end{eqnarray*}
where the first inequality uses $Q(s, \pi^\star(s)) \leq  Q(s,
\pi_Q(s))=Q(s,a)$ due to the definition of $\pi_Q$.
\end{proof}


\begin{theorem}\label{thm:qiter_conv}
  ($Q$-value iteration convergence). Set $Q^{(0)}=0$. For
  $k=0,1,\ldots$, suppose:
  \[
Q^{(k+1)}= \Tcal Q^{(k)}
\]
Let $\pi^{(k)}=\pi_{Q^{(k)}}$. For $k\geq \frac{\log \frac{2}{ (1-\gamma)^2 \eps}}{1-\gamma}$,
\[
V^{\pi^{(k)}} \geq V^{\star} -\eps \mathds{1} \, .
\]
\end{theorem}

\begin{proof}
Since $\|Q^\star\|_\infty\leq 1/(1-\gamma)$, $Q^{(k)} = \Tcal^kQ^{(0)}$ and $Q^\star = \Tcal Q^\star$,  Lemma~\ref{lemma:contraction} gives
  \[
\|Q^{(k)}-Q^\star\|_\infty = \|\Tcal^k Q^{(0)} - \Tcal^kQ^\star\|_\infty \leq \gamma^k \|Q^{(0)}-Q^\star\|_\infty
= (1-(1-\gamma))^k \|Q^\star\|_\infty \leq \frac{\exp(-(1-\gamma)k)}{1-\gamma}\, .
\]
The proof is completed with our choice of $k$ and using Lemma~\ref{lemma:Q_to_policy}.
\end{proof}

\paragraph{Iteration complexity for an exact solution.}
With regards to computing an exact optimal policy, when the  gap
between the current objective value and the optimal objective value is
smaller than $2^{-L(P,r,\gamma)}$, then the greedy policy will be
optimal. This leads to claimed complexity in
Table~\ref{table:computation}. Value iteration is not strongly
polynomial algorithm due to that, in finite time, it may never return
the optimal policy.

\subsection{Policy Iteration}

The policy iteration algorithm, for discounted MDPs, starts from an arbitrary policy
$\pi_0$, and repeats the following iterative procedure: for $k = 0,1, 2,
\ldots$
\begin{enumerate}
\item \emph{Policy evaluation.} Compute $Q^{\pi_{k}}$
\item \emph{Policy improvement.} Update the policy:
\begin{align*}
  \pi_{k+1} = \pi_{Q^{\pi_k}}
\end{align*}
\end{enumerate}
In each iteration, we compute the Q-value function of $\pi_k$,
using the analytical form given in
Equation~\ref{eq:policy_value_matrix_form}, and update the
policy to be greedy with respect to this new $Q$-value. The first step is often called
\emph{policy evaluation}, and the second step is often called
\emph{policy improvement}.

\begin{lemma}
  We have that:
  \begin{enumerate}
  \item $Q^{\pi_{k+1}} \geq \Tcal Q^{\pi_k} \geq Q^{\pi_k}$
  \item   $\| Q^{\pi_{k+1}} - Q^\star\|_\infty \leq \gamma \|Q^{\pi_k} - Q^\star\|_\infty  $
  \end{enumerate}
\end{lemma}

\begin{proof}
First let us show that $\Tcal Q^{\pi_k} \geq Q^{\pi_k}$. Note that the policies produced in
policy iteration are always deterministic, so $V^{\pi_k}(s) =
Q^{\pi_k}(s,\pi_k(s))$ for all iterations $k$ and states $s$. Hence,
\begin{align*}
\Tcal Q^{\pi_k}(s,a) &= r(s,a) + \gamma \E_{s'\sim P(\cdot \given s,a)} [\max_{a'} Q^{\pi_k}(s',a')]\\
& \geq r(s,a) + \gamma \E_{s'\sim P(\cdot \given s,a)} [Q^{\pi_k}(s',\pi_k(s'))]
= Q^{\pi_k}(s,a).
\end{align*}

Now let us prove that $Q^{\pi_{k+1}} \geq \Tcal Q^{\pi_k}$. First, let
us see that $Q^{\pi_{k+1}} \geq Q^{\pi_k}$:
\begin{align*}
Q^{\pi_k} = r + \gamma P^{\pi_k} Q^{\pi_k}
\leq r + \gamma P^{\pi_{k+1}} Q^{\pi_k}
\leq \sum_{t=0}^\infty \gamma^t (P^{\pi_{k+1}})^t r
= Q^{\pi_{k+1}}.
\end{align*}
where we have used that $\pi_{k+1}$ is the greedy policy in the first
inequality and recursion in the second inequality.
Using this,
\begin{align*}
Q^{\pi_{k+1}}(s,a) &= r(s,a) + \gamma \E_{s'\sim P(\cdot \given s,a)} [Q^{\pi_{k+1}}(s',\pi_{k+1}(s'))]\\
&\geq r(s,a) + \gamma \E_{s'\sim P(\cdot \given s,a)} [Q^{\pi_k}(s',\pi_{k+1}(s'))]\\
&= r(s,a) + \gamma \E_{s'\sim P(\cdot \given s,a)} [\max_{a'}Q^{\pi_k}(s',a')]
=\Tcal Q^{\pi_k}(s,a)
\end{align*}
which completes the proof of the first claim.

For the second claim,
\[
\|  Q^\star -Q^{\pi_{k+1}}\|_\infty \leq \|  Q^\star-\Tcal Q^{\pi_k}\|_\infty
=\|  \Tcal Q^\star-\Tcal Q^{\pi_{k}}\|_\infty\leq \gamma \|  Q^\star- Q^{\pi_k}\|_\infty
\]
where we have used that $Q^\star \geq Q^{\pi_{k+1}}\geq \Tcal Q^{\pi_k}$ in
second step and the contraction property of $\Tcal$ (see
Lemma~\ref{lemma:contraction}) in the last step.
\end{proof}

\iffalse
\begin{align}
Q^{\pi_{k+1}}(s,a) - \Tcal Q^{\pi_k}(s,a) &= \gamma \E_{s'\sim P(\cdot \given s,a)}\left[Q^{\pi_{k+1}}(s',\pi_{k+1}(s')) - \max_{a'} Q^{\pi_k}(s',a') \right] \nonumber\\
&\leq \gamma \E_{s'\sim P(\cdot \given s,a)}\left[Q^{\pi_{k+1}}(s',\pi_{k+1}(s')) - Q^{\pi_k}(s', \pi_{k+1}(s')) \right]\nonumber \\
&= \gamma^2 \E_{s'\sim P(\cdot \given s,a), s'' \sim P(\cdot \given s',\pi_{k+1}(s'))}\left[Q^{\pi_{k+1}}(s'',\pi_{k+1}(s'')) - Q^{\pi_k}(s'',\pi_k(s'')) \right]\nonumber \\
&= \gamma^2 \E_{s'\sim P(\cdot \given s,a)}\left[Q^{\pi_{k+1}}(s',\pi_{k+1}(s'))
- \Tcal Q^{\pi_k}(s', \pi_{k+1}(s')) \right]\nonumber \\
&\geq \gamma^2 \E_{s'\sim P(\cdot \given s,a), s'' \sim P(\cdot \given s',\pi_{k+1}(s'))}\left[Q^{\pi_{k+1}}(s'',\pi_{k+1}(s'')) - \max_{a''} Q^{\pi_k}(s'',a'') \right].\nonumber
\end{align}
Unrolling the infinite sum repeatedly with the same argument, we obtain the desired inequality.

Furthermore,
\begin{align}
Q^{\pi_{k+1}}(s,a) - \Tcal Q^{\pi_k}(s,a) &= \gamma \E_{s'\sim P(\cdot \given s,a)}\left[Q^{\pi_{k+1}}(s',\pi_{k+1}(s')) - \max_{a'} Q^{\pi_k}(s',a') \right] \nonumber\\
&= \gamma \E_{s'\sim P(\cdot \given s,a)}\left[Q^{\pi_{k+1}}(s',\pi_{k+1}(s')) - \max_{a'} Q^{\pi_k}(s',a') \right]\nonumber \\
&= \gamma^2 \E_{s'\sim P(\cdot \given s,a), s'' \sim P(\cdot \given s',\pi_{k+1}(s'))}\left[Q^{\pi_{k+1}}(s'',\pi_{k+1}(s'')) - Q^{\pi_k}(s'',\pi_k(s'')) \right]\nonumber \\
&\geq \gamma^2 \E_{s'\sim P(\cdot \given s,a), s'' \sim P(\cdot \given s',\pi_{k+1}(s'))}\left[Q^{\pi_{k+1}}(s'',\pi_{k+1}(s'')) - \max_{a''} Q^{\pi_k}(s'',a'') \right].\nonumber
\end{align}
Unrolling the infinite sum repeatedly with the same argument, we obtain the desired inequality.

For the second part, note that by definition of $Q^\star$, we have $Q^{\pi_k} \leq Q^\star$ for all $k$. Now
\begin{align*}
\max_{s,a} (Q^\star(s,a) - Q^{\pi_{k+1}}(s,a)) &\leq \max_{s,a} (Q^\star(s,a) - \Tcal Q^{\pi_{k}}(s,a))\\
&= \max_{s,a} \gamma \E_{s'\sim P(\cdot\given s,a)} [Q^\star(s',\pi^\star(s')) - \max_{a'} Q^{\pi_k}(s',a')]\\
&\leq \max_{s,a} \gamma \E_{s'\sim P(\cdot\given s,a)} [\max_{a'} (Q^\star(s',a') -Q^{\pi_k}(s',a'))]\\
&\leq \gamma \max_{s,a} (Q^\star(s,a) - Q^{\pi_k}(s,a)).
\end{align*}
Here the first inequality follows from part 1, while the second is a
result of triangle inequality: $\max_a f(a) - \max_a g(a) \leq \max_a
(f(a) - g(a))$.
\end{proof}
\fi

With this lemma, a convergence rate for the policy iteration algorithm
immediately follows.

\begin{theorem}
(Policy iteration convergence). Let $\pi_0$ be any initial policy.
For $k\geq \frac{\log \frac{1}{(1-\gamma)\eps}}{1-\gamma}$, the
$k$-th policy in policy iteration has the following performance bound:
\[
Q^{\pi_k} \geq Q^{\star} -\eps \mathds{1}\, .
\]
\label{thm:pi-convergence}
\end{theorem}

\paragraph{Iteration complexity for an exact solution.}
With regards to computing an exact optimal policy, it is clear from the
previous results that policy iteration is no worse than value
iteration. However, with regards to obtaining an exact solution MDP
that is independent of the bit complexity, $L(P,r,\gamma)$,
improvements are possible (and where we assume basic arithmetic
operations on real numbers are order one cost).  Naively, the number of iterations of policy iterations is
bounded by the number of policies, namely $|\Acal|^{|\Scal|}$; here, a
small improvement is possible, where the number of iterations of
policy iteration can be bounded by $\frac{|\Acal|^{|\Scal|}}{|\Scal|}$.
Remarkably, for a fixed value of
$\gamma$, policy iteration can be show to be a strongly polynomial time algorithm,
where policy iteration finds an exact policy in at most
$\frac{|\Scal|^2 |\Acal| \log\frac{|\Scal|^2 }{1-\gamma}}{1-\gamma}$ iterations.
See Table~\ref{table:computation} for a summary, and Section~\ref{ch1:bib} for references.

\subsection{Value Iteration for Finite Horizon MDPs}

Let us now specify the value iteration algorithm for finite-horizon
MDPs. For the finize-horizon setting, it turns out that the analogues of value iteration and
policy iteration lead to identical algorithms.  The value iteration
algorithm is specified as follows:
\begin{enumerate}
\item Set $Q_{H-1}(s,a)= r_{H-1}(s,a)$.
\item For $h = H-2,\ldots 0$, set:
\[
Q_h(s,a) =  r_h(s,a) + \gamma \EE_{s' \sim P_h(\cdot|s, a)}\left[ \max_{a'\in\Acal} Q_{h+1} (s',a') \right].
\]
\end{enumerate}

By Theorem~\ref{thm:Bellman_finite}, it follows that
$Q_h(s,a)=Q_h^\star(s,a)$ and that 
$\pi(s,h) = \argmax_{a\in \Acal} Q_h^\star(s,a)$ is an optimal policy.

\subsection{The Linear Programming Approach}
\label{sec:LP}

It is helpful to understand an alternative approach
to finding an optimal policy for a known MDP.
With regards to computation, consider the setting where our MDP $M = (\Scal, \Acal, P, r, \gamma, \mu)$ is
known and $P$, $r$, and $\gamma$ are all specified by rational numbers.
Here, from a
computational perspective, the previous iterative algorithms are,
strictly speaking, not
polynomial time algorithms, due to that they depend polynomially on $1/(1-\gamma)$, which is not
polynomial in the description length of the MDP . In particular, note
that any rational value of $1-\gamma$ may be specified with only $O(\log\frac{1}{1-\gamma})$
bits of precision. In this context, we may hope for a fully polynomial
time algorithm, when given knowledge of the MDP, which would have a
computation time which would depend polynomially on the description
length of the MDP $M$, when the parameters are specified as rational numbers.
We now see that the LP approach provides a polynomial
time algorithm.


\subsubsection*{The Primal LP and A Polynomial Time Algorithm}

Consider the following optimization problem with variables $V \in \R^{|\Scal|}$:
\begin{eqnarray*}
\min  && \sum_s \mu(s) V(s)\\
\textrm{ subject to } && V(s) \geq r(s,a) + \gamma \sum_{s'} P(s'|s,a) V(s')
                         \, \quad \forall a \in\Acal, \, s\in\Scal\\
\end{eqnarray*}

Provided that $\mu$ has full support, then the optimal value function $V^\star(s)$ is the unique solution to this linear
program. With regards to computation time, linear programming
approaches only depend on the description length of the coefficients
in the program, due to that this determines the computational complexity
of basic additions and multiplications. Thus, this approach will only depend on the bit length
description of the MDP, when the MDP is specified by rational numbers.

\paragraph{Computational complexity for an exact solution.}
Table~\ref{table:computation} shows the runtime complexity for the LP
approach, where we assume a standard runtime for solving a linear
program.  The strongly polynomial algorithm is
an interior point algorithm.  See Section~\ref{ch1:bib} for
 references.

\paragraph{Policy iteration and the simplex algorithm.}
It turns out that the policy iteration algorithm is actually the
simplex method with block pivot. While the simplex method, in
general, is not a strongly polynomial time algorithm,  the policy
iteration algorithm is a strongly polynomial time algorithm, provided
we keep the discount factor fixed. See \cite{DBLP:journals/mor/Ye11a}.

\subsubsection*{The Dual LP and the State-Action Polytope}

\iffalse
For a fixed (possibly stochastic) policy $\pi$, let us define the
state-action visitation distribution $\nu_{\mu}^\pi$ as:
\[
\nu_{\mu}^\pi(s,a) = (1-\gamma) \sum_{t=0}^\infty \gamma^t
{\Pr}^\pi(s_t=s, a_t=a)
\]
where $\Pr^\pi(s_t=s,a_t=a)$ is the state-action visitation
probability, where we execute $\pi$ in $M$ starting at state $s_0\sim \mu$.
\fi

For a fixed (possibly stochastic) policy $\pi$, let us define
a visitation measure over states \emph{and} actions
induced by following $\pi$ after  starting at $s_0$. Precisely, define 
this distribution, $d_{s_0}^\pi$, as follows:
\begin{equation}\label{eq:dpisa}
d_{s_0}^\pi(s,a) := (1-\gamma) 
\sum_{t=0}^\infty \gamma^t {\Pr}^\pi(s_t=s,a_t=a|s_0)
\end{equation}
where $\Pr^\pi(s_t=s,a_t=a|s_0)$ is the
probability that $s_t=s$ and $a_t=a$, after starting at state
$s_0$ and following $\pi$ thereafter.  It is straightforward to verify
that $d_{s_0}^\pi$ is a distribution over $\Scal \times \Acal$.
We
also overload notation and write:
\[
d_{\mu}^\pi(s,a) = \E_{s_0\sim \mu} \left[d_{s_0}^\pi(s,a)\right].
\]
for a distribution $\mu$ over $\Scal$.
Recall Lemma~\ref{eq:sa_measure} provides a way to easily compute
$d_{\mu}^\pi(s,a)$ through an appropriate vector-matrix multiplication.


%We drop the $\mu$ dependence when clear from context.
% We
%also write:
%\[
%d^\pi(s) = d_{d_0}^\pi(s) = \E_{s_0\sim d_0} \left[d_{s_0}^\pi(s)\right]
%\]
%where we drop the $d_0$ subscript when clear from context.

It is straightforward to verify that $d_{\mu}^\pi$ satisfies, for all states
$s \in \Scal$:
\[
\sum_a d_\mu^\pi(s,a) = (1-\gamma) \mu(s) + \gamma \sum_{s',a'} P(s|s',a') d_\mu^\pi(s',a').
\]
Let us define the state-action polytope as follows:
\[
\mathcal{K}_\mu:=\{d |\, d\geq 0 \textrm{ and } \sum_a d(s,a) = (1-\gamma) \mu(s) + \gamma
\sum_{s',a'} P(s|s',a') d(s',a')  \}
\]
We now see that this set precisely characterizes all state-action visitation distributions.
\begin{proposition} \label{lemma:polytope}
We have that $\mathcal{K}_\mu$ is equal to
  the set of all feasible state-action distributions, i.e. $d \in
  \mathcal{K}_\mu$ if and only if there exists a stationary (and possibly randomized) policy $\pi$
  such that $d_\mu^\pi = d$.
\end{proposition}
% HW

With respect the variables $d \in \R^{|\Scal|\cdot|\Acal|}$, the dual LP formulation is as follows:
\begin{eqnarray*}
\max  && \frac{1}{1-\gamma} \sum_{s,a} d_\mu(s,a) r(s,a)\\
\textrm{ subject to } && d \in \mathcal{K}_\mu\\
\end{eqnarray*}
Note that $\mathcal{K}_\mu$ is itself a polytope, and one can verify that
this is indeed the dual of the aforementioned LP.
 This approach provides an alternative approach to finding an optimal solution.

If $d^\star$ is the solution to this LP, and provided that $\mu$ has full support, then we have that:
\[
\pi^\star(a|s) = \frac{d^\star(s,a)}{\sum_{a'} d^\star(s,a')} ,
\]
is an optimal policy.
An alternative optimal policy is $\argmax_a d^\star(s,a)$ (and these
policies are identical if the optimal policy is unique).

\section{Sample Complexity and Sampling Models}
\label{sec:sampling_models}

Much of reinforcement learning is concerned with finding a near
optimal policy (or obtaining near optimal reward) in settings where
the MDPs is not known to the learner. We will study these questions
in a few different models of how the agent obtains information 
about the unknown underlying MDP. In each of these settings, we are interested
understanding the number of samples required to find a near optimal
policy, i.e. the \emph{sample complexity}. Ultimately, we
interested in obtaining results which are applicable to cases where number of states and
actions is large (or, possibly, countably or uncountably
infinite). This is many ways analogous to the supervised learning
question of generalization, though, as we shall see, this question is
fundamentally more challenging in the reinforcement learning setting.


%, when we are in an \emph{episodic
%setting} and when we have access to a \emph{generative model}.

\paragraph{The Episodic Setting.}
In the episodic setting, in every episode, the
learner acts for some finite number of steps, starting from a fixed starting state
$s_0\sim \mu$, the learner observes the trajectory, and the state
resets to $s_0\sim \mu$.  This episodic
model of feedback is applicable to both the finte-horizon and infinite
horizon settings.
\begin{itemize}
\item (Finite Horizon MDPs) Here, each episode lasts for $H$-steps,
  and then the state is reset to $s_0\sim \mu$.
\item (Infinite Horizon MDPs) Even for infinite horizon MDPs it is
  natural to work in an episodic model for learning, where each
  episode terminates after a finite number of steps. Here, it is often
  natural to assume either the agent can terminate the episode at will
  or that the episode will terminate at each step with probability
  $1-\gamma$. After termination, we again assume that the state is
  reset to  $s_0\sim \mu$.  Note that, if each step in an episode is terminated with probability
  $1-\gamma$, then the observed cumulative reward in an episode of a policy
  provides an unbiased estimate of the infinite-horizon, discounted
  value of that policy.
\end{itemize}
In this setting, we are often interested in either the number
of episodes it takes to find a near optimal policy, which is a \emph{PAC} (probably,
approximately correct) guarantee, or we are interested in a \emph{regret}
guarantee (which we will study in
Chapter~\ref{chap:tabular_exploration}).  Both of these questions are
with regards to statistical complexity (i.e. the sample complexity) of learning.

The episodic setting is challenging in that the agent has to engage in
some exploration in order to gain information at the relevant
state. As we shall see in Chapter~\ref{chap:tabular_exploration}, this
exploration must be strategic, in the sense that simply behaving
randomly will not lead to information being gathered quickly
enough. It is often helpful to study the statistical
complexity of learning in a more abstract sampling model, a
generative model, which allows to avoid having to directly address
this exploration issue. Furthermore, this sampling model is natural in
its own right. 

\paragraph{The generative model setting.} A \emph{generative model}
takes as input a state
action pair $(s,a)$ and returns a sample $s'\sim P(\cdot|s, a)$ and
the reward $r(s,a)$ (or a sample of the reward if the rewards are stochastic).

\iffalse
Let us consider the most naive approach to learning (when
we have access to a generative model):
 suppose we call our
simulator $N$ times at each state action pair. Let $\widehat P$ be our
empirical model, defined as follows:
\[
\widehat P(s'|s,a) =  \frac{\textrm{count}(s',s,a)}{N}
\]
where $\textrm{count}(s',s,a)$ is the number of times the state-action
pair $(s,a)$ transitions to state $s'$. As the $N$ is the number of
calls for each state action pair, the total number of
calls to our generative model is $|\Scal||\Acal|N$. As before, we can
view $\widehat P$ as a matrix of size $|\Scal||\Acal| \times |\Scal|$.
\fi

\paragraph{The offline RL setting.} The \emph{offline RL setting} is
where the agent has access to an offline dataset, say generated under
some policy (or a collection of policies). In the simplest of these
settings, we may assume our dataset is of the form $\{(s,a,s',r)\}$
where $r$ is the reward (corresponding to $r(s,a)$ if the reward is
deterministic) and $s'\sim P(\cdot|s, a)$. Furthermore, for
simplicity, it can be helpful to assume
that the $s,a$ pairs in this dataset were sampled i.i.d. from some
fixed distribution $\nu$ over $\Scal \times \Acal$.


\section{Bonus: Advantages and The Performance Difference Lemma}

Throughout, we will overload notation where, for a distribution $\mu$
over $\Scal$, we write:
\[
V^\pi(\mu) = \E_{s\sim \mu} \left[V^\pi(s)\right].
\]

The \emph{advantage} $A^\pi(s, a)$ of a policy $\pi$ is defined as
\[
A^\pi(s,a) := Q^\pi(s,a)-V^\pi(s) \, .
\]
Note that:
\[
A^*(s,a) := A^{\pi^*}(s,a) \leq 0
\]
for all state-action pairs.

\iffalse
We now define the discounted state visitation distribution.
Let $\tau$
denote a trajectory, whose unconditional distribution
${\Pr}^\pi_{\mu}(\tau) $ under $\pi$ with starting
distribution $\mu$, is
\begin{align} \label{eq:prob_traj}
  {\Pr}^\pi_{\mu}(\tau) = \mu(s_0) \pi(a_0|s_0) P(s_1|s_0,a_0)
  \pi(a_1|s_1) \cdots \, .
\end{align}
\fi

Analogous to the state-action visitation distribution (see Equation~\ref{eq:dpisa}), we can
define a visitation measure over just the states. When clear from
context, we will
overload notation and also denote this distribution by $d_{s_0}^\pi$, where:
\begin{equation}\label{eqn:dpi}
d_{s_0}^\pi(s) = (1-\gamma) \sum_{t=0}^\infty \gamma^t {\Pr}^\pi(s_t=s|s_0).
\end{equation}
Here, $\Pr^\pi(s_t=s|s_0)$ is the state visitation
probability, under $\pi$ starting at state $s_0$. Again,
we write:
\[
d_{\mu}^\pi(s) = \E_{s_0\sim \mu} \left[d_{s_0}^\pi(s)\right].
\]
for a distribution $\mu$ over $\Scal$.

The following lemma is helpful in the analysis of RL algorithms.

\begin{lemma}\label{lemma:perf_diff}
(The performance difference lemma) For all policies $\pi, \pi'$ and distributions
$\mu$ over $\Scal$,
\begin{eqnarray*}
V^\pi(\mu) - V^{\pi'}(\mu) =
\frac{1}{1-\gamma}\E_{s'\sim d_\mu^\pi }\E_{a'\sim \pi(\cdot|s') } \left[A^{\pi'}(s',a')\right].
\end{eqnarray*}
\end{lemma}

\begin{proof}
Let $\Pr^\pi(\tau | s_0 = s)$ denote the probability of observing a
trajectory $\tau$ when starting in state $s$ and following the policy
$\pi$.  By definition of $d^{\pi_\theta}_{s_0}$, observe that for any function $f: \Scal\times\Acal\rightarrow \R$,
\begin{equation}\label{eq:f_equal}
\E_{\tau \sim {\Pr}^\pi }
\left[\sum_{t=0}^\infty \gamma^t f(s_t,a_t) \right] = 
\frac{1}{1-\gamma} \E_{s\sim d^{\pi_\theta}_{s_0} }
\E_{a\sim\pi_\theta(\cdot|s) } \big[ f(s,a)\big].
\end{equation}

Using a telescoping argument, we have: 
\begin{eqnarray*}
V^\pi(s) - V^{\pi'}(s) &=&  \E_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
\left[\sum_{t=0}^\infty \gamma^t r(s_t,a_t)\right] - V^{\pi'}(s) \\
&=& \E_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
\left[\sum_{t=0}^\infty \gamma^t \left(r(s_t,a_t)+V^{\pi'}(s_t)-V^{\pi'}(s_t) \right)\right]-V^{\pi'}(s)\\
&\stackrel{(a)}{=}& \E_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
    \left[\sum_{t=0}^\infty \gamma^t \left(r(s_t,a_t)+\gamma V^{\pi'}(s_{t+1})-V^{\pi'}(s_t)\right)\right]\\
&\stackrel{(b)}{=}&\E_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
    \left[\sum_{t=0}^\infty \gamma^t \left(r(s_t,a_t)+\gamma \E[V^{\pi'}(s_{t+1})|s_t,a_t]-V^{\pi'}(s_t)\right)\right]\\
&\stackrel{(c)}{=}&\E_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
    \left[\sum_{t=0}^\infty \gamma^t \left(Q^{\pi'}(s_t,a_t)-V^{\pi'}(s_t)\right)\right]\\
&=& \E_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
    \left[\sum_{t=0}^\infty \gamma^t A^{\pi'}(s_t,a_t)\right] \\
&=& \frac{1}{1-\gamma}\E_{s'\sim d^\pi_s }\,\E_{a\sim \pi(\cdot | s)}
    A^{\pi'}(s',a),
    \end{eqnarray*}
where step $(a)$ rearranges terms in the summation via telescoping;
step $(b)$
uses the law of iterated expectations; step $(c)$
follows by definition; and the final
equality follows from Equation~\ref{eq:f_equal}. 
  \end{proof}

\section{Bibliographic Remarks and Further Reading}\label{ch1:bib}

We refer the reader to \cite{puterman1994markov} for a more detailed
treatment of dynamic programming and
MDPs.   \cite{puterman1994markov} also contains a thorough treatment
of the dual LP, along with a proof of Lemma~\ref{lemma:polytope}

With regards to the computational complexity of policy iteration, \cite{DBLP:journals/mor/Ye11a}
showed that policy iteration is a strongly polynomial time algorithm
for a fixed discount rate~\footnote{The stated strongly polynomial
  runtime in Table~\ref{table:computation} for
  policy iteration differs from that in \cite{DBLP:journals/mor/Ye11a}
  due to we assume that the runtime per iteration of policy iteration
  is $|\Scal|^3 +|\Scal|^2|\Acal|$. }.   Also, see
\cite{DBLP:journals/mor/Ye11a} for a good summary of the computational complexities
of various approaches.
\cite{MS_PI} showed that the number of iterations of
policy iteration can be bounded by
$\frac{|\Acal|^{|\Scal|}}{|\Scal|}$.

With regards to a strongly polynomial algorithm, the CIPA
algorithm~\cite{yinyu_CIPA} is an interior point algorithm with the
claimed runtime in Table~\ref{table:computation}.

Lemma~\ref{lemma:Q_to_policy} is due to \citet{singh1994upper}. 

The performance difference lemma is due to
~\cite{kakade2002approximately,kakade2003sample}, though the lemma was
implicit in the analysis of a number of prior works. 
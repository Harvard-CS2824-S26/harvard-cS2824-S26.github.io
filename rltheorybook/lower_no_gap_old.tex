%===========================================================
% linearQstar_lowerbound_gapless.tex  (draft, cleaned up)
%===========================================================

\section{An Exponential Lower Bound under Linear $Q^\star$-Realizability (Gapless)}
\label{sec:linearQstar_gapless_lb}

This section sketches a hard family of finite-horizon MDPs in which
$Q^\star$ is linearly realizable in a given feature map, yet learning a near-optimal
policy requires exponentially many samples (in $\min\{d,H\}$).
We emphasize that the lower bound does \emph{not} assume any positive suboptimality
gap condition; indeed, the hard family may have vanishing gaps at some layers.

\subsection{A trajectory-tree construction}

Fix a horizon $H\ge 1$. Let $A$ be the number of actions, chosen later as
$A = \exp(\Theta(\min\{d,H\}))$.

\paragraph{Nearly-orthogonal vectors.}
Let $\rho\in(0,1/6)$ be a fixed constant. By a standard Johnson--Lindenstrauss packing argument,
for $A \le \exp(c_0 \rho^2 d)$ there exist unit vectors $v_1,\dots,v_A\in\R^d$ such that
\[
\forall i\neq j,\qquad |\langle v_i,v_j\rangle|\le \rho.
\]
We write $v(a)$ for $v_a$.

\paragraph{State space (trajectory tree + terminal).}
For each layer $h\in\{0,1,\dots,H\}$ define the nonterminal state set
\[
\Scal_h^{\mathrm{nt}} := \big\{ \overline{a_{1:h}} : a_1,\dots,a_h \in [A]
\text{ are all distinct}\big\},
\]
with the convention $\Scal_0^{\mathrm{nt}}=\{\overline{\emptyset}\}$.
Let $f$ denote an absorbing terminal state, present at all layers.
Set $\Scal_h := \Scal_h^{\mathrm{nt}} \cup \{f\}$.

\paragraph{MDP family indexed by $a^\star$.}
The hard family is $\{ \mathcal{M}_{a^\star} : a^\star \in [A]\}$.
The initial state is $s_0=\overline{\emptyset}$ deterministically.
The action set is $\Acal=[A]$.

\paragraph{Transitions.}
For $h\le H-1$, if $s_h=f$ then $s_{h+1}=f$ surely.
If $s_h=\overline{a_{1:h}}$ is nonterminal and the agent chooses action $a\in[A]$, then
\[
s_{h+1} :=
\begin{cases}
f, & a\in\{a_1,\dots,a_h\} \quad\text{(repeat)}\\
f, & a=a^\star \quad\text{(special action)}\\
\overline{a_{1:h}a}, & \text{otherwise}.
\end{cases}
\]
(So choosing $a^\star$ ends the episode immediately; this is the ``game-over'' mechanism.)

[SK: I don't quite know what you mean by ``repeat'' above.]

\paragraph{Feature map.}
We define a feature map $\phi_h:\Scal_h\times\Acal\to\R^d$ that is \emph{shared across the family} (independent of $a^\star$).
To avoid parent-action indexing confusion, it is convenient to define a state embedding
$\psi_h:\Scal_h\to\R^d$ recursively by
\[
\psi_0(\overline{\emptyset}) := 0,\qquad \psi_h(f):=0\ \ \forall h,
\]
and, whenever $s_{h+1}=\overline{a_{1:h}a}$ is reached from $s_h=\overline{a_{1:h}}$ by choosing $a$,
we set $\psi_{h+1}(s_{h+1}) := \phi_h(s_h,a)$.

Let $(b_h)_{h=1}^H$ be a sequence of positive ``bias'' scalars (chosen below). Then define
for $h\le H-1$ and $s\in\Scal_h$,
\[
\phi_h(s,a) :=
\begin{cases}
\big(\langle \psi_h(s), v(a)\rangle + b_{h+1}\big)\, v(a), & s\neq f,\\
0, & s=f.
\end{cases}
\]
This matches the intended ``multiplicative'' recursion (the $\langle\psi_h(s),v(a)\rangle$ term)
with an additive bias $b_{h+1}$ that stabilizes the scale layer-by-layer.

[SK: can you just get rid of the psi and use the phi's recursively? this was like in Yunhao's note.]

\paragraph{Parameter and target $Q^\star$.}
Fix an index $a^\star\in[A]$ and set
\[
\theta^\star := v(a^\star)\in\R^d.
\]
Define the candidate optimal $Q$-function by
\[
Q_h^\star(s,a) := \langle \phi_h(s,a), \theta^\star\rangle,\qquad h=0,\dots,H-1.
\]
Let $V_h^\star(s):=\max_{a\in\Acal} Q_h^\star(s,a)$ for $h=0,\dots,H-1$, and set $V_H^\star(\cdot)\equiv 0$.

\paragraph{Rewards (defined to satisfy Bellman optimality).}
Finally, define the reward function by
\[
r_h(s,a) :=
Q_h^\star(s,a) - \E\!\left[V_{h+1}^\star(s_{h+1})\mid s_h=s,a_h=a\right],
\qquad h=0,\dots,H-1,
\]
where the expectation is with respect to the (deterministic) transition above.
Equivalently, since transitions are deterministic given $(s,a)$,
\[
r_h(s,a) =
\begin{cases}
Q_h^\star(s,a), & s_{h+1}=f,\\
Q_h^\star(s,a)-V_{h+1}^\star(s_{h+1}), & s_{h+1}\in\Scal_{h+1}^{\mathrm{nt}}.
\end{cases}
\]
(If you prefer bounded rewards in $[0,1]$, one can rescale all $b_h$ by a small constant and then shift by $+1/2$; we omit this normalization here.)

SK:
Let's spell these out directly. I don't like it done this way. I think we wall want something like Yunhao's note (my collaborator from a while back.)

Rewards are:
    \begin{align*}
    r(\overline{a_{1} \dots a_{h-1}}, a^{*}) &= \langle \phi(\overline{a_{1} \dots a_{h-1}}, a_{h}), v(a^{*}) \rangle \\
    r(\overline{a_{1} \dots a_{H-1}}, a_{H}) &= \langle \phi(\overline{a_{1} \dots a_{H-1}}, a_{H}), v(a^{*}) \rangle + \xi
    \end{align*}
    and for $a_h\neq a^{*}$ and $0 \leq h\leq H-1$.
    \[
      r(\overline{a_{1} \dots a_{h-1}}, a_h) = b_h
      \]
[SK: this last step is to get the bias right.]

\subsection{Verifying linear realizability}

\begin{lemma}[Linear $Q^\star$-realizability holds by construction]
\label{lem:tree_realizability}
For every $a^\star\in[A]$, the MDP $\mathcal{M}_{a^\star}$ satisfies:
for all $h\in\{0,\dots,H-1\}$ and $(s,a)\in\Scal_h\times\Acal$,
\[
Q_h^\star(s,a)=\langle \phi_h(s,a), \theta^\star\rangle
\quad\text{with}\quad \theta^\star=v(a^\star).
\]
Moreover, the optimality (Bellman) equations hold:
\[
Q_h^\star(s,a)=r_h(s,a)+\E\!\left[V_{h+1}^\star(s_{h+1})\mid s_h=s,a_h=a\right].
\]
\end{lemma}

\begin{proof}
The linear form is the definition of $Q_h^\star$. The Bellman optimality equation holds
by the definition of $r_h(s,a)$.
\end{proof}

SK: this part is the whole point of the proof! and this is what was done nicely in Yuanhoa's note. I think we want to show things: that the Q(s,a*)>=Q(s,a). and then we want to show consistency of this Q*. I think that woudl do it. The first part is from Yunaho's note below. The second part will be just the reward construction (that shift I used).

Here, we claim that the maximum is attained at $a_{h+1}=a^{*}$. Let us first show the following inequality for any $h$ and any $a' \notin \{a_{1}, \dots, a_{h}\}$:
\[
|\langle v(a'), \phi(\overline{a_{1} \dots a_{h-1}}, a_{h}) \rangle| \le \frac{1}{2}b_{h} = \frac{1}{2}(3\gamma)^{h}.
\]
We show this by induction on $h$. Assume that the inequality holds for $1, \dots, h-1$. Then
\begin{align*}
|\langle v(a'), \phi(\overline{a_{1} \dots a_{h-1}}, a_{h}) \rangle| &= |\langle v(a'), v(a_{h}) \rangle| \cdot |\langle \phi(\overline{a_{1} \dots a_{h-2}}, a_{h-1}), v(a_{h}) \rangle + b_{h}| \\
&\le \gamma \cdot \left[\frac{1}{2}(3\gamma)^{h-1} + (3\gamma)^{h-1}\right] = \frac{1}{2}(3\gamma)^{h}.
\end{align*}

Thus, if $a_{h+1}=a^{*}$ then the RHS of (1) is
\begin{align*}
\langle v(a^{*}), \phi(\overline{a_{1} \dots a_{h-1}a_{h}}, a_{h+1}) \rangle &= (v(a^{*}))^{T} [(\langle \phi(\overline{a_{1} \dots a_{h-1}}, a_{h}), v(a^{*}) \rangle + b_{h}) \cdot v(a^{*})] \\
&= \langle \phi(\overline{a_{1} \dots a_{h-1}}, a_{h}), v(a^{*}) \rangle + b_{h} \\
&\ge -\frac{1}{2}(3\gamma)^{h} + (3\gamma)^{h} = \frac{1}{2}(3\gamma)^{h}.
\end{align*}

\paragraph{Remark on the role of the bias sequence $(b_h)$.}
To use this construction for a lower bound, one chooses $(b_h)$ so that:
(i) the values $V_h^\star$ remain $\Theta(1)$ (so ``near-optimal'' is nontrivial), while
(ii) the \emph{information} about $a^\star$ that appears in observable trajectories is
concentrated in exponentially rare events (e.g.\ reaching deep layers without hitting $f$).
A typical choice is a geometrically decaying bias, e.g.\ $b_h=(3\rho)^{h-1}$, but the
final choice depends on the exact statistical argument you want to run.

\subsection{Information-theoretic lower bound (proof sketch)}

SK: ingore all this for now. this should be easy whend one right.

The key observation is that the feature map $\phi$ and the transition kernel are
\emph{shared across the family} $\{\mathcal{M}_{a^\star}\}$; the only dependence on $a^\star$
is through the parameter $\theta^\star=v(a^\star)$ appearing inside $Q^\star$ (and hence $r_h$).

To make the indistinguishability argument clean, it is convenient to add a small amount of
reward noise (e.g.\ $R_h = r_h(s,a)+\xi_h$ with $\xi_h\sim \mathcal{N}(0,1)$ i.i.d.),
so that KL divergences between transcripts can be controlled by squared mean differences.
(If you want bounded rewards, one can instead use Bernoulli noise with a standard quadratic KL bound.)

\paragraph{Hardness mechanism.}
Under any algorithm, the only ways to obtain significant information about $a^\star$ are:
\begin{enumerate}
\item \textbf{Playing $a^\star$:} choosing the special action ends the episode immediately (transition to $f$),
and the resulting reward reveals a direction aligned with $\theta^\star$.
\item \textbf{Reaching deep nonterminal states:} the state embeddings $\psi_h(s)$ (and therefore $\phi_h(s,\cdot)$)
only become strongly aligned with $\theta^\star$ along ``correct'' trajectories; if the construction is tuned so that
such trajectories occur with probability $\exp(-\Omega(h))$, then observing them requires exponentially many episodes.
\end{enumerate}

\paragraph{KL template.}
Let $\mathsf{T}$ denote the full transcript of interaction (states, actions, rewards) over $T$ episodes.
A standard route is:
\[
\mathrm{KL}\!\left(\mathsf{T}\mid \mathcal{M}_{a^\star}\ \big\|\ \mathsf{T}\mid \mathcal{M}_{a'}\right)
\ \le\ \sum_{t=1}^T \sum_{h=0}^{H-1}
\EE\!\left[ \mathrm{KL}\!\left(R_{t,h}\mid \mathcal{M}_{a^\star},\mathcal{F}_{t,h}\ \big\|\ R_{t,h}\mid \mathcal{M}_{a'},\mathcal{F}_{t,h}\right)\right],
\]
and with Gaussian reward noise the inner KL is
$\frac12\left(r_h^{(a^\star)}(s,a)-r_h^{(a')}(s,a)\right)^2$.
One then shows that, unless $T$ is exponential in $\min\{d,H\}$, this KL is $o(1)$
for many pairs $(a^\star,a')$, which (via Le Cam or Fano) implies that the algorithm
cannot identify a near-optimal policy with constant success probability.

\paragraph{Finishing step (policy suboptimality).}
Finally, one shows that any policy that fails to identify $a^\star$ (and therefore plays $a^\star$ with too small probability
in early layers) suffers a constant loss in value, e.g.\ at least $0.05$ in $V^\pi(s_0)$.

\medskip
\noindent
\textbf{TODO (where the remaining work is):}
to turn this into a complete proof, we should:
(i) pick the bias sequence $(b_h)$ explicitly,
(ii) specify a reward noise model and bound the per-step KL,
(iii) prove the required indistinguishability inequality and translate it to a value gap.
%===========================================================


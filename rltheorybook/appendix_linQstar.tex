\chapter{Linearly Realizable $Q^\star$ Lower Bound}
\label{app:linearQstar_lb}

This section records a trajectory-tree hard instance in which $Q^\star$
is linear in a given feature map, yet identifying a near-optimal policy
requires exponentially many samples in $\min\{d,H\}$ (once the action
set is taken to be exponentially large in $d$). 

\begin{theorem}[Linear $Q^\star$ realizability can be exponentially hard (generative model)]
\label{thm:lower_gen_gapless_simple}
Consider any algorithm $\mathcal{A}$ that has access to a generative model and
is given as input a feature map $\phi:\Scal\times\Acal\times[H]\to\R^d$.
There exists a finite-horizon MDP $M$ with horizon $H$, together with a feature map $\phi$,
such that Assumption~\ref{assumption:realizability} holds (i.e.\ for each $h\in[H]$,
$Q_h^\star(\cdot,\cdot)$ is linear in $\phi(\cdot,\cdot,h)$), and such that the following is true:

If $\mathcal{A}$ outputs a policy $\pi$ satisfying
\[
V_0^{\pi}(s_0) \;\ge\; V_0^\star(s_0)-0.05
\]
with probability at least $0.1$, then $\mathcal{A}$ must make $\exp(\Omega(d))$
calls to the generative model.
\end{theorem}

SK: say we prove a weaker version than that in the chapter due to that
even this version is pretty involved. say we have exp(d) actions.

\subsection*{Construction}

Fix a horizon $H\ge 1$ and a feature dimension $d$. Let $\rho\in(0,1/6]$
be a constant.

\paragraph{Nearly orthogonal vectors.}
Assume we are given unit vectors $\{v(a)\}_{a\in[A]}\subset\R^d$ such that
\[
\forall a\neq a',\qquad |\langle v(a),v(a')\rangle|\le \rho.
\]
Such a set exists for $A\le \exp(c_0\rho^2 d)$ (Johnson--Lindenstrauss packing).

\paragraph{State space.}
For each layer $h\in\{0,1,\dots,H\}$, define the set of nonterminal states
\[
\Scal_h^{\mathrm{nt}}
:= \big\{ \overline{a_{1:h}} : a_1,\dots,a_h\in[A]\ \text{are all distinct}\big\},
\]
with $\Scal_0^{\mathrm{nt}}=\{\overline{\emptyset}\}$.
Let $f$ be an absorbing terminal state and set $\Scal_h:=\Scal_h^{\mathrm{nt}}\cup\{f\}$.

\paragraph{MDP family.}
The family is indexed by $a^\star\in[A]$.  We write $\mathcal{M}_{a^\star}$ for the MDP
with optimal-action index $a^\star$.  The action space is $\Acal=[A]$.
The initial state is $s_0=\overline{\emptyset}$.

\paragraph{Transitions.}
For $h\le H-1$, if $s_h=f$ then $s_{h+1}=f$ surely.
If $s_h=\overline{a_{1:h}}$ is nonterminal and the agent chooses $a\in[A]$, then
\[
s_{h+1} :=
\begin{cases}
f, & a\in\{a_1,\dots,a_h\} \qquad\text{(repeat an action already on the path)}\\
f, & a=a^\star \qquad\qquad\quad\text{(the distinguished action)}\\
\overline{a_{1:h}a}, & \text{otherwise}.
\end{cases}
\]
Thus along any \emph{nonterminal} trajectory, the actions are distinct and never equal to $a^\star$.

%-----------------------------
% Bias sequence (0-indexed)
%-----------------------------

\paragraph{Bias sequence.}
Set $b_0:=1$ and define recursively for $h=0,1,\dots,H-1$,
\[
b_{h+1} := 3\rho\, b_h .
\]
Equivalently, $b_h=(3\rho)^h$ for $h\ge 0$ (note $3\rho\le 1/2$ when $\rho\le 1/6$).

%-----------------------------
% Feature map (with repeat-action truncation)
%-----------------------------

\paragraph{Feature map.}
Define $\phi_h:\Scal_h\times\Acal\to\R^d$ recursively as follows.
For the terminal state, set $\phi_h(f,a)=0$ for all $h$ and $a$.
For the root,
\[
\phi_0(\overline{\emptyset},a_1) := b_0\, v(a_1)=v(a_1).
\]
For $h\ge 1$ and a nonterminal state $s_h=\overline{a_{1:h}}$, define for each $a\in[A]$,
\[
\phi_h(\overline{a_{1:h}},a)
:=
\begin{cases}
0, & a\in\{a_1,\dots,a_h\} \qquad\text{(repeat action)}\\[3pt]
\Big(\big\langle \phi_{h-1}(\overline{a_{1:h-1}},a_h),\, v(a)\big\rangle + b_h\Big)\, v(a),
& a\notin\{a_1,\dots,a_h\}.
\end{cases}
\]
(As usual, $\phi_{h-1}(\overline{a_{1:h-1}},a_h)$ is the feature on the edge entering $\overline{a_{1:h}}$.)

%-----------------------------
% Candidate parameter and Q
%-----------------------------
\paragraph{Parameter and candidate $Q$.}
Fix $a^\star\in[A]$ and set $\theta^\star := v(a^\star)$.
Define the \emph{candidate} action-value function
\[
\widetilde Q_h(s,a) := \langle \phi_h(s,a), \theta^\star\rangle,\qquad h=0,1,\dots,H-1,
\]
and $\widetilde V_h(s):=\max_{a\in\Acal}\widetilde Q_h(s,a)$, with $\widetilde V_H\equiv 0$.

%-----------------------------
% Rewards
%-----------------------------


\paragraph{Rewards (explicit, piecewise).}
Define deterministic rewards $r_h:\Scal_h\times\Acal\to\R$ by:
\begin{itemize}
\item For the last layer $h=H-1$:
\[
r_{H-1}(s,a) := \widetilde Q_{H-1}(s,a)=\langle \phi_{H-1}(s,a),\theta^\star\rangle.
\]
\item For layers $h=0,1,\dots,H-2$:
\[
r_h(s,a) :=
\begin{cases}
\widetilde Q_h(s,a), & \text{if } s_{h+1}=f,\\
-b_{h+1}, & \text{if } s_{h+1}\in\Scal_{h+1}^{\mathrm{nt}}.
\end{cases}
\]
\end{itemize}

\paragraph{Observed rewards (Gaussian noise model).}
The environment has deterministic mean rewards $r_h(s,a)$ defined below, but the learner observes
\[
R_h = r_h(s,a) + \xi_h,\qquad \xi_h\sim\mathcal{N}(0,1)\ \text{i.i.d.}
\]
independently across all queries (and independent of the learner's randomness).


%===========================================================
\subsection*{Key inequality: $a^\star$ is greedy everywhere}

\begin{lemma}[Fresh-direction correlation bound]
\label{lem:fresh_corr_bound}
For any $t\in\{0,1,\dots,H-1\}$, any nonterminal $\overline{a_{1:t}}\in\Scal_t^{\mathrm{nt}}$,
any action $a_{t+1}\notin\{a_1,\dots,a_t\}$, and any
$a'\notin\{a_1,\dots,a_t,a_{t+1}\}$, we have
\[
\big|\big\langle \phi_t(\overline{a_{1:t}},a_{t+1}),\, v(a')\big\rangle\big|
\le \frac{1}{2}\, b_{t+1}.
\]
\end{lemma}

\begin{proof}
We proceed by induction on $t$.

\emph{Base case $t=0$.}
$\phi_0(\overline{\emptyset},a_1)=b_0 v(a_1)=v(a_1)$, hence for $a'\neq a_1$,
\[
|\langle \phi_0(\overline{\emptyset},a_1),v(a')\rangle|
= |\langle v(a_1),v(a')\rangle|
\le \rho
= \frac{1}{3} b_1
\le \frac{1}{2} b_1,
\]
since $b_1=3\rho b_0=3\rho$.

\emph{Inductive step.}
Assume the claim holds for $t-1\ge 0$. Let $a'\notin\{a_1,\dots,a_{t+1}\}$.
By definition (and since $a_{t+1}$ is not a repeat),
\[
\phi_t(\overline{a_{1:t}},a_{t+1})
=
\Big(\langle \phi_{t-1}(\overline{a_{1:t-1}},a_t), v(a_{t+1})\rangle + b_t\Big) v(a_{t+1}).
\]
Thus
\begin{align*}
\big|\langle \phi_t(\overline{a_{1:t}},a_{t+1}), v(a')\rangle\big|
&= |\langle v(a_{t+1}), v(a')\rangle|\,
\Big|\langle \phi_{t-1}(\overline{a_{1:t-1}},a_t), v(a_{t+1})\rangle + b_t\Big|\\
&\le \rho\Big(\frac{1}{2}b_t + b_t\Big)
= \frac{3}{2}\rho b_t
= \frac{1}{2} b_{t+1},
\end{align*}
using the induction hypothesis with $a'=a_{t+1}$ and the recursion $b_{t+1}=3\rho b_t$.
\end{proof}

\begin{lemma}[Greedy action is $a^\star$ everywhere]
\label{lem:greedy_astar}
For every $h\in\{0,1,\dots,H-1\}$ and every nonterminal state $s_h\in\Scal_h^{\mathrm{nt}}$,
\[
\widetilde Q_h(s_h,a^\star)\ \ge\ \frac{1}{2} b_h
\qquad\text{and}\qquad
\max_{a\neq a^\star}\widetilde Q_h(s_h,a)\ \le\ \frac{1}{4} b_h,
\]
hence $a^\star\in\argmax_a \widetilde Q_h(s_h,a)$.
\end{lemma}

\begin{proof}
Fix $h$ and $s_h=\overline{a_{1:h}}\in\Scal_h^{\mathrm{nt}}$.

\emph{Lower bound for $a^\star$.}
If $h=0$, then $\widetilde Q_0(\overline{\emptyset},a^\star)=\langle v(a^\star),v(a^\star)\rangle=1=b_0$.
If $h\ge 1$, then $a^\star\notin\{a_1,\dots,a_h\}$ along any nonterminal path, so
\[
\widetilde Q_h(s_h,a^\star)
= \langle \phi_h(s_h,a^\star), v(a^\star)\rangle
= \langle \phi_{h-1}(\overline{a_{1:h-1}},a_h), v(a^\star)\rangle + b_h .
\]
By Lemma~\ref{lem:fresh_corr_bound} applied at time $t=h-1$ with $a_{t+1}=a_h$ and $a'=a^\star$,
we have
$\big|\langle \phi_{h-1}(\overline{a_{1:h-1}},a_h), v(a^\star)\rangle\big|\le \frac12 b_h$.
Hence $\widetilde Q_h(s_h,a^\star)\ge \frac12 b_h$.

\emph{Upper bound for $a\neq a^\star$.}
Let $a\neq a^\star$.
If $a$ is a repeat action (i.e.\ $a\in\{a_1,\dots,a_h\}$), then by construction $\phi_h(s_h,a)=0$,
so $\widetilde Q_h(s_h,a)=0\le \frac14 b_h$.

Otherwise $a\notin\{a_1,\dots,a_h\}$, and
\[
\widetilde Q_h(s_h,a)
= \langle \phi_h(s_h,a), v(a^\star)\rangle
= \Big(\langle \phi_{h-1}(\overline{a_{1:h-1}},a_h), v(a)\rangle + b_h\Big)\langle v(a),v(a^\star)\rangle.
\]
By Lemma~\ref{lem:fresh_corr_bound} (with $t=h-1$, $a_{t+1}=a_h$, $a'=a$),
$|\langle \phi_{h-1}(\overline{a_{1:h-1}},a_h), v(a)\rangle|\le \frac12 b_h$.
Also $|\langle v(a),v(a^\star)\rangle|\le \rho$.
Therefore,
\[
\widetilde Q_h(s_h,a)
\le \Big(\tfrac12 b_h + b_h\Big)\rho
= \tfrac32 \rho\, b_h
\le \tfrac14 b_h,
\]
using $\rho\le 1/6$.
\end{proof}

%===========================================================
\subsection*{Realizability and Bellman optimality}

\begin{lemma}[Bellman optimality of $\widetilde Q$]
\label{lem:realizability_bellman}
In $\mathcal{M}_{a^\star}$, the function $\widetilde Q$ satisfies the Bellman optimality equations:
for all $h\in\{0,\dots,H-1\}$ and $(s,a)\in\Scal_h\times\Acal$,
\[
\widetilde Q_h(s,a)= r_h(s,a)+ \E\!\left[\widetilde V_{h+1}(s_{h+1})\mid s_h=s,a_h=a\right].
\]
Consequently, $\widetilde Q=Q^\star$ and the greedy policy is optimal.
\end{lemma}

\begin{proof}
We verify the Bellman equation case-by-case.

If $h=H-1$, then $\widetilde V_H\equiv 0$ and $r_{H-1}(s,a)=\widetilde Q_{H-1}(s,a)$ by definition.

Now fix $h\le H-2$.
If $s_{h+1}=f$, then $\widetilde V_{h+1}(f)=0$ and $r_h(s,a)=\widetilde Q_h(s,a)$ by definition.

Otherwise $s_{h+1}\in\Scal_{h+1}^{\mathrm{nt}}$. By Lemma~\ref{lem:greedy_astar},
the greedy action at $s_{h+1}$ is $a^\star$, hence
\[
\widetilde V_{h+1}(s_{h+1}) = \widetilde Q_{h+1}(s_{h+1},a^\star)
= \langle \phi_{h+1}(s_{h+1},a^\star), \theta^\star\rangle.
\]
By the feature recursion and $\theta^\star=v(a^\star)$,
\[
\langle \phi_{h+1}(s_{h+1},a^\star),\theta^\star\rangle
= \langle \phi_h(s,a),\theta^\star\rangle + b_{h+1}
= \widetilde Q_h(s,a) + b_{h+1}.
\]
Therefore, using $r_h(s,a)=-b_{h+1}$ on this branch,
\[
r_h(s,a)+\widetilde V_{h+1}(s_{h+1})
= (-b_{h+1}) + (\widetilde Q_h(s,a)+b_{h+1})
= \widetilde Q_h(s,a),
\]
which is exactly the Bellman equation.

Finally, since $\widetilde Q$ satisfies Bellman optimality, it equals $Q^\star$.
\end{proof}

SK: move theorem to be the start
\begin{theorem}[Linear $Q^\star$ realizability can be exponentially hard (generative model)]
\label{thm:lower_gen_gapless_simple}
Consider any algorithm $\mathcal{A}$ that has access to a generative model and
is given as input a feature map $\phi:\Scal\times\Acal\times[H]\to\R^d$.
There exists a finite-horizon MDP $M$ with horizon $H$, together with a feature map $\phi$,
such that Assumption~\ref{assumption:realizability} holds (i.e.\ for each $h\in[H]$,
$Q_h^\star(\cdot,\cdot)$ is linear in $\phi(\cdot,\cdot,h)$), and such that the following is true:

If $\mathcal{A}$ outputs a policy $\pi$ satisfying
\[
V_0^{\pi}(s_0) \;\ge\; V_0^\star(s_0)-0.05
\]
with probability at least $0.1$, then $\mathcal{A}$ must make $\exp(\Omega(d))$
calls to the generative model.
\end{theorem}


\begin{proof}[Proof sketch]
We instantiate $M$ as one member of the trajectory-tree family $\{M_{a^\star}\}$ from
Section~\ref{sec:linearQstar_gapless_lb}, with Gaussian reward noise as defined above.
In this family the feature map $\phi$ is shared across all instances; the index $a^\star$
only appears through the parameter $\theta^\star=v(a^\star)$ inside $Q^\star$ (and hence the mean reward).

\smallskip\noindent
\textbf{Step 1: Near-optimality forces identifying $a^\star$.}
By Lemma~\ref{lem:greedy_astar} (greedy action is $a^\star$ at every nonterminal state),
in particular at the start state $s_0$ the unique optimal action is $a^\star$ and any $a\neq a^\star$
incurs a constant value loss at $s_0$. Hence any policy that is $0.05$-optimal must select $a^\star$
at $s_0$ with constant probability, i.e.\ it must identify $a^\star$ with constant probability.

\smallskip\noindent
\textbf{Step 2: Until $\mathcal{A}$ queries $a^\star$, its transcript carries (essentially) no information about $a^\star$.}
Fix any query $(s,a,h)$ with $a\neq a^\star$. In the construction, the transition is independent of $a^\star$
and the mean reward $r_h(s,a)$ is also independent of $a^\star$; with Gaussian noise added, the full conditional
distribution of the observed reward remains independent of $a^\star$.
Therefore, the entire transcript distribution is identical across different $a^\star$ unless $\mathcal{A}$
queries an action equal to $a^\star$ at least once.

\smallskip\noindent
\textbf{Step 3: A Fano/averaging bound gives $T=\Omega(|\Acal|)$.}
Let $A:=|\Acal|$, and take $a^\star$ uniformly random in $[A]$.
Let $T$ be the number of generative-model queries made by $\mathcal{A}$, and let $N_a$ be the number of times
$\mathcal{A}$ queries action $a$. Then $\sum_{a=1}^A N_a = T$, so
\[
\Pr(\text{$\mathcal{A}$ ever queries $a^\star$}) \;=\; \E\!\left[\frac{N_{a^\star}}{1}\right]
\;=\; \frac{1}{A}\sum_{a=1}^A \Pr(\text{$\mathcal{A}$ queries $a$ at least once})
\;\le\; \frac{T}{A}.
\]
On the complementary event (probability $\ge 1-T/A$), the transcript is independent of $a^\star$ (Step 2),
so $\mathcal{A}$ cannot identify $a^\star$ with probability better than $1/A$ on that event.
Thus the overall success probability of identifying $a^\star$ is at most
\[
\Pr(\text{identify $a^\star$}) \;\le\; \frac{T}{A} + \frac{1}{A}.
\]
Choosing $T \le cA$ for a sufficiently small constant $c$ makes this strictly smaller than $0.1$.
By Step 1, failing to identify $a^\star$ with constant probability implies failing to output a $0.05$-optimal
policy with probability $0.1$.

Finally, in our instantiation we take $A=\exp(\Theta(d))$ (via the packing lemma), yielding the stated
$\exp(\Omega(d))$ lower bound on the required number of generative-model queries.
\end{proof}



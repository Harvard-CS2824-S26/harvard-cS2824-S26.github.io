\chapter{Information Theoretic Lower Bounds}
%\chapter{Generalization in RL \\ \& Reductions to Supervised Learning}
\label{chap:generalization_stat_limits}

In reinforcement learning, we seek to have learnability results which
are applicable to cases where number of states is large (or, possibly,
countably or uncountably infinite). This is a question of
generalization, which, more broadly, is one of the central
challenges in machine learning. 

%More generally, these are questions of
%generalization, which is one of the central
%challenges in machine learning.

The previous two chapters largely focussed on \emph{sufficient}
conditions under which we can obtain sample complexity results which
do not explicitly depend on the size of the state (or action) space.  This chapter 
focusses on what are \emph{necessary} conditions for
generalization. Here, we can frame our questions by examining 
the extent to which generalization in RL is similar to (or different
from) that in supervised learning. Two most basic settings in
supervised learning are: (i) agnostic learning (i.e. finding the best
classifier or hypothesis in some class) and (ii) learning with linear
models (i.e. learning the best linear regressor or the best linear
classifier).  This chapter will focus on \emph{lower bounds} with
regards to the analogues of these two questions for reinforcement learning:
\begin{itemize}
\item (Agnostic learning) Given some hypothesis class (of policies,
  value functions, or models), what is the sample complexity of
  finding (nearly) the best hypothesis in this class?
\item (Linearly realizable values or policies) Suppose we are given
  some $d$-dimensional feature mapping where we are guaranteed that
  either the optimal value function is linear in these given features
  or that the optimal policy has a linear parameterization.  Are we
  able to obtain sample complexity guarantees that are polynomial in
  $d$, with little to no explicit dependence on the size of the state
  or action spaces? We will consider this question in both the offline
  setting (for the purposes of policy evaluation, as in
  Chapter~\ref{chap:Bellman_complete}) and for in online setting where
  our goal is to learn a near optimal optimal policy.
\end{itemize}
Observe that supervised learning can be viewed as horizon one, $H=1$, RL
problem (where the learner only receives feedback for the ``label'',
i.e. the action, chosen). We can view the second question above as the
analogue of linear regression or classification with halfspaces. In
supervised learning, both of these settings have postive answers, and
they are fundamental in our understanding of generalization.  Perhaps
surprisingly, we will see negative answers to these questions in the
RL setting.  The
significance of this provides insights as to why our study of
generalization in reinforcement learning is substantially more subtle than in
supervised learning. Importantly, the insights we develop here will also help us to
motivate the more refined assumptions and settings that we consider in
subsequent chapters (see Section~\ref{sec:how_to_study_RL} for discussion).


This chapter will work with finite horizon MDPs, where we consider
both the episodic setting and the generative model setting. 
With regards to the first question on agnostic learning, this chapter
follows the ideas first introduced
in~\cite{NIPS1999_1664}.  With regards to the second
question on linear realizability, this chapter follows the results
in~\cite{du2019good,wang2020statistical,WeiszAS21,Wang_linear_lower}. 


\section{Agnostic Learning}

Suppose we have a hypothesis class $\Hcal$ (either finite or
infinite), where for each $f \in \Hcal$ we have an associated policy
$\pi_f:\Scal \rightarrow \Acal$, which, for simplicity, we assume is deterministic.
Here, we could have that:
\begin{itemize}
\item $\Hcal$ itself is a class of policies.
\item $\Hcal$ is a set of state-action values, where for $f \in \Hcal$,
  we associate it with the greedy policy $\pi_f(s,h) = \argmax_a
  f_h(s,a)$, where $s\in\Scal$ and $h\in[H]$.
\item $\Hcal$ could be a class of models (i.e. each $f \in \Hcal$ is an
  MDP itself). Here, for each $f \in \Hcal$, we can let $\pi_f$ be the
  optimal policy in the MDP $f$. 
\end{itemize}
We let $\Pi$ denote the induced set of policies from our hypothesis
class $\Hcal$, i.e. $\Pi= \{\pi_f | f \in \Hcal\}$.


The goal of agnostic learning can be formulated by the following optimization problem:
\begin{equation}\label{eq:ag_learn_objective}
\max_{\pi\in\Pi} \E_{s_0\sim \mu} V^{\pi}(s_0),
\end{equation}
where we are interested in the number of samples required to 
approximately solve this optimization problem. We will work both in
the episodic setting and the generative model setting, and, for
simplicity, we will restrict ourselves to finite hypothesis classes.

\iffalse
As before, we only hope to perform favorably against the best policy
in $\Pi$. Recall that in our aforementioned sampling model we have
the ability to obtain trajectories from $s_0\sim \mu$ under policies
of our choosing. As we have seen, agnostic learning is possible in
the supervised learning setting, with regret bounds that have no dependence
on the size of the domain --- the size of domain is analogous to the size
the state space $|\Scal|$. 
\fi

\paragraph{Binary classification as an $H=1$ RL problem:} 
Observe that the problem of binary classification can be viewed as
learning in an MDP with a horizon of one. In particular, take $H=1$;
take $|\Acal|=2$; let the distribution over starting states $s_0\sim\mu$
correspond to the input distribution; and take the reward function as
$r(s,a) = \ind(\textrm{label}(s) =a)$. In other words, we equate our
action with the prediction of the binary class membership, and the reward
function is determined by if our prediction is correct or not.

\subsection{Review: Binary Classification}

One of the most important concepts for learning binary classifiers is
that it is possible to \emph{generalize} even when the state space is
infinite. Here note that the domain of our classifiers, often denoted
by $\mathcal{X}$, is analogous to the state space $\Scal$. We now
briefly review some basics of supervised learning before we turn to
the question of generalization in reinforcement learning.

Consider the problem of binary classification with $N$ labeled
examples of the form $(x_i, y_i)_{i=1}^N$, with $x_i \in
\Xcal$ and $y_i \in \{0,1\}$. Suppose we have a (finite or infinte) set $\Hcal$ of
binary classifiers where each $h \in \Hcal$ is a mapping of the form
$h :\Xcal \to \{0,1\}$. Let $\ind(h(x) \ne y)$ be an indicator which
takes the value $0$ if $h(x) = y$ and $1$ otherwise. We assume that our samples
are drawn i.i.d. according to a fixed joint distribution $D$ over $(x,y)$.

Define the empirical error and the true error as:
\[
\widehat{\err}(h) = 
\frac{1}{N} \sum_{i=1}^N \ind(h(x_i) \ne y_i) , \quad
\err(h) = \E_{(X,Y)\sim D} \ind(h(X) \ne Y).
\]
For a given $h \in \Hcal$, Hoeffding's inequality implies that with
probability at least $1-\delta$: 
\[\left|\textrm{err}(h) - \widehat{\textrm{err}}(h)
\right| \leq \sqrt{\frac{1}{2N}\log\frac{2}{\delta}}.\]
This and the union bound give rise to what is often referred to as the
``Occam's razor'' bound:

\begin{proposition}\label{proposition:occam_sl}
(The ``Occam's razor'' bound)
Suppose $\Hcal$ is finite.
Let $\widehat{h} = \arg\min_{h \in \Hcal}\widehat{\textrm{err}}(h)$.
%and $h^\star = \arg\min_{h \in  \Hcal} \err(h)$. 
With probability at least $1-\delta$:
\[
\err(\widehat{h})  \leq \min_{h \in  \Hcal}\err(h) +
\sqrt{\frac{2}{N}\log\frac{2|\Hcal|}{\delta}}.
\]
\end{proposition}

Hence, provided that
\[
N \geq \frac{2 \log\frac{2|\Hcal|}{\delta}}{\eps^2},
\]
then with probability at least
$1-\delta$, we have that:
\[
\err(\widehat{h})  \leq \min_{h \in  \Hcal}\err(h)+\eps.
\]
A key observation here is that the
our regret --- the regret is the left hand side of the above
inequality --- has \emph{no dependence} on the size of
$\mathcal{X}$ (i.e. $\Scal$) which may be infinite and is only
logarithmic in the number of hypothesis in our class.



\subsection{Importance Sampling and a Reduction to Supervised Learning}

Now let us return to the agnostic learning question in
Equation~\ref{eq:ag_learn_objective}. We will see that, provided we
are willing to be exponential in the horizon $H$, then agnostic
learning is possible. Furthermore, it is a straightforward argument to
see that we are not able to do better.


\subsubsection*{An Occam's Razor Bound for RL}

We now provide a reduction of RL to the supervised learning problem,
given only sampling access in the episodic setting. The key issue is
how to efficiently reuse data. The idea is that we will simply collect $N$
trajectories by executing a policy which chooses actions uniformly at
random; let $\textrm{Unif}_{\Acal}$ denote this policy. 

The following shows how we can obtain an unbiased estimate of
the value of any policy $\pi$ using this uniform policy $\textrm{Unif}_{\Acal}$:
\begin{lemma}
(Unbiased estimation of $V^\pi_0(\mu)$) Let $\pi$ be any
deterministic policy. We have that:
\[
V^\pi_0(\mu) = 
|\Acal|^H \cdot \E_{\tau\sim \Pr_{\textrm{Unif}_{\Acal}}}
\left[\ind\Big(\pi(s_0)=a_0,\ \ldots ,\pi(s_{H-1})=a_{H-1}\Big)
\sum_{h=0}^{H-1} r(s_h,a_h)\right]
\]
where $\Pr_{\textrm{Unif}_{\Acal}}$ specifies the distribution over 
trajectories $\tau=(s_0,a_0,r_0, \ldots s_{H-1}, a_{H-1},r_{H-1})$
under the policy $\textrm{Unif}_{\Acal}$.
\end{lemma}

The proof follows from a standard importance sampling
argument (applied to the distribution over the trajectories). 
\begin{proof}
We have that:
\begin{align*}
V^\pi_0(\mu) &= 
\E_{\tau\sim \Pr_{\pi}}
\left[\sum_{h=0}^{H-1} r_h\right]\\
&=
\E_{\tau\sim \Pr_{\textrm{Unif}_{\Acal}}}
\left[
\frac{\Pr_{\pi}(\tau)}{\Pr_{\textrm{Unif}_{\Acal}}(\tau)}
%\ind\Big(\pi(s_0)=a_0,\ \ldots ,\pi(s_{H-1})=a_{H-1}\Big)
\sum_{h=0}^{H-1} r_h\right]\\
&=|\Acal|^H \cdot \E_{\tau\sim \Pr_{\textrm{Unif}_{\Acal}}}
\left[\ind\Big(\pi(s_0)=a_0,\ \ldots ,\pi(s_{H-1})=a_{H-1}\Big)
\sum_{h=0}^{H-1} r_h\right]
\end{align*}
where the last step follows due to that the probability ratio is only
nonzero when $\textrm{Unif}_{\Acal}$ choose actions identical to that
of $\pi$.
\end{proof}

Crudely, the factor of $|\Acal|^H$ is due to that the estimated reward of $\pi$ on a
trajectory is nonzero only when $\pi$ takes exactly identical
actions to those taken by $\textrm{Unif}_{\Acal}$ on the trajectory,
which occurs with probability $1/|\Acal|^H$.

% in which case the
%estimated value of $\pi$ is $|\Acal|^H$ times that of $\textrm{Unif}_{\Acal}$.


Now, given sampling access in the episodic model, we can use
$\textrm{Unif}_{\Acal}$ to get any estimate of any other policy
$\pi\in\Pi$.
Note that the factor of $|\Acal|^H$ in the previous lemma will lead to this approach being a high
variance estimator. Suppose we draw $N$ trajectories under $\textrm{Unif}_{\Acal}$.
Denote the $n$-th sampled trajectory by $(s_0^n, a_0^n, r_1^n, s_1^n,
\ldots,s_{H-1}^n, a_{H-1}^n, r_{H-1}^n)$. We can then use following to estimate the
finite horizon reward of any given policy $\pi$:
\[
\widehat V^\pi_0(\mu) = \frac{|\Acal|^H}{N}\sum_{n=1}^N 
\ind\Big(\pi(s_0^n)=a_0^n, \ldots \pi(s_{H-1}^n)=a_{H-1}^n\Big)
\sum_{t=0}^{H-1} r(s_t^n,a_t^n).
\]

\begin{proposition}\label{proposition:occam_RL}
(An ``Occam's razor bound'' for RL)
Let $\delta\geq 0$. Suppose $\Pi$ is a finite and suppose we use the aforementioned estimator, $\widehat
V^{\pi}_0(\mu)$, to estimate the value of every $\pi\in\Pi$.  Let
$\widehat{\pi} = \arg\max_{\pi\in\Pi}\widehat V^{\pi}_0(\mu)$. 
%and  $\pi^\star = \arg\max_{\pi \in  \Pi} V^\pi(s_0)$.  
We have that with probability at least
$1-\delta$,
\[
V_0^{\widehat{\pi}}(\mu) \geq \max_{\pi \in  \Pi} V_0^{\pi}(\mu) -
H |\Acal|^H\sqrt{\frac{2}{N}\log\frac{2|\Pi|}{\delta}}.
\]
\end{proposition}

\begin{proof}
Note that, fixing $\pi$, our estimator is sum of i.i.d. random variables,
where each independent estimator,  $ |\Acal|^H
\ind\Big(\pi(s_0^n)=a_0^n, \ldots \pi(s_{H-1}^n)=a_{H-1}^n\Big)
\sum_{t=0}^{H-1} r(s_t^n,a_t^n)$, is bounded by $H |\Acal|^H$. The
remainder of the argument is identical to that used in Proposition~\ref{proposition:occam_sl}.
\end{proof}

Hence, provided that 
\[
N \geq  H |\Acal|^H \ \frac{2\log(2|\Pi|/\delta)}{\eps^2},
\]
then with probability at least
$1-\delta$, we have that:
\[
V_0^{\widehat \pi}(s_0) \geq \max_{\pi \in  \Pi} V_0^\pi(s_0)  -\eps.
\]
This is the analogue of the Occam's razor bound for RL.

Importantly, the above shows that we can avoid dependence on the size
of the state space, though this comes at the price of an exponential dependence
on the horizon. As we see in the next section, this dependence is
unavoidable (without making further assumptions).

\paragraph{Infinite Policy Classes.}
In the supervised learning setting, a crucial observation is that even
though a hypothesis set $\Hcal$ of binary classifiers may be
infinite, we may still be able to obtain non-trivial generalization
bounds. A crucial observation here is that even
though the set $\Hcal$ may be infinite, the number of
possible behaviors of on a finite set of states is not necessarily
exhaustive. 
The
\emph{Vapnikâ€“Chervonenkis} (VC) dimension of $\Hcal$, $VC(\Hcal)$, is formal way to
characterize this intuition, and, using this concept, we are able to
obtain generalization bounds in terms of the $VC(\Hcal)$.

With regards to infinite hypothesis classes of policies (say for the
case where $|\Acal|=2$), extending our Occam's razor bound can be done
with precisely the same approach. In particular, when $|\Acal|=2$,
each $\pi\in\Pi$ can be viewed as Boolean function, and this gives
rise to the VC dimension $\textrm{VC}(\Pi)$ of our policy class. The
bound in Proposition~\ref{proposition:occam_RL} can be replaced with a
VC-dimension bound that is analogous to that of binary
classification (see Section~\ref{ch3:bib}). 


\subsubsection*{Lower Bounds}

Clearly, the drawback of this agnostic learning approach is that we
would require a number of samples that is exponential in
the problem horizon. We now see that if we desire a
sample complexity that scales with $O(\log|\Pi|)$, then an
exponential dependence on the effective horizon is unavoidable,
without making further assumptions.

Here, our lower bound permits a (possibly randomized) algorithm to utilize a generative
model (which is a more powerful sampling model than the episodic one).
% and
%the algorithm may return a policy $\pi$ not necessarily corresponding to
%any policy in $\Pi$.

\begin{proposition}
(Lower Bound with a Generative Model)
Suppose algorithm $\Acal$ has access to a generative model.
There exists a policy class $\Pi$, where $|\Pi| =|\Acal|^H$  such that if algorithm $\Acal$
returns any policy $\pi$ (not necessarily in $\Pi$) such that
\[
V_0^{\pi}(\mu) \geq \max_{\pi\in\Pi} V_0^{\pi}(\mu) - 0.5.
\]
with probability greater than $1/2$, then $\Acal$ must make a number
of number calls $N$ to the generative model where:
\[
N \geq c |\Acal|^H
\]
(where $c$ is an absolute constant).
\end{proposition}


\begin{proof}
The proof is simple. Consider a $|\Acal|$-ary balanced tree, with
$|\Acal|^H$ states and $|\Acal|$ actions, where states correspond
nodes and actions correspond to edges; actions always move the agent
from the root towards a leaf node. We make only one leaf node
rewarding, which is unknown to the algorithm. We consider the policy class  to be
all $|\Acal|^H$ policies. The theorem now immediately follows since
the algorithm gains no knowledge of the rewarding leaf node unless it
queries that node.
\end{proof}

Note this immediately rules out the possibility that any algorithm which can obtain a $\log
|\Pi|$ dependence without paying a factor of $|\Acal|^H$ in the
sample size due to that $\log |\Pi| = H \log |\Acal|$ in the example above.


\section{Linear Realizability}
\label{sec:linear_Qvalues}

In supervised learning, two of the most widely studied settings are
those of linear regression and binary classification with
halfspaces. In both settings, we are able to obtain sample complexity
results that are polynomial in the feature dimension. We now consider the
analogue of these assumptions for RL, starting with the analogue of
linear regression.

When the state space is large or infinite, we may hope that linearly
realizability assumptions may permit a more sample
efficient approach. We will start with linear realizability on $Q^\pi$
and consider the offline policy evaluation problem. Then we will
consider the problem of learning with only a linearly realizability assumption
on $Q^\star$ (along with access to either a generative model or
sampling access in the episodic setting).

\subsection{Offline Policy Evaluation with Linearly Realizable Values}

In Chapter~\ref{chap:Bellman_complete}, we observed that the LSVI and LSPE
algorithm could be used with an offline dataset for the purposes of
policy evaluation. Here, we made the linear Bellman completeness
assumption on our features.  Let us now show that with only a linear
realizability assumption, then not only is LSPE sample inefficient but
we will also see that, information theoretically, \emph{every} algorithm is
sample inefficient, in a minmax sense.

This section is concerned with the offline RL setting.  In this
setting, the agent does not have direct access to the MDP and instead
is given access to data distributions $\{\mu_h\}_{h = 0}^{H-1}$ where for
each $h \in [H]$, $\mu_h \in \Delta(\Scal_h \times \Acal)$.  The
inputs of the agent are $H$ datasets $\{D_h\}_{h = 0}^{H-1}$, and for each
$h \in [H]$, $D_h$ consists i.i.d. samples of the form
$(s, a, r, s') \in \Scal_h \times \Acal \times \mathbb{R} \times
\Scal_{h + 1}$
tuples, where $(s, a)\sim\mu_h$, $r \sim r(s, a)$, $s' \sim P(s, a)$.

We now focus on the \emph{offline policy evaluation} problem: given a policy
$\pi : \Scal \to \Delta\left(\Acal\right)$ and a feature
mapping $\phi : \Scal \times \Acal \to \mathbb{R}^d$, the goal
is to output an accurate estimate of the value of $\pi$ (i.e.,
$V^{\pi}$) approximately, using the collected datasets
$\{D_h\}_{h = 0}^{H-1}$, with as few samples as possible.

We will make the following linear realizability assumption with
regards to a feature mapping
$\phi : \Scal \times \Acal \to \mathbb{R}^d$, which we can think
of as  either being
hand-crafted or coming from a pre-trained neural network that transforms a
state-action pair to a $d$-dimensional embedding, and the
$Q$-functions can be predicted by linear functions of the features.
In particular, this section will assume the following \emph{linear realizability}
assumption with regards to \emph{every} policy $\pi$.
\begin{assumption}[Realizable Linear Function Approximation] \label{assmp:realizability}
For every policy $\pi :
\Scal \to \Delta(\Acal)$, there exists $\theta_0^{\pi}, \ldots
\theta_{H - 1}^{\pi} \in \mathbb{R}^d$ such that for all $(s, a) \in \Scal
\times \Acal
$ and $h \in [H]$,
\[
Q^{\pi}_h(s, a) = \left(\theta_h^{\pi}\right)^{\top}\phi(s, a).
\]
\end{assumption}
Note that this assumption is substantially stronger than 
assuming realizability with regards to a single target policy $\pi$
(say the policy that we
wish to evaluate); this assumption imposes realizability for
all policies. 

We will also assume a \emph{coverage} assumption, analogous to
Assumption~\ref{assumption:coverage}.  It should be evident that without
feature coverage in our dataset, realizability alone is clearly not
sufficient for sample-efficient estimation.  Note that , we will make the
strongest possible assumption, with regards to the conditioning of the
feature covariance matrix; in particular, this assumption is equivalent to
$\mu$ being a $D$-optimal design.

\begin{assumption}[Coverage]\label{assmp:coverage}
For all $(s, a) \in \Scal \times \Acal$, assume our feature map
is bounded such that $\|\phi(s, a)\|_2 \le 1$. Furthermore,
suppose for each $h \in [H]$, the data distributions $\mu_h$ satisfies
the following:
\[\E_{(s, a) \sim \mu_h}[\phi(s, a)\phi(s, a)^{\top}]=
\frac{1}{d} I.
\]
Note that the minimum eigenvalue of the above matrix is $1/d$, which is the largest possible minimum
eigenvalue over all data distributions $\widetilde\mu_h$, since
$\sigma_{\min}(\E_{(s, a) \sim \widetilde{\mu}_h}[\phi(s, a)\phi(s,
a)^{\top}] )$ is less than or equal to $1/d$ (due to that $\|\phi(s, a)\|_2\leq 1$ for all 
$(s,a) \in \Scal \times \Acal$). Also, it is not difficult to see that
this distribution satisfies the
$D$-optimal design property.
\end{assumption}

Clearly, for the case where $H=1$, 
the realizability assumption (Assumption~\ref{assmp:realizability}),
%boundedness assumption (Assumption~\ref{assmp:boundedness}) 
and
coverage assumption (Assumption~\ref{assmp:coverage}) 
imply 
that the ordinary least squares estimator will accurately estimate
$\theta_0^{\pi}$.
%\footnote{For $H=1$, the ordinary least squares estimator will satisfy
%that $\|\theta_1-\widehat \theta_{\textrm{OLS}}\|_2^2 \leq
%O(d/n)$ with high probability. See e.g.~\citep{hsu2012random}.} 
The following shows these
assumptions are not sufficient for offline policy evaluation for long horizon problems.

\begin{theorem}\label{thm:hard_det_1}
%Suppose Assumption~\ref{assmp:boundedness} and Assumption~\ref{assmp:coverage} hold.
Suppose Assumption~\ref{assmp:coverage} holds.
Fix an algorithm that takes as input both a policy and a feature mapping.
There exists a (deterministic) MDP satisfying Assumption~\ref{assmp:realizability}, such that for \emph{any} policy $\pi : \Scal \to \Delta(\Acal)$, 
the algorithm requires $\Omega((d / 2)^H)$ samples to output
the value of $\pi$ up to constant additive approximation error
with probability at least $0.9$. 
\end{theorem}

Although we focus on offline policy evaluation, this
hardness result also holds for finding near-optimal policies under
Assumption~\ref{assmp:realizability} in the offline RL setting with
linear function approximation.  Below we give a simple reduction.  At
the initial state, if the agent chooses action $a_1$, then the agent
receives a fixed reward value (say $0.5$) and terminates.  If the
agent chooses action $a_2$, then the agent transits to the hard
instance.  Therefore, in order to find a policy with suboptimality at
most $0.5$, the agent must evaluate the value of the optimal policy in
the hard instance up to an error of $0.5$, and hence the hardness
result holds.

\paragraph{Least-Squares Policy Evaluation (LSPE) has exponential variance.}
For offline policy evaluation with linear function approximation, 
it is not difficult to see that LSPE, Algorithm~\ref{alg:lspe}, will provide an unbiased
  estimate (provided the feature covariance matrices are full rank,
  which will occur with high probability).  Interestingly, as a direct corollary,
  the above theorem implies that LSPE has
  exponential variance in $H$. 
   More generally, this theorem implies that
  there is no estimator that can avoid such
  exponential dependence in the offline setting. 

\subsubsection*{A Hard Instance for Offline Policy Evaluation}
%dProof of Theorem~\ref{thm:hard_det_1}}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{Figures/hard1_new}
\caption{An illustration of the hard instance.
Recall that $\hat{d} = d/2$.
States on the top are those in the first level ($h = 0$), while states at the bottom are those in the last level $(h = H - 1)$. 
Solid line (with arrow) corresponds to transitions associated with action $a_1$, while dotted line (with arrow) corresponds to transitions associated with action $a_2$.
For each level $h \in [H]$, reward values and $Q$-values associated with $s_h^1, s_h^2, \ldots, s_h^{\hat{d}}$ are marked on the left, while  reward values and $Q$-values associated with $s_h^{\hat{d} + 1}$ are mark on the right. 
Rewards and transitions are all deterministic, except for the reward distributions associated with $s_{H - 1}^1, s_{H - 1}^2, \ldots, s_{H - 1}^{\hat{d}}$.
We mark the expectation of the reward value when it is stochastic. 
For each level $h \in [H]$, for the data distribution $\mu_h$, the state is chosen uniformly at random from those states in the dashed rectangle, i.e., $\{s_h^1, s_h^2, \ldots, s_h^{\hat{d}}\}$, while the action is chosen uniformly at random from $\{a_1, a_2\}$.
Suppose the initial state is $s_1^{\hat{d} + 1}$.
When $r_\infty = 0$, the value of the policy is $0$.
When $r_\infty = {\hat{d}}^{-H/2}$, the value of the policy is $r_\infty \cdot {\hat{d}}^{H / 2} = 1$.
}
\label{fig:hard1}
\end{figure}

We now provide the hard instance construction and the proof of Theorem~\ref{thm:hard_det_1}.
We use $d$ to denote the feature dimension, and we assume $d$ is even for simplicity.
We use $\hat{d}$ to denote $d / 2$ for convenience.
We also provide an illustration of the construction in Figure~\ref{fig:hard1}.
 
\paragraph{State Space, Action Space and Transition Operator.}
The action space $\Acal = \{a_1, a_2\}$.
For each $h \in [H]$, $\Scal_h$ contains $\hat{d} + 1$ states $s_h^1, s_h^2, \ldots, s_h^{\hat{d}}$ and $s_h^{\hat{d} + 1}$. 
For each $h \in \{0, 1, \ldots, H - 2\}$, for each $c \in \{1, 2, \ldots, \hat{d} + 1\}$, we have 
\[
P(s|s_h^c, a) = 
\begin{cases}
1 & s=s_{h + 1}^{\hat{d} + 1}, \  a = a_1\\
1 & s=s_{h + 1}^c , \ a = a_2\\
0 & \textrm{else}
\end{cases}.
\]

\paragraph{Reward Distributions.}
Let $0 \le r_\infty \le \hat{d}^{-H/2}$ be a parameter to be determined. 
For each $(h, c) \in \{0, 1, \ldots, H - 2\} \times [\hat{d}]$ and $a \in \Acal$, we set $r(s_h^c, a) = 0$ and $r(s_h^{\hat{d} + 1}, a) = r_\infty \cdot (\hat{d}^{1/2} - 1) \cdot {\hat{d}}^{(H - h - 1) / 2}$.
For the last level, for each $c \in [\hat{d}]$ and $a \in \Acal$, we set 
\[
r(s_{H - 1}^c, a) = 
\begin{cases}
1 & \text{with probability $(1+r_\infty)/2$}\\
-1 & \text{with probability $(1 - r_\infty)/2$}
\end{cases}
\]
so that $\E[r(s_{H - 1}^c, a)] = r_\infty$.
Moreover, for all actions $a \in \Acal$, $r(s_{H - 1}^{\hat{d} + 1}, a) = r_\infty \cdot \hat{d}^{1/2}$.

\paragraph{Feature Mapping.}
Let $e_1, e_2, \ldots, e_d$ be a set of orthonormal vectors in $\mathbb{R}^d$.
Here, one possible choice is to set $e_1, e_2, \ldots, e_d$ to be the standard basis vectors. 
For each $(h, c) \in [H] \times [\hat{d}]$, we set 
$\phi(s_h^c, a_1) = e_c, \phi(s_h^c, a_2) = e_{c + \hat{d}}$,
and \[\phi(s_h^{\hat{d} + 1}, a) =\frac{1}{\hat{d}^{1/2}}\sum_{c \in \hat{d}}e_c \] for all $a \in \Acal$.
 
%\paragraph{Verifying Assumption~\ref{assmp:realizability} and Assumption~\ref{assmp:boundedness}.}
%Now we verify that Assumption~\ref{assmp:realizability} and Assumption~\ref{assmp:boundedness} hold for our construction. 

\paragraph{Verifying Assumption~\ref{assmp:realizability}.}
Now we verify that Assumption~\ref{assmp:realizability} holds for this construction. 
\begin{lemma}\label{lem:q_linear_1}
For every policy $\pi : \Scal \to \Delta(\Acal)$, for each $h \in [H]$, for all $(s, a) \in \Scal_h \times \Acal$, we have $Q^{\pi}_h(s, a) = \left( \theta_h^{\pi}\right)^{\top}\phi(s, a)$ for some $\theta_h^{\pi} \in \mathbb{R}^d$.
\end{lemma}
\begin{proof}
We first verify $Q^{\pi}$ is linear for the first $H - 1$ levels. 
For each $(h, c) \in  \{0, 1, \ldots, H - 2\} \times [\hat{d}]$, we have
\begin{align*}
Q^{\pi}_h(s_h^c, a_1)  = &r(s_h^c, a_1) + r(s_{h + 1}^{\hat{d} + 1}, a_1) + r(s_{h + 2}^{\hat{d} + 1}, a_1) + \ldots + r(s_{H - 1}^{\hat{d} + 1}, a_1) = r_\infty \cdot {\hat{d}}^{(H - h - 1) / 2}.
\end{align*}

Moreover, for all $a \in \Acal$, 
\begin{align*}
Q^{\pi}_h(s_h^{\hat{d} + 1}, a) = & r(s_h^{\hat{d} + 1}, a) + r(s_{h + 1}^{\hat{d} + 1}, a_1) + r(s_{h + 2}^{\hat{d} + 1}, a_1) + \ldots + r(s_{H - 1}^{\hat{d} + 1}, a_1) =  r_\infty \cdot {\hat{d}}^{(H - h) / 2 }.
\end{align*}

Therefore, if we define 
\[\theta_h^{\pi} = \sum_{c = 1}^{\hat{d}} r_\infty \cdot {\hat{d}}^{(H - h - 1) / 2} \cdot e_c + \sum_{c = 1}^{\hat{d}}  Q^{\pi}_{h}(s_{h}^c, a_2) \cdot e_{c + \hat{d}},\] 
then $Q_h^{\pi}(s, a) = \left( \theta_h^{\pi}\right)^{\top}\phi(s, a)$ for all $(s, a) \in \Scal_h \times \Acal$. 
%Clearly, we have $\|\theta_h^{\pi}\|_2 \le H\sqrt{d}$ for all $h \in [H - 1]$.

Now we verify that the $Q$-function is linear for the last level. 
Clearly, for all $c \in [\hat{d}]$ and $a \in \Acal$, $Q_{H - 1}^{\pi}(s_{H - 1}^c, a) = r_\infty$ and $Q_{H - 1}^{\pi}(s_{H - 1}^{\hat{d} + 1}, a) = r_\infty \cdot \sqrt{\hat{d}}$.
Thus by defining $\theta_{H - 1}^{\pi} = \sum_{c = 1}^d r_\infty \cdot e_c$,  we have $Q_{H - 1}^{\pi}(s, a) = \left(\theta_{H - 1}^{\pi}\right)^{\top} \phi(s, a)$ for all $(s, a) \in \Scal_{H - 1} \times \Acal$. 

\end{proof}

\paragraph{The Data Distributions.}
For each level $h \in [H]$, the data distribution $\mu_h$ is a uniform
distribution over the set
$\{(s_h^1, a_1), (s_h^1, a_2), (s_h^2, a_1), (s_h^2, a_2), \ldots,
(s_h^{\hat{d}}, a_1), (s_h^{\hat{d}}, a_2)\}$.
Notice that $(s_h^{\hat{d} + 1}, a)$ is {\em not} in the support of
$\mu_h$ for all $a \in \Acal$.  It can be seen that,
\[
\E_{(s, a) \sim \mu_h}\left[\phi(s, a)\phi(s, a)^{\top}\right] =
\frac{1}{d} \sum_{c = 1}^d e_c e_c^{\top} = \frac{1}{d}I.
\]

\subsubsection*{The Information-Theoretic Argument}
%\paragraph{The Information Theoretic Argument.}

We show that it is information-theoretically hard for any algorithm to distinguish the case $r_\infty = 0$ and $r_\infty = {\hat{d}}^{-H/2}$.
We fix the initial state to be $s_0^{\hat{d} + 1}$, and consider any policy $\pi : \Scal \to \Delta(\Acal)$.
When $r_\infty = 0$, all reward values will be zero, and thus the value of $\pi$ would be zero.
On the other hand, when $r_\infty = {\hat{d}}^{-H / 2}$, the value of $\pi$ would be $r_\infty \cdot {\hat{d}}^{H / 2 } = 1$.
Thus, if the algorithm approximates the value of the policy up to an error of $1/2$, then it must distinguish the case that $r_\infty = 0$ and $r_\infty = {\hat{d}}^{-H/2}$.

We first notice that for the case $r_\infty = 0$ and $r_\infty =
{\hat{d}}^{-H/2}$, the data distributions $\{\mu_h\}_{h= 0}^{H-1}$, the
feature mapping $\phi : \Scal
 \times \Acal \to \mathbb{R}^d$, the policy $\pi$ to be evaluated and the transition operator $P$ are the same.
Thus, in order to distinguish the case $r_\infty = 0$ and $r_\infty = {\hat{d}}^{-H/2}$, the only way is to query the reward distribution by using sampling taken from the data distributions. 

For all state-action pairs $(s, a)$ in the support of the data distributions of the first $H - 1$ levels, the reward distributions will be identical.
This is because for all $s \in \Scal_h \setminus \{s_h^{\hat{d} + 1}\}$ and $a \in \Acal$, we have $r(s, a) = 0$.
For the case $r_\infty = 0$ and $r_\infty = {\hat{d}}^{-H/2}$, for all state-action pairs $(s, a)$ in the support of the data distribution of the last level, 
\[
r(s, a) = 
\begin{cases}
1 & \text{with probability $(1 + r_\infty)/2$} \\
-1 & \text{with probability $(1 - r_\infty) / 2$} 
\end{cases}.
\]
Therefore, to distinguish the case that $r_\infty = 0$ and $r_\infty = {\hat{d}}^{-H/2}$, the agent needs to distinguish two reward distributions  
\[
r^{(1)} = \begin{cases}
1 & \text{with probability $1/2$}\\
-1 & \text{with probability $1/2$}
\end{cases}
\]
and 
\[
r^{(2)} = \begin{cases}
1 & \text{with probability $(1 + {\hat{d}}^{-H/2}) / 2$}\\
-1 & \text{with probability $(1 - {\hat{d}}^{-H/2}) / 2$}
\end{cases}.
\]
It is standard argument that in order to distinguish $r^{(1)}$ and $r^{(2)}$ with
probability at least $0.9$, any algorithm requires
$\Omega({\hat{d}}^H)$ samples.   
%See e.g. Lemma 5.1 in~\citep{anthony2009neural}. See
%also~\citep{chernoff1972sequential, mannor2004sample}. 

The key in this construction is the state $s_h^{\hat{d} + 1}$ in each
level, whose feature vector is defined to be
$\sum_{c \in \hat{d}}e_c / \hat{d}^{1/2}$.  In each level,
$s_h^{\hat{d} + 1}$ amplifies the $Q$-values by a $\hat{d}^{1/2}$
factor, due to the linearity of the $Q$-function.  After all the $H$
levels, the value will be amplified by a $\hat{d}^{H / 2}$ factor.
Since $s_h^{\hat{d} + 1}$ is not in the support of the data
distribution, the only way for the agent to estimate the value of the
policy is to estimate the expected reward value in the last level.
This construction forces the estimation error of the last level to be
amplified exponentially and thus implies an exponential lower bound.

\iffalse
We would like to remark that the design of the feature mapping in this construction could be flexible.
It suffices if $e_1, e_2, \ldots, e_d$ are only nearly orthogonal.
Moreover, the feature of $s_h^{\hat{d} + 1}$ can be changed to
$\sum_{c=1}^{\hat{d}} w_c e_c$ for a general set of coefficients $w_1,
w_2, \ldots, w_{\hat{d}}$ so long as $\sum_{c = 1}^{\hat{d}}w_c$ is
sufficiently large. 
\fi


\subsection{Linearly Realizable $Q^\star$}
\label{sec:linearQstar}

Specifically, we will assume
access to a feature map $\phi:\mathcal{S}\times\Acal\to\R^d$,
and we will assume that a linear function of $\phi$ can accurately
represent the $Q^\star$-function. Specifically,
\begin{assumption}[Linear $Q^\star$ Realizability]
\label{assumption:realizability}
For all $h\in [H]$, assume there exists $\theta^*_h\in\R^d$ such that
for all $(s, a)\in \mathcal{S} \times \Acal$,
\[
Q^*_h(s,a)= \theta^*_h \cdot \phi(s,a).
\] 
\end{assumption}
The hope is that this assumption may permit a sample complexity that
is polynomial in $d$ and $H$, with no explicit $|\Scal|$ or $|\Acal|$ dependence.

\iffalse
This assumption is widely used in existing reinforcement learning and
contextual bandit
literature~\citep{du2019provably,foster2020beyond}. However, even for
linear function approximation, realizability alone is not sufficient
for sample-efficient reinforcement learning
~\citep{weisz2020exponential}. In this work, we also impose the
regularity condition that $\Vert \theta^*_h\Vert_2=O(1)$ and
$\Vert\phi(s,a)\Vert_2=O(1)$, which can always be achieved via
rescaling.
\fi

Another assumption that we will use is that the minimum suboptimality
gap is lower bounded. 
\begin{assumption}[Constant Sub-optimality Gap]
\label{assumption:gap}
For any state $s\in \mathcal{S}$, $a\in\Acal$, the suboptimality
gap is defined as $\Delta_h(s,a):=V_h^*(s)-Q_h^*(s,a)$. We assume that
\[
\min_{h\in
  [H],s\in\mathcal{S},a\in\Acal}\left\{\Delta_h(s,a):\Delta_h(s,a)>0\right\}\ge
\Delta_{\min}.
\] 
\end{assumption}
The hope is that with a ``large gap'', the identification of the
optimal policy itself (as opposed to just estimating its value
accurately) may be statistically easier, thus making the problem easier.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/lowermdp_new.pdf}
    \caption{\textbf{The Leaking Complete Graph Construction.} Illustration of a hard MDP. There are $m+1$ states in the MDP, where $f$ is an absorbing terminal state. Starting from any non-terminal state $\overline{a}$, regardless of the action, there is at least $\alpha=1/6$ probability that the next state will be $f$.}
    \label{fig:mdp}
\end{figure}

%\subsection{Linear $Q^\star$ is Statistically Hard}

We now present two hardness results.
We start with the case where we have access to a generative model.
  %Our goal is to prove the following theorem.

\begin{theorem}\label{thm:lower_gen}
(Linear $Q^\star$; Generative Model Case) 
Consider any algorithm $\mathcal{A}$ which has access to a generative
model and which takes as input the feature mapping $\phi : \mathcal{S}
\times \mathcal{A} \to \mathbb{R}^d$. 
There exists an MDP with a feature mapping $\phi$ satisfying
Assumption~\ref{assumption:realizability} and where the size of the
action space is
$|\Acal| = c_1 \lceil \min\{d^{1/4},H^{1/2} \}\rceil$
such that if $\mathcal{A}$ (when given $\phi$ as input) finds
a policy $\pi$ such that
%$\min\{\Omega(|\mathcal{A}|), 2^{\Omega(d)}, 2^{\Omega(H)}\}$ samples to find a policy $\pi$ with
\begin{equation*}
   \E_{s_1\sim\mu}V^{\pi}(s_1)\ge \E_{s_1\sim\mu}V^*(s_1)-0.05
\end{equation*}
with probability $0.1$, then $\mathcal{A}$ 
requires $\min\{2^{c_2 d}, 2^{c_2 H}\}$ samples ($c_1$ and $c_2$ are absolute constants).
\end{theorem}

Note that theorem above uses an MDP whose size of the action space is
only of moderate size (actually sublinear in both $d$ and $H$). Of
course, in order to prove such a result, we must rely on a state
space which is exponential in $d$ or $H$ (else, we could apply a tabular
algorithm to obtain a polynomial result). The implications of the
above show that the linear $Q^\star$ assumption, alone, is not
sufficient for sample efficient RL, even with access to a generative model.

We now see that in the (online) episodic setting, even if we have a constant gap in the
MDP, the hardness of learning is still unavoidable.

\begin{theorem}\label{thm:lower_episodic}
(Linear $Q^\star$ Realizability + Gap; Episodic Setting) Consider any algorithm $\mathcal{A}$ which has access to the episodic sampling
model and which takes as input the feature mapping $\phi : \mathcal{S}
\times \mathcal{A} \to \mathbb{R}^d$. 
There exists an MDP with a feature mapping $\phi$ satisfying Assumption~\ref{assumption:realizability} 
and Assumption~\ref{assumption:gap} (where $\Delta_{\min}$ is an
absolute constant)
such that if $\mathcal{A}$ (using $\phi$ as input) finds
a policy $\pi$ such that
\begin{equation*}
   \E_{s_1\sim\mu}V^{\pi}(s_1)\ge \E_{s_1\sim\mu}V^*(s_1)-0.05
\end{equation*}
with probability $0.1$, then $\mathcal{A}$ 
requires $\min\{2^{c d}, 2^{c H}\}$ samples (where $c$ is an absolute constant).
\end{theorem}

The theorem above also holds with an MDP
whose action space is of size $|\Acal| = \Omega( \lceil
\min\{d^{1/4},H^{1/2} \}\rceil)$. However, for ease of exposition, we
will present a proof of this theorem that uses a number of actions
that is exponential in $d$ (see Section~\ref{ch3:bib}).

\paragraph{Linear $Q^\star$ Realizability + Gap; Generative Model
  Case.} For this case, it is not difficult to see that, with a
generative model, it is possible to \emph{exactly} learn an optimal policy
with a number of samples that is $\textrm{poly}(d,H,1/\Delta_{\min})$ 
(see Section~\ref{ch3:bib}). The key idea is that, at the last stage, the large gap
permits us to learn the optimal policy itself (using linear regression). Furthermore, if we have
an optimal policy from timestep $h$ onwards, then we are able to
obtain unbiased estimates of $Q^\star_{h-1}(s,a)$, at \emph{any} state
action pair, using the generative model. We leave the proof as an
exercise to the interested reader.

\subsubsection*{A Hard Instance with Constant Suboptimality Gap}
\label{sec:linearQstar_proof}

We now prove Theorem~\ref{thm:lower_episodic}, and we do not prove
Theorem~\ref{thm:lower_gen} (see Section~\ref{ch3:bib}).
The remainder of this section provides the construction of a hard
family of MDPs where $Q^*$ is linearly realizable and has constant
suboptimality gap and where it takes exponential samples to learn a
near-optimal policy.  Each of these hard MDPs can roughly
be seen as a ``leaking complete graph'' (see
Fig.~\ref{fig:mdp}). Information about the optimal policy can only be
gained by: (1) taking the optimal action; (2) reaching a non-terminal
state at level $H$. We will show that when there are exponentially
many actions, either events happen with negligible probability unless
exponentially many trajectories are played.

Let $m$ be an integer to be determined. The state space is
$\{\bar{1},\cdots,\bar{m},f\}$. The special state $f$ is called the
\emph{terminal state}.
The action space is simply $\Acal = \{1, 2, \ldots, m\}$.
Each MDP in this family is specified by an index $a^*\in \{1, 2, \ldots, m\}$ and
denoted by $\mathcal{M}_{a^*}$. In other words, there are $m$ MDPs in
this family. 
We will make use of a (large) set of approximately orthogonal vectors, which exists by the
Johnson-Lindenstrauss lemma. We state the following lemma without proof: 
\begin{lemma}[Johnson-Lindenstrauss]
\label{lemma:jl}
For any $\alpha>0$, if $m\le \exp(\frac{1}{8}\alpha^2d')$, there
exists $m$ unit vectors $\{v_1,\cdots,v_m\}$ in $\R^{d'}$ such that
$\forall i,j\in \{1, 2, \ldots, m\}$ such that $i\neq j$,
$|\langle v_i,v_j\rangle|\le \alpha$.
\end{lemma}
We will set $\alpha=\frac{1}{6}$ and
$m=\lfloor \exp(\frac{1}{8}\alpha^2d)\rfloor$. By
Lemma~\ref{lemma:jl}, we can find such a set of $d$-dimensional unit
vectors $\{v_1,\cdots,v_m\}$. For the clarity of presentation, we will
use $v_i$ and $v(i)$ interchangeably. The construction of
$\mathcal{M}_{a^*}$ is specified below.  Note that in this
construction, the features, the rewards and the transitions are
defined for all $\overline{a_1},a_2$ with $a_1,a_2\in \{1, 2, \ldots, m\}$ and
$a_1 \neq a_2$.  In particular, this construction is properly defined
even when $a_1 = a^*$.

\paragraph{Features.} The feature map, which maps state-action pairs to $d + 1$ dimensional vectors, is defined as follows. 
\begin{align*}
     \phi(\overline{a_1},a_2) &:= \left(0,\left(\Big\langle v(a_1),v(a_2)\Big\rangle+2\alpha\right)\cdot v(a_2)\right), \tag*{($\forall a_1,a_2\in \{1, 2, \ldots, m\}, a_1\neq a_2$)}\\
   \phi(\overline{a_1},a_1) &:= \left(\frac{3}{4}\alpha,\mathbf{0}\right), \tag*{($\forall a_1\in \{1, 2, \ldots, m\}$)} \\
   \phi(f,1) &= \left(0,\mathbf{0}\right),\\
   \phi(f,a) &:= \left(-1,\mathbf{0}\right).\tag*{($\forall a\neq 1$)}
\end{align*}
	Here $\mathbf{0}$ is the zero vector in $\mathbb{R}^d$. 
    Note that the feature map is independent of $a^*$ and is shared across the MDP family. 
    %For $\mathcal{M}_{a^*}$, define $\theta^*:=v(a^*)$.%For the MDP specified by $a^*$, define $\theta^*:=v(a^*)$.
    
    
    
\paragraph{Rewards.} For $h\in [H]$, the rewards are defined as
    \begin{align*}
        r_h(\overline{a_1},a^*) &:= \Big\langle v(a_1), v(a^*)\Big\rangle+2\alpha,\tag*{($a_1\neq a^*$)}\\
        r_h(\overline{a_1},a_2) &:= -2\alpha\left[\Big\langle v(a_1),v(a_2)\Big\rangle+2\alpha\right], \tag*{($a_2\neq a^*$, $a_2\neq a_1$)}\\
        r_h(\overline{a_1},a_1) &:= \frac{3}{4}\alpha,\tag*{($\forall a_1$)}\\%\langle v(1),v(a^*)\rangle-\alpha, \tag*{($\forall a_1$)}\\
        r_h(f,1) &:= 0, \\
        r_h(f,a) &:= -1. \tag*{($a\neq 1$)}
    \end{align*}
    For $h=H-1$, $r_{H-1}(s,a):=\langle \phi(s,a),  (1,  v(a^*))\rangle$ for every state-action pair.

\paragraph{Transitions.} The initial state distribution $\mu$ is set as a uniform distribution over $\{\bar{1},\cdots,\bar{m}\}$. The transition probabilities are set as follows.
\begin{align*}
\Pr[f|\overline{a_1},a^*] &= 1,\\
\Pr[f|\overline{a_1},a_1] &= 1,\\
    %\Pr[f|\overline{a_1},a_1] &=1,    \\
    \Pr[\cdot|\overline{a_1},a_2] &= \begin{cases}
    \overline{a_2}:\Big\langle v(a_1), v(a_2)\Big\rangle+2\alpha \\
    f:1-\Big\langle v(a_1), v(a_2)\Big\rangle-2\alpha
    \end{cases},\tag*{($a_2\neq a^*$, $a_2\neq a_1$)}\\
\Pr[f|f,\cdot] &= 1.
\end{align*}

After taking action $a_2$, the next state is either $\overline{a_2}$ or $f$. Thus this MDP looks roughly like a ``leaking complete graph'' (see Fig.~\ref{fig:mdp}): starting from state $\overline{a}$, it is possible to visit any other state (except for $\overline{a^*}$); however, there is always at least $1-3\alpha$ probability of going to the terminal state $f$. The transition probabilities are indeed valid, because
$$0<\alpha\le \Big\langle v(a_1), v(a_2)\Big\rangle+2\alpha\le 3\alpha<1.$$

We now verify that realizability,
i.e. Assumption~\ref{assumption:realizability}, is satisfied. 
\begin{lemma}\label{lem:realizability}
(Linear realizability) In the MDP $\mathcal{M}_{a^*}$, $\forall h\in
[H]$, for any state-action pair $(s,a)$, $Q^*_h(s,a) = \langle
\phi(s,a),\theta^*\rangle$ with $\theta^* = (1,  v(a^*))$.  
\end{lemma}
\begin{proof}
\iffalse
We first verify the statement for the terminal state $f$. Recall that
$\theta^*=(1,v(a^*))$. Thus $\langle \phi(f,1),\theta^*\rangle=0$,
while for $a\neq 1$, $\langle \phi(f,a),\theta^*\rangle=-1$. We now
show that for any $h$, $Q^*_h(f,1)=0$ while for $a\neq 1$,
$Q^*_h(f,a)=-1$. First, note that when $h=H-1$,
$Q^*_{H-1}(f,a)=r_{H-1}(f,a)=-I[a=1]$ by definition. Thus
$V^*_{H-1}(f)=0$. Since regardless of the action taken at terminal
state $f$, the next state will still be $f$, for any $h<H-1$,
$Q^*_h(f,a)=R_h(f,a)+V^*_{h+1}(f)=-I[a=1]+V^*_{h+1}(f)$. Hence
$V^*_h(f)=\max_a Q^*_h(f,a)=V^*_{h+1}(f)=0$. It then follows that
$\forall h$, $$Q^*_h(f,a)=-I[a=1]=\langle \phi(f,a),\theta^*\rangle.$$ 
\fi
We first verify the statement for the terminal state $f$. Observe that at the terminal state $f$, the next state is always $f$ and the reward is either $0$ (if action $1$ is chosen) or $-1$ (if an action other than $1$ is chosen). 
Hence, we have 
\[
Q_h^*(f, a) = \begin{cases}
0 & a  = 1\\
-1 & a \neq 1
\end{cases}
\]
and
\[
V_h^*(f)=0.
\]
This implies $Q_h^*(f,\cdot)=\langle\phi(f,\cdot),(1,  v(a^*)) \rangle $.

We now verify realizability for other states via induction on $h=H-1,\cdots,0$. The induction hypothesis is that for all $a_1, a_2 \in \{1, 2, \ldots, m\}$, we have 
\begin{equation}\label{eq:qstar}
    Q_h^*(\overline{a_1}, {a_2}) = \begin{cases}
    	\left(\left\langle v(a_1),v(a_2)\right\rangle+2\alpha\right)\cdot \left\langle v(a_2), v(a^*)\right\rangle & a_1 \neq a_2\\
	\frac{3}{4}\alpha  & a_1 = a_2
   \end{cases}
   \end{equation}
and
\begin{equation}\label{eq:vstar}
V_h^*(\overline{a_1}) = \begin{cases}
    	\left\langle v(a_1), v(a^*) \right\rangle + 2\alpha & a_1 \neq a^* \\
	\frac{3}{4}\alpha  & a_1 = a^*
   \end{cases}.
   \end{equation}

Note that (\ref{eq:qstar}) implies that realizability is satisfied. In the remaining part of the proof we verify Eq. (\ref{eq:qstar}) and (\ref{eq:vstar}). 

When $h=H-1$, (\ref{eq:qstar}) holds by the definition of rewards. Next, note that for all $h \in [H]$, (\ref{eq:vstar}) follows from (\ref{eq:qstar}). This is because for all $a_1\neq a^*$, for all $a_2\notin \{ a_1, a^*\}$.
\begin{align*}
    Q_h^*(\overline{a_1},{a_2}) &= \left(\left\langle v(a_1),v(a_2)\right\rangle+2\alpha\right)\cdot \left\langle v(a_2), v(a^*)\right\rangle\le 3\alpha^2,
\end{align*}
Moreover, for all $a_1\neq a^*$,
\begin{align*}
    Q_h^*(\overline{a_1},{a_1}) &= \frac{3}{4}\alpha < \alpha.
\end{align*}
Furthermore, for all $a_1\neq a^*$,
\begin{align*}
    Q_h^*(\overline{a_1},{a^*}) = \left\langle v(a_1), v(a^*) \right\rangle + 2\alpha \ge \alpha > 3\alpha^2.
\end{align*}
In other words, (\ref{eq:qstar}) implies that $a^*$ is always the optimal action for all state $\overline{a_1}$ with $a_1 \neq a^*$. 
Now, for state $\overline{a^*}$, for all $a \neq a^*$, we have
\begin{align*}
    Q_h^*(\overline{a^*}, a) &= \left(\left\langle v(a^*),v(a)\right\rangle+2\alpha\right)\cdot \left\langle v(a^*), v(a)\right\rangle\le 3\alpha^2 < \frac{3}{4}\alpha = Q_h^*(\overline{a^*}, a^*).
\end{align*}
Hence, (\ref{eq:qstar}) implies that $a^*$ is always the optimal action for all states $\overline{a}$ with $a \in \{1, 2, \ldots, m\}$.

Thus, it remains to show that (\ref{eq:qstar}) holds for $h$ assuming (\ref{eq:vstar}) holds for $h+1$. 
Here we only consider the case that $a_2 \neq a_1$ and $a_2 \neq a^*$, since otherwise $\Pr[f|\overline{a_1},a_2] = 1$ and thus (\ref{eq:qstar}) holds by the definition of the rewards and the fact that $V_h^*(f) = 0$. When $a_2 \notin \{a_1, a^*\}$, we have
\begin{align*}
    Q_h^*(\overline{a_1},{a_2}) &= r_h(\overline{a_1},a_2) + \E_{s_{h+1}}\left[\left.V_{h+1}^*(s_{h+1})\right|\overline{a_1},a_2\right]\\
    &=-2\alpha\left[\Big\langle v(a_1),v(a_2)\Big\rangle+2\alpha\right] + \Pr[s_{h+1}=\overline{a_2}]\cdot V_{h+1}^*(\overline{a_2})+\Pr[s_{h+1}=f]\cdot V_{h+1}^*(f)\\
    &=-2\alpha\left[\Big\langle v(a_1),v(a_2)\Big\rangle+2\alpha\right] + \left[\Big\langle v(a_1),v(a_2)\Big\rangle+2\alpha\right]\cdot \left( \Big\langle v(a_2), v(a^*) \Big\rangle + 2\alpha\right)\\
    &=\left(\Big\langle v(a_1),v(a_2)\Big\rangle+2\alpha\right)\cdot  \Big\langle v(a_2), v(a^*) \Big\rangle.
\end{align*}
This is exactly (\ref{eq:qstar}) for $h$. Hence both (\ref{eq:qstar}) and (\ref{eq:vstar}) hold for all $h\in [H]$.
\end{proof}

We now verify the constant sub-optimality gap,
i.e. Assumption~\ref{assumption:gap}, is satisfied. 
\begin{lemma}\label{lem:realizability}
(Constant Gap) Assumption~\ref{assumption:gap} is satisfied with $\Delta_{\min}=1/24$.
\end{lemma}
\begin{proof}
From Eq. (\ref{eq:qstar}) and (\ref{eq:vstar}), it is easy to see that at state $\overline{a_1}\neq \overline{a^*}$, for $a_2\neq a^*$, the suboptimality gap is
\begin{align*}
    \Delta_h(\overline{a_1},a_2) := V_h^*(\overline{a_1}) - Q^*_h(\overline{a_1}, a_2) \ge \alpha - \max\left\{ 3\alpha^2,\frac{3}{4}\alpha\right\} = \frac{1}{24}.
\end{align*}
Moreover, at state $\overline{a^*}$, for $a\neq a^*$, the suboptimality gap is
\begin{align*}
    \Delta_h(\overline{a^*}, a) := V_h^*(\overline{a^*}) - Q^*_h(\overline{a^*}, a) \ge \frac{3}{4}\alpha - 3\alpha^2= \frac{1}{24}.
\end{align*}
Finally, for the terminal state $f$, the suboptimality gap is
obviously $1$. Therefore $\Delta_{\min}\ge \frac{1}{24}$ for all MDPs
under consideration.  
\end{proof}

\subsubsection*{The Information-Theoretic Argument}

We provide a proof sketch for the lower bound below in Theorem~\ref{thm:lower_episodic}.
\paragraph{Proof sketch.} 
We will show that for any algorithm, there exists $a^*\in \{1, 2, \ldots, m\}$ such that in order to output $\pi$ with
\begin{equation*}
   \E_{s_1\sim\mu}V^{\pi}(s_1)\ge \E_{s_1\sim\mu}V^*(s_1)-0.05
\end{equation*}
with probability at least $0.1$ for $\mathcal{M}_{a^*}$, the number of
samples required is $2^{\Omega(\min\{d,H\})}$.

Observe that the feature map of $\mathcal{M}_{a^*}$ does not depend on $a^*$, and that for $h<H-1$ and $a_2\neq a^*$, the reward $R_h(\overline{a_1},a_2)$ also contains no information about $a^*$. The transition probabilities are also independent of $a^*$, unless the action $a^*$ is taken. Moreover, the reward at state $f$ is always $0$. Thus, to receive information about $a^*$, the agent either needs to take the action $a^*$, or be at a non-game-over state at the final time step ($h=H-1$).

However, note that the probability of remaining at a non-terminal state at the next layer is at most
$$\sup_{a_1\neq a_2}\langle v(a_1), v(a_2)\rangle+2\alpha \le 3\alpha\le \frac{3}{4}.$$
Thus for any algorithm, $\Pr[s_{H-1}\neq f]\le \left(\frac{3}{4}\right)^H$, which is exponentially small.

In other words, any algorithm that does not know $a^*$ either needs to ``be lucky'' so that $s_{H-1}=f$, or needs to take $a^*$ ``by accident''. Since the number of actions is $m=2^{\Theta(d)}$, either event cannot happen with constant probability unless the number of episodes is exponential in $\min\{d,H\}$. %We now formalize this intuition to prove the theorem. 

In order to make this claim rigorous, we can construct a reference MDP $\mathcal{M}_0$ as follows. The state space, action space, and features of $\mathcal{M}_0$ are the same as those of $\mathcal{M}_a$. The transitions are defined as follows:
    \begin{align*}
        \Pr[\cdot|\overline{a_1},a_2] &= \begin{cases}
        \overline{a_2}:\Big\langle v(a_1), v(a_2)\Big\rangle+2\alpha \\
        f:1-\Big\langle v(a_1), v(a_2)\Big\rangle-2\alpha
        \end{cases},\tag*{($\forall a_1, a_2$ s.t. $a_1\neq a_2$)}\\
    \Pr[f|f,\cdot] &= 1.
    \end{align*}
The rewards are defined as follows:
\begin{align*}
     r_h(\overline{a_1},a_2) &:= -2\alpha\left[\Big\langle v(a_1),v(a_2)\Big\rangle+2\alpha\right], \tag*{( $\forall a_1, a_2$ s.t. $a_1\neq a_2$)}\\
        r_h(f,\cdot) &:= 0.
\end{align*}
Note that $\mathcal{M}_0$ is identical to $\mathcal{M}_{a^*}$, except
when $a^*$ is taken, or when an trajectory ends at a non-terminal
state. Since the latter event happens with an exponentially small
probability, we can show that for any algorithm the probability of
taking $a^*$ in $\mathcal{M}_{a^*}$ is close to the probability of
taking $a^*$ in $\mathcal{M}_0$. Since $\mathcal{M}_0$ is independent
of $a^*$, unless an exponential number of samples are used, for any
algorithm there exists $a^*\in \{1, 2, \ldots, m\}$ such that the
probability of taking $a^*$ in $\mathcal{M}_0$ is $o(1)$. It then
follows that the probability of taking $a^*$ in $\mathcal{M}_{a^*}$ is
$o(1)$. Since $a^*$ is the optimal action for every state, such an
algorithm cannot output a near-optimal policy for $\mathcal{M}_{a^*}$.

\subsection{Linearly Realizable $\pi^\star$}
\label{sec:linearQstar_pi}

Similar to value-based learning, a natural assumption for policy-based
learning is that the optimal policy is realizable by a halfspace.

\begin{assumption}[Linear $\pi^*$ Realizability]
  \label{asmp:linear_policy_realizable}
For any $h \in [H]$, there exists $\theta_h^\star \in \mathbb{R}^d$ that
satisfies for any $s \in \Scal$, we have 
\[
\pi^*\left(s\right) \in \argmax_{a} \left \langle\theta_h,\phi\left(s,a\right)\right\rangle.
\]
\end{assumption}
As before (and as is natural in supervised learning), we will also
consider the case where there is a large margin (which is
analogous to previous large gap assumption). 
\begin{assumption}[Constant Margin]
  \label{asmp:linear_policy_margin}
Without loss of generality, assume the scalings are such that for all
$(s,a,h) \in \Scal \times \Acal \times [H]$,
$\norm{\phi(s,a)}_2\leq1$  and $\|\theta_h^\star\|\leq 1$ Now suppose
that for all $s \in  \Scal$ and any
$a \notin \pi^*(s)$, 
\[
\left\langle\theta_h,
          \phi\left(s,\pi^*(s)\right)\right\rangle -
        \left\langle\theta_h , \phi\left(s,a\right)\right\rangle \ge
        \Delta_{\min}.
\] 
\end{assumption}
Here we restrict the linear coefficients and features to have unit
norm for normalization. 

We state the following without proof (see Section~\ref{ch3:bib}):
\begin{theorem}\label{thm:lower_pi}
(Linear $\pi^\star$ Realizability + Margin) 
Consider any algorithm $\mathcal{A}$ which has access to a generative model and which takes as input the feature mapping $\phi : \mathcal{S}
\times \mathcal{A} \to \mathbb{R}^d$. 
There exists an MDP with a feature mapping $\phi$ satisfying
Assumption~\ref{asmp:linear_policy_realizable}
and Assumption~\ref {asmp:linear_policy_margin} (where $\Delta_{\min}$ is an
absolute constant)
such that if  $\mathcal{A}$ (with $\phi$ as input) finds
a policy $\pi$ such that
%$\min\{\Omega(|\mathcal{A}|), 2^{\Omega(d)}, 2^{\Omega(H)}\}$ samples to find a policy $\pi$ with
\begin{equation*}
   \E_{s_1\sim\mu}V^{\pi}(s_1)\ge \E_{s_1\sim\mu}V^*(s_1)-0.05
\end{equation*}
with probability $0.1$, then $\mathcal{A}$ 
requires $\min\{2^{c d}, 2^{c H}\}$ samples, where $c$ is an absolute constant.
\end{theorem}

\section{Discussion: Studying Generalization in RL}
\label{sec:how_to_study_RL}

The previous lower bounds shows that: (i) without further assumptions, agnostic
learning (in the standard supervised learning sense) is not possible
in RL, unless we can tolerate an exponential dependence on the
horizon, and (ii) simple realizability assumptions are also not
sufficient for sample efficient RL.

This motivates the study of RL to settings where we either make stronger assumptions
or have a means in which the agent can obtain side information. Three
examples of these approaches that are considered in this book are:
\begin{itemize}
\item Structural (and Modelling) Assumptions: By making stronger
  assumptions about the how the hypothesis class relates to the
  underlying MDP, we can move away from agnostic learning lower
  bound. We have seen one example of this with the stronger Bellman completeness assumption that we
  considered in Chapter~\ref{chap:Bellman_complete}. We will
  see more examples of this in Part 2. 
\item Distribution Dependent Results (and Distribution Shift): 
We have seen one example of this approach when we
  consider the approximate dynamic programming approach in
  Chapter~\ref{chap:api}.
When we
  move to policy gradient methods (in Part 3), we will also consider
  results which depend on the given distribution (i.e. where we suppose our
  starting state distribution $\mu$ has some reasonable notion of
  coverage). 
\item Imitation Learning and Behavior Cloning: here we will consider
  settings where the agent has input from, effectively, a teacher, and
  we see how this can circumvent statistical hardness results.
\end{itemize}



\section{Bibliographic Remarks and Further Readings}\label{ch3:bib}

The reduction from reinforcement learning to supervised learning was
first introduced in~\cite{NIPS1999_1664}, which used a different
algorithm (the ``trajectory tree'' algorithm), as opposed to the
importance sampling approach presented here. \cite{NIPS1999_1664} made
the connection to the VC dimension of the policy class. The fundamental
sample complexity tradeoff --- between polynomial dependence on the
size of the state space and exponential dependence on the horizon --- 
was discussed in depth in~\cite{kakade2003sample}.

\iffalse
Utilizing linear methods for dynamic programming goes back to, at
least, \cite{Shan50,bellman1959functional}.  Formally considering the
linear $Q^\star$ assumption goes back to at
least~\cite{wen2017efficient}.  Resolving the learnability under this
assumption was an important open question discussed in
\cite{du2019good}, which is now resolved.  Here,
Theorem~\ref{thm:lower_gen}, due to~\cite{WeiszAS21}, resolved this
question. Furthermore, Theorem~\ref{thm:lower_episodic}, due
to~\cite{Wang_linear_lower}, resolves this question in the online
setting with the additional assumption of having a constant sub-optimality
gap. Theorem~\ref{thm:lower_pi}, which assumed a linearly realizable
optimal policy, is due to~\cite{du2019good}. Theorem~\ref{thm:hard_det_1}, on
offline policy evaluation, is due to~\cite{wang2020statistical,zanette2021exponential}.
\fi

Utilizing linear methods for dynamic programming goes back to, at
least, \cite{Shan50,bellman1959functional}.  Formally considering the
linear $Q^\star$ assumption goes back to at
least~\cite{wen2017efficient}.  Resolving the learnability under this
assumption was an important open question discussed in
\cite{du2019good}, which is now resolved.  In the offline (policy
evaluation) setting, the lower bound in Theorem~\ref{thm:hard_det_1}
is due to~\cite{wang2020statistical,zanette2021exponential}.  With a
generative model, the breakthrough result of~\cite{WeiszAS21}
established the impossibility result with the linear $Q^\star$
assumption.  Furthermore, Theorem~\ref{thm:lower_episodic}, due
to~\cite{Wang_linear_lower}, resolves this question in the online
setting with the additional assumption of having a constant
sub-optimality gap.  Also,~\cite{weisz2021tensorplan} extends the
ideas from ~\cite{WeiszAS21}, so that the lower bound is applicable
with action spaces of polynomial size (in $d$ and $H$); this is the
result we use for Theorem~\ref{thm:lower_gen}.

Theorem~\ref{thm:lower_pi}, which assumed a linearly realizable
optimal policy, is due to~\cite{du2019good}. 



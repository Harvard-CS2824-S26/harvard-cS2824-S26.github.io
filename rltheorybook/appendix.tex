%\documentclass{article}

%\usepackage{times}

%\input{../macros}

\chapter{Concentration}



\begin{lemma}
  (Hoeffding's inequality)
Suppose $X_1, X_2, \ldots X_n$ are a sequence of independent,
identically distributed (i.i.d.) random variables with mean $\mu$. Let $\bar{X}_n=n^{-1} \sum_{i=1}^n
X_i$. Suppose that $X_i \in [b_{-},b_{+}]$ with probability $1$, then
\[
P( \bar{X}_n \geq \mu + \epsilon) \leq e^{-2n \epsilon^2/(b_{+}-b_{-})^2} .
\]
Similarly,
\[
P( \bar{X}_n \leq \mu - \epsilon) \leq e^{-2n \epsilon^2/(b_{+}-b_{-})^2} .
\]\label{lem:hoeffding}
\end{lemma}

The Chernoff bound implies that with probability $1-\delta$:
\[
  \bar{X}_n - E X \leq (b_{+}-b_{-})\sqrt{\ln (1/\delta)/(2n)}\, .
  \]

Now we introduce the definition of subGaussian random variables.

\begin{definition}\label{def:sub-gauss}[Sub-Gaussian Random Variable] A random variable $X$ is $\sigma$-subGaussian if for all $\lambda \in \mathbb{R}$, it holds that $\mathbb{E}\left[ \exp(\lambda X) \right] \leq \exp\left( \lambda^2 \sigma^2 / 2 \right)$.
\end{definition}
One can show that a Gaussian random variable with zero mean and standard deviation $\sigma$ is a $\sigma$-subGaussian random variable. 


The following theorem shows that the tails of a $\sigma$-subGaussian random variable decay approximately as fast as that of a Gaussian variable with zero mean and standard deviation $\sigma$. 
\begin{theorem} \label{them:sub_gaussian_property_1}
If $X$ is a $\sigma$-subGaussian, then for any $\epsilon > 0$, we have $\mathrm{Pr}\left( X \geq \epsilon \right) \leq \exp\left( - \frac{ \epsilon^2 }{2\sigma^2} \right)$.
\end{theorem}
The following lemma  shows that the sum of independent sub-Gaussian variables is still sub-Gaussian. 
\begin{lemma}\label{lemma:sub_gaussian_property_2}
Suppose that $X_1$ and $X_2$ are independent and $\sigma_1$ and $\sigma_2$ subGaussian, respectively. Then for any $c\in \mathbb{R}$, we have $cX$ being $|c|\sigma$-subGaussian. We also have $X_1 + X_2$ being $\sqrt{\sigma_1^2 + \sigma_2^2}$-subGaussian.
\end{lemma}





\begin{theorem}[Hoeffding-Azuma Inequality] \label{thm:Azuma}
Suppose $X_1, \dots, X_T$ is a martingale difference sequence where
each $X_t$ is a $\sigma_t$ sub-Gaussian, 
Then, for all $\epsilon > 0$ and all positive integer $N$,
\[
P\left(\sum_{i=1}^N X_i \ge \eps\right) \le \exp
\left(\frac{-\eps^2}{2\sum_{i=1}^N \sigma_i^2}\right).
\]
\end{theorem}


\begin{lemma}
(Bernstein's inequality)  Suppose $X_1,\ldots, X_n$ are independent random variables.
Let $\bar{X}_n= n^{-1} \sum_{i=1}^n X_i$, $\mu= \E \bar{X}_n$, and
$\textrm{\textrm{Var}}(X_i)$ denote the variance of $X_i$.
If $X_i- E X_i \leq b$ for all $i$, then
\[
P( \bar{X}_n \geq \mu + \epsilon) \leq \exp \left[- \frac{n^2 \epsilon^2}{2\sum_{i=1}^n \textrm{\textrm{Var}}(X_i) + 2 n b\epsilon/3} \right] .
\]
\end{lemma}

If all the variances are equal, the Bernstein inequality implies that, with probability at least $1-\delta$,
\[
\bar{X}_n - E X \leq \sqrt{2 \textrm{Var}(X) \ln(1/\delta)/n} + \frac{2 b \ln (1/\delta)}{3n}  .
\]


\begin{lemma}[Bernstein's Inequality for Martingales]
Suppose $X_1,X_2\dots$ is a martingale difference sequence where $|X_i| \leq M \in \mathbb{R}^+$ almost surely. Then for all positive $t$ and $ n \in \mathbb{N}^+$, we have:
\begin{align*}
P\left( \sum_{i=1}^n X_i \geq t \right) \leq \exp\left( - \frac{t^2 / 2 }{  \sum_{i=1}^n \EE\left[ X^2_i | X_{<i}\right]  + M t / 3 }  \right).
\end{align*}
\end{lemma}



The following concentration bound is a simple application of the
McDiarmid's inequality \citep{McD89} (e.g. see ~\citep{HsuSpectral}
for proof).


\begin{proposition} \label{app:discrete}
(Concentration for Discrete Distributions)
Let $z$ be a discrete random variable that takes values in
$\{1,\ldots,d\}$, distributed according to $q$. We write $q$ as a
vector where $\vec{q}=[\Pr(z=j)]_{j=1}^d$.
% We are interested in estimating the vector
%$\vec{q}=[\Pr(z=j)]_{j=1}^d$ from $N$ i.i.d.~copies $z_i$ of $z$ ($i=1,\ldo
%ts,N$). 
Assume we have $N$ iid samples, and that our empirical estimate of
$\vec{q}$ is $[\widehat{q}]_j=\sum_{i=1}^N \mathbf{1}[z_i=j]/
N$.

We have that $\forall \epsilon>0$:
  \[
  \Pr\left(\left\|\widehat{q} -  \vec{q}\right\|_2 \geq 
    1/\sqrt{N} + \epsilon \right)
  \leq e^{-N \epsilon^2} .
  \]
which implies that:
  \[
  \Pr\big(\left\|\widehat{q} -  \vec{q}\right\|_1 \geq 
    \sqrt{d}(1/\sqrt{N} + \epsilon) \big)
  \leq e^{-N \epsilon^2} .
  \]
\end{proposition}



\begin{lemma}[Self-Normalized Bound for Vector-Valued Martingales;
  \citep{abbasi2011improved}] \label{lemma:self_norm}
Let $\{\varepsilon_i\}_{i=1}^{\infty}$ be a real-valued stochastic
process with corresponding filtration $\{\mathcal{F}_{i}\}_{i=1}^{\infty}$
such that $\varepsilon_i$ is $\mathcal{F}_i$ measurable, $\E[
\varepsilon_i | \mathcal{F}_{i-1} ] = 0$, and $\varepsilon_i$ is
conditionally $\sigma$-sub-Gaussian with $\sigma\in\mathbb{R}^+$. Let
$\{X_i\}_{i=1}^{\infty}$ be a stochastic process with $X_i\in
\mathcal{H}$ (some Hilbert space) and $X_i$ being $\mathcal{F}_t$ measurable. Assume that a
linear operator $\Sigma:\mathcal{H}\to\mathcal{H}$ is positive definite,
i.e., $x^{\top} \Sigma x > 0$ for any $x\in\mathcal{H}$. For any $t$,
define the linear operator $\Sigma_t = \Sigma_0 + \sum_{i=1}^{t} X_iX_i^{\top}$ (here
$xx^{\top}$ denotes outer-product in $\mathcal{H}$). With probability
at least $1-\delta$, we have for all $t\geq 1$: 
\begin{align*}
\left\|  \sum_{i=1}^{t} X_i \varepsilon_i \right\|^2_{\Sigma_t^{-1}} \leq \sigma^2 \log\left(\frac{ \det( \Sigma_t) \det(\Sigma_0)^{-1} }{ \delta^2  }\right).
\end{align*}
\end{lemma}

\begin{theorem}[OLS with a fixed design] \label{thm:ols_fixed_design} Consider a fixed dataset $\Dcal = \{x_i, y_i\}_{i=1}^N$ where $x_i\in\mathbb{R}^d$, and $y_i = (\theta^\star)^{\top} x_i + \epsilon_i$,  where $\{\epsilon_i\}_{i=1}^N $ are independent $\sigma$-sub-Gaussian random variables, and $\EE[\epsilon_i] = 0$. Denote $\Lambda  = \sum_{i=1}^N x_i x_i^{\top}$, and we further assume that $\Lambda$ is full rank such that $\Lambda^{-1}$ exists.  Denote $\hat\theta$ as the least squares solution, i.e., $\hat\theta = \Lambda^{-1} \sum_{i=1}^N x_i y_i$. Pick $\delta \in (0,1)$, with probability at least $1-\delta$, we have:
\begin{align*}
\left\| \hat\theta - \theta^\star \right\|^2_{\Lambda} \leq { \sigma^2 ( d + \sqrt{d \ln(1/\delta)} + 2 \ln(1/\delta)) }.
\end{align*}
\end{theorem}
The proof of the above theorem is standard (e.g. the proof can be
easily adapted from Lemma 6 in \cite{Hsu2012RandomDA}).


\begin{lemma}[Least Squares Generalization Bound] \label{lem:least_square_gen}Given a dataset $\Dcal = \{x_i, y_i\}_{i=1}^n$ where $x_i\in\Xcal$ and $x_i,y_i \sim \nu$, $y_i = f^\star(x_i) + \epsilon_i$, where $|y_i| \leq Y, \max_x |f^\star(x) |\leq Y$ and $ |\epsilon_i | \leq \sigma$ for all $i$, and $\{\epsilon_i\}$ are independent from each other. Given a function class $\Fcal: \Xcal\mapsto [0, Y]$, we assume approximate realizable, i.e., $\min_{f\in\Fcal} \EE_{x\sim \nu} \lvert f^\star(x)- f(x) \rvert^2 \leq \epsilon_{approx}$. Denote $\hat{f}$ as the least square solution, i.e., $\hat{f} = \argmin_{f\in\Fcal} \sum_{i=1}^n \left( f(x_i) - y_i \right)^2$. With probability at least $1-\delta$, we have:
\begin{align*}
\EE_{x\sim \nu}\left( \hat{f}(x) - f^\star(x) \right)^2 \leq  \frac{22 Y^2 \ln(|\Fcal |/\delta)}{ n } + 20 \epsilon_{approx}.
\end{align*}
\end{lemma}

\begin{proof}%[Proof of Lemma~\ref{lem:least_square_gen}]

For notation simplicity, let us denote $f^\star(x) := \EE[y | x]$. Let us consider a fixed function $f\in\Fcal$. 

Denote random variables $z_i^f := ( f(x_i) - y_i )^2 - ( f^\star(x_i) - y_i )^2$. We now show that $\EE[z_i^f] = \| f - f^\star \|^2_{2,v}$, and $\EE[(z_i^f)^2] \leq 4 Y^2 \|f - f^\star\|_{2,\nu}^2$:
\begin{align*}
\EE[ z_i^f  ] = \EE\left[ ( f(x_i) - f^\star(x_i)) (f(x_i) + f^\star(x_i) - 2y_i) \right] = \EE[ (f(x_i) - f^\star(x_i))^2 ] := \| f- f^\star \|_{2,\nu}^2
\end{align*} where the last equality uses the fact that $\EE[ y_i | x_i] = f^\star(x_i)$. For the second moment, we have:
\begin{align*}
\EE[ (z_i^f)^2] = \EE\left[ ( f(x_i) - f^\star(x_i))^2 (f(x_i) + f^\star(x_i) - 2y_i)^2 \right]  \leq 4Y^2 \EE[ (f(x_i) - f^\star(x_i))^2 ] := 4Y^2 \| f - f^\star \|_{2,\nu}^2,
\end{align*} where the inequality uses the conditions that $\|f^\star\|_{\infty} \leq Y, \|f\|_{\infty}\leq Y, | y_i|\leq Y$. Now we can bound the deviation from the empirical mean $\sum_{i=1}^n  z_i^f / n$ to its expectation $\|f - f^\star\|_{2,\nu}^2$ using Bernstein's inequality. Together with a union bound over all $f\in\Fcal$, with probability at least $1-\delta$, we have
\begin{align}
\forall f\in\Fcal: \| f -f^\star\|_{2,\nu}^2 - \sum_{i=1}^n z_i^f /n \leq \sqrt{ \frac{ 8 Y^2 \| f-f^\star \|_{2,\nu}^2\ln(|\Fcal|/\delta)  }{n}  } + \frac{4Y^2 \ln(|\Fcal|/\delta)}{n}.
\label{eq:uniform_convergence_ls}
\end{align}
Below we first bound $\sum_{i=1}^n z_i^{\hat f} / n$ where recall $\hat{f}$ is the least squares solution.

Denote $\tilde{f} := \argmin_{f\in\Fcal} \| f -  f^\star \|_{2,\nu}^2$. Note that $\tilde{f}$ is independent of the dataset by its definition.   Our previous calculation implies that $\EE[z_i^{\tilde{f}} ] = \| \tilde{f} - f^\star \|_{2,\nu}^2$, and $\EE[ (z_i^{\tilde{f}})^2 ] \leq 4Y^2 \|\tilde{f} - f^\star \|_{2,\nu}^2$. Directly applying Bernstein's inequality to bound $\sum_{i=1}^n z_i^{\tilde{f}} / n  - \| \tilde{f} - f^\star \|_{2,\nu}^2$, we have that with probability at least $1-\delta$:
\begin{align*}
\sum_{i=1}^n z_i^{\tilde f} / n - \|\tilde{f} - f^\star \|_{2,\nu}^2 \leq \sqrt{ \frac{8 Y^2 \| \tilde{f} - f^\star \|_{2,\nu}^2 \ln(1/\delta)}{n}} + \frac{4Y^2 \ln(1/\delta)}{3n}.
\end{align*}
Using this inequality, we discuss two cases.  First when $\sum_{i=1}^n z_i^{\tilde f} / n \geq Y^2 \ln(1/\delta) / n$. In this case, the above inequality implies that:
\begin{align*}
\sum_{i=1}^n z_i^{\tilde f} / n - \|\tilde{f} - f^\star \|_{2,\nu}^2 \leq \sqrt{  8  \| \tilde{f} - f^\star \|_{2,\nu}^2 \cdot \sum_{i=1}^n z_i^{\tilde f} / n}  + \frac{4Y^2 \ln(1/\delta)}{3n}.
\end{align*} Solve for $\sum_{i=1}^n z_i^{\tilde f} / n$, we get:
\begin{align*}
\sum_{i=1}^n z_i^{\tilde f} / n \leq 10 \| \tilde{f} -f^\star \|_{2,\nu}^2 + \frac{8 Y^2 \ln(1/\delta)}{3n}.
\end{align*}

For the other case where $\sum_{i=1}^n z_i^{\tilde f} / n \leq Y^2 \ln(1/\delta) / n$, we immediately have:
\begin{align*}
\sum_{i=1}^n z_i^{\tilde f} / n \leq Y^2 \ln(1/\delta) / n \leq 10 \| \tilde{f} -f^\star \|_{2,\nu}^2 + \frac{8 Y^2 \ln(1/\delta)}{3n}.
\end{align*}Thus $\sum_{i=1}^n z_i^{\tilde f} / n \leq  10 \| \tilde{f} -f^\star \|_{2,\nu}^2 + \frac{8 Y^2 \ln(1/\delta)}{3n}$ regardless.

%Under this inequality, when $\sum_{i=1}^n z_i^{\tilde f} / n \geq 4Y^2 \ln(1/\delta) / n$, we can show that the above inequality implies that:
%\begin{align*}
%\sum_{i=1}^n z_i^{\tilde f} / n \leq \frac{ \| \tilde{f} -f^\star \|_{2,\nu}^2 }{  2} + \frac{10 Y^2 \ln(1/\delta)}{ 3n}.
%\end{align*} 
Since $\|\tilde{f} - f^\star \|_{2,\nu}^2 \leq \epsilon_{approx}$ by the assumption, we can conclude that:
\begin{align*}
\sum_{i=1}^n z_i^{\tilde f} / n \leq {10 \epsilon_{approx} } + \frac{8 Y^2 \ln(1/\delta)}{ 3n}.
\end{align*}
Note that by definition of $\hat{f}$, we have $\sum_{i=1}^n z_i^{\hat f} \leq \sum_{i=1}^n z_i^{\tilde f}$, which implies that:
\begin{align*}
\sum_{i=1}^n z_i^{\hat f} / n \leq {10 \epsilon_{approx} } + \frac{8 Y^2 \ln(1/\delta)}{ 3n},
\end{align*} which concludes the upper bound of $\sum_{i=1}^n z_i^{\hat f} / n$.

Going back to Eq.~\ref{eq:uniform_convergence_ls}, by setting $f =\hat{f}$ there, and use the upper bound on $\sum_{i=1}^n z_i^{\hat f} /n$, we have that:
\begin{align*}
\| \hat{f} -f^\star\|_{2,\nu}^2 \leq \sqrt{ \frac{ 8 Y^2 \| \hat{f}-f^\star \|_{2,\nu}^2\ln(|\Fcal|/\delta)  }{n}  } + \frac{4Y^2 \ln(|\Fcal|/\delta)}{n} + \frac{8 Y^2 \ln( |\Fcal| / \delta)}{3n} +10 \epsilon_{approx}.
\end{align*}
Solve for $\|\hat{f} - f^\star\|_{2,\nu}^2$ from the above inequality, we get:
\begin{align*}
\| \hat{f} -f^\star\|_{2,\nu}^2 \leq \frac{22 Y^2 \ln(|\Fcal |/\delta)}{ n } + 20 \epsilon_{approx}.
\end{align*}
This concludes the proof. 
\end{proof}

\begin{lemma}[Uniform convergence for Bellman error under linear function hypothesis class ]  \label{lem:b_error_linear_uniform_conv}
Given a feature $\phi:\Scal\times\Acal\mapsto \mathbb{R}^d$ with $\| \phi(s,a) \|_2 \leq 1, \forall s,a$. Define $\Hcal_h = \{ w^{\top} \phi(s,a) : \|w\|_2 \leq W, w\in\mathbb{R}^d, \|w^{\top}\phi(\cdot,\cdot)\|_{\infty} \leq H \}$. Given $g\in\Hcal$, denote $\ell(s,a,s', g)$ as follows:
\begin{align*}
\ell(s,a, s', h, g) =  \frac{ \one\{\pi_{h,g}(s) = a \}  }{1/A} \left(  w_h^{\top} \phi(s,a) - r(s,a) - \max_{a'} w_{h+1}^{\top}  \phi(s',a')\right).
\end{align*}
For any distribution $\nu \in\Delta(\Scal)$, with probability $1-\delta$ over the randomness of the $m$ i.i.d triples $\{s_i,a_i,s'_i\}_{i=1}^m$ with $s_i\sim \mu,a_i\sim \mathrm{Unif}_{\Acal}, s'_i\sim P_h(\cdot | s_i,a_i)$, we have:
 %Define $\mathcal{W} \subset \{ w: w\in\mathbb{R}^d, \|w\|_2 \leq W\}$. Suppose for all $x\in\mathcal{X}$ we have $\|x\|_2 \leq 1$. For any distribution $\nu\in \Delta(\Xcal)$, with probability $1-\delta$ over the randomness of the $m$ i.i.d samples $\{x_i\}_{i=1}^m \sim \nu$, we have:
\begin{align*}
\max_{g\in\Hcal} \left\lvert \EE_{s\sim \nu,a\sim \text{Unif}_{\Acal}, s'\sim P_h(\cdot|s,a)} [ \ell(s,a,s', h, g) ] - \sum_{i=1}^m \ell(s_i,a_i,s'_i,h, g) / m  \right\rvert \leq 3 HA \sqrt{ \frac{ 2d \ln(1 + 4A W m )  + \ln(1/\delta)}{ m} }.
\end{align*}
Similarly, when $\ell(s,a,s',g)$ is defined as:
\begin{align*}
\ell(s,a, s', h, g) = w_h^{\top} \phi(s,a) - r(s,a) - \max_{a'} w_{h+1}^{\top}  \phi(s',a') ,
\end{align*}  for any distribution $\nu\in \Delta(\Scal\times\Acal)$, with probability $1-\delta$ over randomness of the $m$ i.i.d triples $\{s_i,a_i,s'_i\}_{i=1}^m$ with $s_i,a_i\sim \nu, s_i' \sim P_h(\cdot | s_i,a_i)$, we have:
\begin{align*}
\max_{g\in\Hcal} \left\lvert \EE_{s\sim \nu,a\sim \text{Unif}_{\Acal}, s'\sim P_h(\cdot|s,a)} [ \ell(s,a,s', h, g) ] - \sum_{i=1}^m \ell(s_i,a_i,s'_i,h, g) / m  \right\rvert \leq 3 H \sqrt{ \frac{ 2d \ln(1 + 4 W m )  + \ln(1/\delta)}{ m} }.
\end{align*}
\end{lemma}
\begin{proof}
The proof uses the standard $\epsilon$-net argument.  Define $\mathcal{N}_{\epsilon}$ as the $\epsilon$-net for  $\Wcal = \{ w: w\in\mathbb{R}^d, \|w\|_2 \leq W, w^{\top} \phi(\cdot,\cdot) \in \Hcal \}$. Standard $\epsilon$-net argument shows that $|\Ncal_{\epsilon}| \leq (1 + W / \epsilon)^d$.  Denote $g' = \{ (w'_h)^{\top} \phi\}_{h=0}^{H-1}$, we have:
\begin{align*}
&\left\lvert \ell(s,a,s', h, g) - \ell(s,a,s', h,g')  \right\rvert \leq  \frac{ \one\{\pi_{h,g}(s) = a \}  }{1/A} \left[ | w_h^{\top} \phi(s,a) - (w_h')^{\top}\phi(s,a)  | + \max_{a'} |  w_{h+1}^{\top} \phi(s',a') - (w_{h+1}')^{\top}\phi(s',a')  |   \right]\\
&\leq A \sup_{s,a} | w_h^{\top} \phi(s,a) - (w_h')^{\top}\phi(s,a)  |  + A \sup_{s,a} | w_{h+1}^{\top} \phi(s,a) - (w_{h+1}')^{\top}\phi(s,a)  | \\
& \leq A \| w_h - w'_{h} \|_2 + A \|w_{h+1} - w_{h+1}' \|_2.
%& \leq  \frac{2 \one\{\pi_{h,g}(s) = a \}  }{1/A} \sup_{s,a} | w_h^{\top} \phi(s,a) - (w_h')^{\top}\phi(s,a)  | \leq 2A | w_h^{\top} \phi(s,a) - (w_h')^{\top}\phi(s,a)  |.
\end{align*} Thus, if $\|w_h - w_h'\|_2 \leq \epsilon$, $\| w_{h+1} -w_{h+1}' \|_2 \leq \epsilon$, we have $\left\lvert \ell(s,a,s', h, g) - \ell(s,a,s', h,g')  \right\rvert \leq  2A\epsilon$. 

Consider a $w_h \in \Ncal_{\epsilon}, w_{h+1} \in \Ncal_{\epsilon}$, via Hoeffding's inequality, we have that with probability at least $1-\delta$:
\begin{align*}
\left\lvert \EE [ \ell(s,a,s', h, (w_h,w_{h+1})) ] -  \bar{\EE} [ \ell(s,a,s', h, (w_h,w_{h+1})) ]  \right\rvert \leq  2 HA \sqrt{ \ln(1/\delta) / m  }.
\end{align*} where for notation simplicity, we denote $\bar\EE$ as the empirical average over the $m$ data triples. 
Apply a union bound over all $w_h\in\Ncal_\epsilon$ and $w_{h+1} \in \Ncal_{\epsilon}$, we have that with probability at least $1-\delta$:
\begin{align*}
\forall w_h\in\Ncal_\epsilon, w_{h+1}\in\Ncal_\epsilon: \left\lvert  \EE [ \ell(s,a,s', h, (w_h,w_{h+1})) ]  - \bar\EE [ \ell(s,a,s', h, (w_h,w_{h+1}))  ] \right\rvert \leq  2 HA \sqrt{ 2 \ln(|\Ncal_\epsilon |/\delta) / m  }.
\end{align*}
Thus for any $w_h \in \Wcal, w_{h+1} \in \Wcal$,  we have:
\begin{align*}
\left\lvert \EE [ \ell(s,a,s', h, (w_h,w_{h+1})) ]  -\bar\EE [  \ell(s,a,s', h, (w_h,w_{h+1}))  ]  \right\rvert \leq  2 HA \sqrt{ 2 \ln(|\Ncal_\epsilon |/\delta) / m  }
 + 4A \epsilon.
 \end{align*} Set $\epsilon = 1 / (4 A m)$, we have:
 \begin{align*}
 \left\lvert \EE [ \ell(s,a,s', h, (w_h,w_{h+1}))  ] -\bar\EE [  \ell(s,a,s', h, (w_h,w_{h+1})) ] \right\rvert & \leq  2HA \sqrt{ ( 2 d \ln  (1 + 4 A W m ) + \ln(1/\delta) )/ m   } + 1 / m \\
 & \leq  3 HA \sqrt{ \frac{ 2d \ln(1 + 4A W m )  + \ln(1/\delta)}{ m} }.
 \end{align*} This proves the first claim in the lemma.

The second claim in the lemma can be proved in a similar way, noting that without the importance weighting term on the loss $\ell$, we have:
\begin{align*}
\left\lvert \ell(s,a,s', h, g) - \ell(s,a,s', h,g')  \right\rvert  \leq \| w_h - w_h'  \|_2 + \| w_{h+1} - w_{h+1}'\|_2.
\end{align*}

\end{proof}









\iffalse
\begin{lemma}[Version of Freedman's inequality from~\citet{beygelzimer2011contextual}]
    Let $X_1,X_2,\ldots,X_T$ be a sequence of real-valued random variables adapted to the filtration $\calF_i$. That is, $X_i$ is measurable with respect to $\calF_i$ and further assume that $\E[X_i \given \calF_{i-1}]$. Define $S = \sum_{t=1}^T X_t$, $V = \sum_{t=1}^T \E[X_t^2\given \calF_{t-1}]$ and let $X_t \leq R$ almost surely for all $t$. Then for any $\delta \in (0,1)$ and $\lambda \in [0,1/R]$, with probability at least $1-\delta$, 
    \[
        S \leq (e-2)\lambda V + \frac{\ln(1/\delta)}{\lambda}.
    \]
    In particular, choosing $\lambda = \min\left\{\frac{1}{R}, \sqrt{\ln(1/\delta)}{V}\right\}$, we get the Bernstein-style bound
    \[
        S \leq 2\sqrt{V\ln\frac{1}{\delta}} + R\ln\frac{1}{\delta}.
    \]
\label{lemma:freedman}
\end{lemma}
\fi
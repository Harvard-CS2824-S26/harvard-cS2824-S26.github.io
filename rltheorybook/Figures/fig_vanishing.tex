
\begin{figure}
		\centering

			\begin{tikzpicture}[->,>=stealth,node distance=2.0cm]
		\node[state] (s0) {$s_0$};
		\node[state, right of=s0] (s1) {$s_1$};
		\node[state, draw=none] (d1) [right=of s1, xshift=-2.0cm] {$\cdots$};
		\node[state, right of=d1] (sh) {$s_{H}$};
		\node[state, accepting, right of=sh] (shp) {$s_{H+1}$};
		\draw (shp) edge[loop above] node{$a_1$} (shp)
		(s0) edge[bend right=30, below] node{$a_1$} (s1)
		(d1) edge[bend right=30, below] node{$a_1$} (sh)
		(sh) edge[bend right=0, below] node{$a_1$} (shp)
		(s1) edge[bend right=0, above] node{$a_2$} (s0)
		(sh) edge[bend right=0, above] node{$a_2$} (d1)
		(s1) edge[bend right=40, above] node{$a_3$} (s0)
		(s1) edge[bend right=90, above] node{$a_4$} (s0)
		(sh) edge[bend right=40, above] node{$a_3$} (d1)
		(sh) edge[bend right=90, above] node{$a_4$} (d1);
			\end{tikzpicture}
			\caption{(Vanishing gradient example) A deterministic, chain MDP of length $H+2$. We
                          consider a policy where $\pi(a | s_i) =
                          \theta_{s_i,a}$ for $i=1,2,\ldots,H$. Rewards
                          are $0$ everywhere other than $r(s_{H+1},
                          a_1) = 1$. See Proposition~\ref{proposition:small_grad}.}
			\label{fig:chain}

	\end{figure}

\chapter{Approximate Dynamic Programming in Large MDPs: Offline Learning and Coverage}
\label{chap:approx_dp_offline}

This chapter studies a complementary regime to Chapter~\ref{chap:linear_features}.
In Chapter~\ref{chap:linear_features} we obtained strong (worst-case) sample
complexity guarantees in large MDPs by imposing a \emph{structural closure}
assumption: the relevant Bellman updates remain within a prescribed (typically
low-dimensional) function class (e.g., linear Bellman completeness / linear MDP
structure). Under such closure, one can propagate estimation error cleanly and
avoid explicit dependence on the size of the state space.

Here, we consider a more classical approach based on \emph{approximate dynamic
programming}, but with using \emph{supervised regression} subroutines. Concretely, we will
study \emph{fitted} methods that repeatedly regress to Bellman targets and then
act greedily with respect to the resulting approximations. This style of
algorithm is attractive because it is modular --- it reduces reinforcement
learning to a sequence of standard prediction problems --- and it allows
substantial flexibility in the choice of function class.  We study this question in the discounted MDP framework in this chapter. Again, we seek guarantees which do not depend on the size of the state and action spaces. 

However, this flexibility comes at a price. In the offline setting, the learner
does not control data collection: it is restricted to a fixed dataset drawn
from some behavior distribution. In such settings, purely worst-case guarantees
cannot hold without additional assumptions: the data distribution must provide
\emph{coverage} of the state-action pairs that matter for planning. Moreover,
even with good coverage, the function class need not be closed under Bellman
updates, leading to an irreducible approximation term.

The main message of this chapter is that fitted methods yield meaningful
guarantees under two explicit requirements:
\begin{enumerate}
\item \textbf{Distributional coverage (concentrability).} The offline data
  distribution must dominate the state-action distributions induced by policies
  the algorithm might implicitly reason about. This is quantified by a
  \emph{concentrability coefficient}.
\item \textbf{Bellman approximation (inherent Bellman error).} The function
  class must be able to approximate Bellman updates in an average-case sense
  under the data distribution.
\end{enumerate}
Under these conditions, fitted $Q$-iteration (FQI) can be analyzed as a stable
approximate value iteration procedure with regression error at each step,
propagated through the MDP dynamics via concentrability.

\section{Fitted $Q$-Iteration (FQI) in Offline RL}
\label{sec:fqi_offline}

We work with an infinite-horizon discounted MDP
$\mathcal{M}=(\Scal,\Acal,P,r,\gamma,\mu)$ where $\gamma\in(0,1)$, the reward is
bounded $r(s,a)\in[0,1]$, and $\mu$ is an initial-state distribution. Let
\[
\vmax := \frac{1}{1-\gamma}.
\]
For any function $f:\Scal\times\Acal\to\mathbb{R}$, recall the Bellman optimality
operator
\[
(\Tcal f)(s,a) := r(s,a) + \gamma \E_{s'\sim P(\cdot\mid s,a)} \max_{a'\in\Acal} f(s',a').
\]
We write $Q^\star$ for the unique fixed point of $\Tcal$ and $V^\star(s):=\max_a
Q^\star(s,a)$, with overall value $V^\star := \E_{s_0\sim\mu}[V^\star(s_0)]$.
For a policy $\pi$, we similarly denote $V^\pi(s_0)$ and $V^\pi :=
\E_{s_0\sim\mu}[V^\pi(s_0)]$.

\paragraph{Offline sampling model.}
We are given a fixed distribution $\nu\in\Delta(\Scal\times\Acal)$ and an i.i.d.
dataset
\[
\Dcal := \{(s_i,a_i,r_i,s_i')\}_{i=1}^n,
\qquad (s_i,a_i)\sim\nu,\ \ r_i=r(s_i,a_i),\ \ s_i'\sim P(\cdot\mid s_i,a_i).
\]
The learner must output a policy $\hat{\pi}$ using only $\Dcal$.
For any distribution $\rho$ over $\Scal\times\Acal$ we use
$\|f\|_{2,\rho}^2 := \E_{(s,a)\sim\rho}[f(s,a)^2]$.

\subsection{Two explicit requirements: coverage and Bellman approximation}

\paragraph{Coverage via concentrability.}
Let $d^\pi_\mu$ denote the discounted state-action occupancy measure of policy
$\pi$, i.e.\ the distribution over $(s,a)$ obtained by starting at $s_0\sim\mu$,
then following $\pi$, and sampling a time step according to the geometric
weights $(1-\gamma)\gamma^t$.
The dataset distribution $\nu$ must cover the occupancy measures of the policies
that matter for evaluation and improvement.

\begin{assumption}[Concentrability / coverage]\label{assum:concentrability_fqi}
There exists a constant $C\ge 1$ such that for every (possibly non-stationary)
policy $\pi$,
\[
\sup_{s,a}\ \frac{d^\pi_\mu(s,a)}{\nu(s,a)} \le C,
\]
with the convention that the ratio is $+\infty$ when $\nu(s,a)=0$ and
$d^\pi_\mu(s,a)>0$.
\end{assumption}

This assumption ensures that average-case errors measured under $\nu$ control
the same errors under the visitation distributions that appear in performance
comparisons. Informally: \emph{if the data never visits a region that an
evaluated policy might visit, then no offline method can reliably reason about
that region.}

\paragraph{Bellman approximation via inherent Bellman error.}
Fix a bounded function class
\[
\Fcal \subseteq \{f:\Scal\times\Acal\to[0,\vmax]\}.
\]
In fitted methods, regression targets are Bellman updates $\Tcal f$ of previous
iterates $f\in\Fcal$. Without closure of $\Fcal$ under $\Tcal$, there is an
irreducible approximation error. We quantify this by the standard \emph{inherent
Bellman error} under $\nu$.

\begin{assumption}[Inherent Bellman error under $\nu$]\label{assum:ibe}
Define
\[
\epsilon_{\mathrm{approx},\nu}
:= \max_{f\in\Fcal}\ \min_{f'\in\Fcal}\ \|f' - \Tcal f\|_{2,\nu}^2.
\]
We refer to $\epsilon_{\mathrm{approx},\nu}$ as the inherent Bellman error
(with respect to $\nu$).
\end{assumption}

When $\epsilon_{\mathrm{approx},\nu}=0$, the class is (average-case) Bellman
complete under $\nu$; otherwise, this term will appear additively in the final
suboptimality bound.

\subsection{The FQI algorithm}

Fitted $Q$-iteration (FQI) constructs a sequence $\{f_t\}_{t\ge 0}\subset\Fcal$
by repeatedly regressing to Bellman targets computed from the previous iterate.
Starting from an arbitrary $f_0\in\Fcal$, iterate for $t=1,2,\dots,K$:
\begin{equation}\label{eq:fqi_update}
f_t \in \argmin_{f\in\Fcal}\ \sum_{i=1}^n
\Big( f(s_i,a_i) - \big[r_i + \gamma \max_{a'\in\Acal} f_{t-1}(s_i',a')\big]\Big)^2.
\end{equation}
After $K$ iterations, output the greedy policy
\[
\pi^K(s) \in \argmax_{a\in\Acal} f_K(s,a).
\]

\paragraph{Interpretation.}
At each iteration, the regression problem in \eqref{eq:fqi_update} has a
well-defined conditional expectation target:
\[
\E\!\left[r + \gamma \max_{a'} f_{t-1}(s',a') \mid s,a\right] = (\Tcal f_{t-1})(s,a).
\]
Moreover, since $r\in[0,1]$ and $f_{t-1}\in[0,\vmax]$,
\[
0 \le r + \gamma \max_{a'} f_{t-1}(s',a') \le 1+\gamma\vmax = \vmax,
\qquad\text{and}\qquad
0\le (\Tcal f_{t-1})(s,a)\le \vmax.
\]
Thus, if the regression step returns $f_t$ close to $\Tcal f_{t-1}$ in
$\|\cdot\|_{2,\nu}$, and if $\nu$ provides coverage so that
$\|\cdot\|_{2,\nu}$ controls errors under relevant visitation distributions,
then the iterates behave like an approximate value iteration procedure.

\subsection{Main guarantee: what you get, and what you pay}

The following theorem states that the performance loss decomposes into three
terms:
(i) a \emph{statistical error} term scaling as $1/\sqrt{n}$,
(ii) an \emph{approximation error} term governed by
$\epsilon_{\mathrm{approx},\nu}$, and
(iii) a \emph{finite-iteration} error that decays geometrically in $K$.

\begin{theorem}[Performance guarantee for FQI]\label{thm:fqi_main_ch4}
Fix $K\in\mathbb{N}^+$. Under Assumptions~\ref{assum:concentrability_fqi}
and~\ref{assum:ibe}, and for a finite function class $\Fcal$, FQI
\eqref{eq:fqi_update} satisfies the following: with probability at least $1-\delta$,
\[
V^\star - V^{\pi^K}
\le
\frac{2}{(1-\gamma)^2}
\left(
\sqrt{\frac{22\,C\,\vmax^2\,\ln\!\big(|\Fcal|^2 K/\delta\big)}{n}}
\;+\;
\sqrt{20\,C\,\epsilon_{\mathrm{approx},\nu}}
\right)
\;+\;
\frac{2\gamma^K \vmax}{1-\gamma}.
\]
\end{theorem}

Let us now interpret the theorem, which makes the two ``prices'' explicit:
The \emph{coverage price} is the multiplicative factor $\sqrt{C}$ (and
  hence $C$ inside the square roots). If $C$ is large, average-case regression
  error under $\nu$ can amplify dramatically when translated to the
  distributions induced by greedy policies.
The \emph{Bellman approximation price} is the term
  $\sqrt{\epsilon_{\mathrm{approx},\nu}}$. Even with infinite data, if the class
  cannot approximate Bellman updates under $\nu$, the method converges only up
  to this floor.
Finally, the last term $2\gamma^K\vmax/(1-\gamma)$ is purely algorithmic: it is
the error from running only $K$ fitted iterations.



\subsection*{Proofs for Theorem~\ref{thm:fqi_main_ch4}}

The proof roadmap can be decomposed into two reusable lemmas:
First, we have a \emph{one-step regression lemma} showing that each fitted regression
  step controls the Bellman residual $\|f_t - \Tcal f_{t-1}\|_{2,\nu}$ in terms
  of (i) a statistical error term and (ii) the inherent Bellman error
  $\epsilon_{\mathrm{approx},\nu}$.
Second, we have a \emph{propagation lemma} showing that if every step has small Bellman
  residual in $\|\cdot\|_{2,\nu}$, then the greedy policies $\pi^t$ become
  near-optimal, with concentrability translating $\|\cdot\|_{2,\nu}$ control
  into control under the relevant occupancy measures.

  
We now develop these lemmas and then combine them to prove
Theorem~\ref{thm:fqi_main_ch4}.


We first record the two key ingredients: (i) how Bellman error along the
iteration translates into performance loss, and (ii) a uniform
generalization bound for the regression step defining $f_{t+1}$.

\begin{lemma}[Bellman residual $\Rightarrow$ performance loss]\label{lem:approximate_be_fqi}
Assume that for all $t=0,1,\dots,K-1$ we have
\[
\|f_{t+1}-\Tcal f_t\|_{2,\nu}\le \varepsilon.
\]
Let $\pi^t$ be greedy w.r.t.\ $f_t$ (i.e.\ $\pi^t(s)\in\arg\max_a f_t(s,a)$).
Then for every $k\in\{0,1,\dots,K\}$,
\[
V^\star - V^{\pi^k}
\;\le\;
\frac{2\sqrt{C}\,\varepsilon}{(1-\gamma)^2}
\;+\;
\frac{2\gamma^k\,\vmax}{1-\gamma}.
\]
\end{lemma}

\begin{proof}
Fix $k$. Using the performance difference lemma with $\pi=\pi^k$ and $\pi'=\pi^\star$,
\[
(1-\gamma)\big(V^\star - V^{\pi^k}\big)
=
\E_{s\sim d^{\pi^k}_\mu}\!\left[V^\star(s)-Q^\star\big(s,\pi^k(s)\big)\right]
=
\E_{s\sim d^{\pi^k}_\mu}\!\left[Q^\star\big(s,\pi^\star(s)\big)-Q^\star\big(s,\pi^k(s)\big)\right].
\]
Since $\pi^k$ is greedy w.r.t.\ $f_k$,
\begin{align*}
Q^\star\big(s,\pi^\star(s)\big)-Q^\star\big(s,\pi^k(s)\big)
&\le
\Big(Q^\star\big(s,\pi^\star(s)\big)-f_k\big(s,\pi^\star(s)\big)\Big)
+
\Big(f_k\big(s,\pi^k(s)\big)-Q^\star\big(s,\pi^k(s)\big)\Big).
\end{align*}
Taking expectation and applying Cauchy--Schwarz yields
\begin{align}
(1-\gamma)\big(V^\star - V^{\pi^k}\big)
&\le
\|Q^\star-f_k\|_{1,d^{\pi^k}_\mu\circ\pi^\star}
+
\|Q^\star-f_k\|_{1,d^{\pi^k}_\mu\circ\pi^k}\notag\\
&\le
\|Q^\star-f_k\|_{2,d^{\pi^k}_\mu\circ\pi^\star}
+
\|Q^\star-f_k\|_{2,d^{\pi^k}_\mu\circ\pi^k}.
\label{eq:pdl_split}
\end{align}
Thus it suffices to control $\|Q^\star-f_k\|_{2,\beta}$ for distributions
$\beta$ of the form $d^{\pi^k}_\mu\circ\pi$.

Fix any such $\beta$. Decompose
\[
Q^\star - f_k = (Q^\star - \Tcal f_{k-1}) + (\Tcal f_{k-1} - f_k),
\]
so by the triangle inequality,
\begin{equation}\label{eq:triangle_Qstar_fk}
\|Q^\star-f_k\|_{2,\beta}
\le
\|Q^\star-\Tcal f_{k-1}\|_{2,\beta}
+
\|f_k-\Tcal f_{k-1}\|_{2,\beta}.
\end{equation}
For the second term, Assumption~\ref{assum:concentrability_fqi} implies
$\beta(s,a)\le C\,\nu(s,a)$ pointwise, hence
\begin{equation}\label{eq:conc_l2}
\|f_k-\Tcal f_{k-1}\|_{2,\beta}\le \sqrt{C}\,\|f_k-\Tcal f_{k-1}\|_{2,\nu}.
\end{equation}
For the first term, note that for every $(s,a)$,
\[
\big(Q^\star-\Tcal f_{k-1}\big)(s,a)
=
\gamma\,\E_{s'\sim P(\cdot\mid s,a)}\!\left[
\max_{a'} Q^\star(s',a') - \max_{a'} f_{k-1}(s',a')
\right].
\]
Using $(\E[X])^2\le \E[X^2]$ and
$\big(\max_x u_x - \max_x v_x\big)^2 \le \max_x (u_x-v_x)^2$ gives
\begin{align*}
\|Q^\star-\Tcal f_{k-1}\|_{2,\beta}^2
&\le
\gamma^2\,\E_{\substack{(s,a)\sim\beta\\ s'\sim P(\cdot\mid s,a)}}\!
\left[
\max_{a'}\big(Q^\star(s',a')-f_{k-1}(s',a')\big)^2
\right].
\end{align*}
Define a distribution $\beta'$ over $(s',a')$ by sampling
$(s,a)\sim\beta$, then $s'\sim P(\cdot\mid s,a)$, and finally choosing
\[
a' \in \arg\max_{a\in\Acal}\ \big(Q^\star(s',a)-f_{k-1}(s',a)\big)^2.
\]
With this definition,
\[
\E_{\substack{(s,a)\sim\beta\\ s'\sim P(\cdot\mid s,a)}}\!
\left[
\max_{a'}\big(Q^\star(s',a')-f_{k-1}(s',a')\big)^2
\right]
=
\E_{(s',a')\sim\beta'}\!\left[\big(Q^\star(s',a')-f_{k-1}(s',a')\big)^2\right]
=
\|Q^\star-f_{k-1}\|_{2,\beta'}^2,
\]
and therefore
\begin{equation}\label{eq:bellman_to_next_dist}
\|Q^\star-\Tcal f_{k-1}\|_{2,\beta} \le \gamma\,\|Q^\star-f_{k-1}\|_{2,\beta'}.
\end{equation}
Combining \eqref{eq:triangle_Qstar_fk}, \eqref{eq:conc_l2}, and \eqref{eq:bellman_to_next_dist} gives
\begin{equation}\label{eq:one_step_recursion}
\|Q^\star-f_k\|_{2,\beta}
\le
\gamma\,\|Q^\star-f_{k-1}\|_{2,\beta'}
+
\sqrt{C}\,\|f_k-\Tcal f_{k-1}\|_{2,\nu}.
\end{equation}
Iterating \eqref{eq:one_step_recursion} for $k$ steps yields
\[
\|Q^\star-f_k\|_{2,\beta}
\le
\sqrt{C}\sum_{t=0}^{k-1}\gamma^t\,\|f_{k-t}-\Tcal f_{k-t-1}\|_{2,\nu}
+
\gamma^k\,\|Q^\star-f_0\|_{2,\widetilde\beta},
\]
for some distribution $\widetilde\beta$ over $\Scal\times\Acal$.
Under the hypothesis $\|f_{t+1}-\Tcal f_t\|_{2,\nu}\le \varepsilon$ and since
$Q^\star,f_0\in[0,\vmax]$ pointwise, we obtain
\[
\|Q^\star-f_k\|_{2,\beta}
\le
\frac{\sqrt{C}\,\varepsilon}{1-\gamma} + \gamma^k\vmax.
\]
Applying this bound to both distributions in \eqref{eq:pdl_split} gives
\[
(1-\gamma)\big(V^\star - V^{\pi^k}\big)
\le
2\left(\frac{\sqrt{C}\,\varepsilon}{1-\gamma} + \gamma^k\vmax\right),
\]
and dividing by $(1-\gamma)$ yields the claim.
\end{proof}

\begin{lemma}[Uniform regression (Bellman error) bound]\label{lem:generalization_least_square}
With probability at least $1-\delta$, for all $t=0,1,\dots,K-1$,
\[
\| f_{t+1} - \Tcal f_t \|_{2,\nu}^2
\;\le\;
\frac{22 \vmax^2 \ln(|\Fcal|^2 K/\delta)}{n}
\;+\;
20\,\epsilon_{\mathrm{approx},\nu}.
\]
\end{lemma}

\begin{proof}
Fix $g\in\Fcal$ and consider the regression problem with inputs $(s_i,a_i)$ and targets
\[
y_i := r_i + \gamma \max_{a'} g(s_i',a').
\]
Then $\E[y_i\mid s_i,a_i] = (\Tcal g)(s_i,a_i)$, and, as noted above,
$0\le y_i \le \vmax$ and $0\le (\Tcal g)(s,a)\le \vmax$.
By Assumption~\ref{assum:ibe},
\[
\min_{f\in\Fcal}\ \|f-\Tcal g\|_{2,\nu}^2 \le \epsilon_{\mathrm{approx},\nu}.
\]
Applying the least-squares generalization bound from Lemma~\ref{lem:least_square_gen}
(square loss, bounded responses, finite $\Fcal$) yields that, with probability at least
$1-\delta$, for this fixed $g$,
\[
\|\hat f_g - \Tcal g\|_{2,\nu}^2
\le
\frac{22 \vmax^2 \ln(|\Fcal|/\delta)}{n}
+ 20\,\epsilon_{\mathrm{approx},\nu},
\]
where $\hat f_g$ is the ERM over $\Fcal$ for targets $y_i$.

Now take a union bound over all $g\in\Fcal$ (replacing $\delta$ by $\delta/|\Fcal|$),
and then a union bound over $t=0,\dots,K-1$ (replacing $\delta$ by $\delta/K$).
Since $f_{t+1}=\hat f_{f_t}$ by construction, this yields the stated bound
simultaneously for all $t$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:fqi_main_ch4}]
By Lemma~\ref{lem:generalization_least_square}, with probability at least $1-\delta$,
for all $t\le K-1$,
\[
\| f_{t+1} - \Tcal f_t \|_{2,\nu}
\le
\sqrt{\frac{22 \vmax^2 \ln(|\Fcal|^2 K/\delta)}{n}}
+
\sqrt{20\,\epsilon_{\mathrm{approx},\nu}}.
\]
Set
\[
\varepsilon :=
\sqrt{\frac{22 \vmax^2 \ln(|\Fcal|^2 K/\delta)}{n}}
+
\sqrt{20\,\epsilon_{\mathrm{approx},\nu}}.
\]
Applying Lemma~\ref{lem:approximate_be_fqi} with this $\varepsilon$ and $k=K$ gives
\[
V^\star - V^{\pi^K}
\le
\frac{2\sqrt{C}}{(1-\gamma)^2}\left(
\sqrt{\frac{22 \vmax^2 \ln(|\Fcal|^2 K/\delta)}{n}}
+
\sqrt{20\,\epsilon_{\mathrm{approx},\nu}}
\right)
+
\frac{2\gamma^K \vmax}{1-\gamma},
\]
which is exactly the theorem statement.
\end{proof}

\subsection{What breaks without coverage: a minimal impossibility}
\label{sec:no_concentrability}

Assumption~\ref{assum:concentrability_fqi} is not a technical artifact of the
analysis: without some form of \emph{coverage} (overlap between the data
distribution $\nu$ and the occupancy measures of relevant policies), offline RL
is information-theoretically impossible. The simplest obstruction is
\emph{support mismatch}: if $\nu$ assigns zero probability to a state--action
pair that an optimal (or even just a competitive) policy may visit, then the
offline dataset contains \emph{no information} about what would happen there.

We give a crisp two-action counterexample showing that without concentrability,
even the most basic offline learning goal can fail.

\begin{proposition}[No coverage $\Rightarrow$ offline RL is impossible]
\label{prop:no_coverage_impossible}
There exists an offline data distribution $\nu$ and two discounted MDPs
$\mathcal{M}_+$ and $\mathcal{M}_-$ such that:
\begin{enumerate}
\item $\mathcal{M}_+$ and $\mathcal{M}_-$ induce \emph{exactly the same}
distribution over datasets $\Dcal$ drawn i.i.d.\ from $\nu$ (hence no offline
algorithm can distinguish them from $\Dcal$), but
\item the optimal policies (and optimal values) differ substantially, so any
offline algorithm must incur nontrivial suboptimality on at least one of the two
MDPs.
\end{enumerate}
In particular, for any (possibly randomized) offline algorithm $\Acal$ that maps
$\Dcal$ to a policy $\hat\pi$, we have
\[
\max\Big\{\E_{\Dcal\sim \mathcal{M}_+}\big[V^\star - V^{\hat\pi}\big],\;
         \E_{\Dcal\sim \mathcal{M}_-}\big[V^\star - V^{\hat\pi}\big]\Big\}
\;\ge\; \frac{1}{4}\vmax.
\]
\end{proposition}

\begin{proof}
Consider the one-state MDP with $\Scal=\{s\}$, two actions $\Acal=\{a_0,a_1\}$,
deterministic self-loop dynamics $P(s\mid s,a)=1$, and initial distribution
$\mu(s)=1$. Define the offline data distribution $\nu$ to put all of its mass on
the single pair $(s,a_0)$, i.e.\ $\nu(s,a_0)=1$ and $\nu(s,a_1)=0$. Hence every
offline dataset $\Dcal$ consists only of samples of $(s,a_0)$.

Now define two reward functions (keeping the same dynamics):
\[
\mathcal{M}_+: \quad r(s,a_0)=\tfrac12,\ \ r(s,a_1)=1,
\qquad\qquad
\mathcal{M}_-: \quad r(s,a_0)=\tfrac12,\ \ r(s,a_1)=0.
\]
Under $\nu$, the algorithm only ever observes rewards for $(s,a_0)$, which are
identically $\tfrac12$ in both MDPs. Therefore the distribution of $\Dcal$ is
\emph{exactly the same} under $\mathcal{M}_+$ and $\mathcal{M}_-$, and any
algorithm $\Acal$ induces the same distribution over outputs $\hat\pi$ under both
MDPs.

Since $\Dcal$ has the same distribution under $\mathcal{M}_+$ and $\mathcal{M}_-$,
no offline algorithm can reliably tell which MDP it is in, and therefore it
cannot reliably choose between $a_0$ and $a_1$. In $\mathcal{M}_+$, choosing
$a_0$ is suboptimal by $\vmax/2$ (since $V^\star=\vmax$ but $V^{a_0}=\vmax/2$),
while in $\mathcal{M}_-$, choosing $a_1$ is suboptimal by $\vmax/2$ (since
$V^\star=\vmax/2$ but $V^{a_1}=0$). Consequently, any decision rule that outputs
some action (possibly randomized) must incur expected suboptimality at least
$\vmax/4$ on at least one of the two MDPs (the minimax choice is to randomize
$50$--$50$, yielding $\vmax/4$ in either case).
\end{proof}

Let us connect this proposition back to concentrability.
In Proposition~\ref{prop:no_coverage_impossible}, the policy that plays $a_1$
induces occupancy $d^\pi_\mu(s,a_1)=1$, yet $\nu(s,a_1)=0$, so the ratio
$d^\pi_\mu(s,a_1)/\nu(s,a_1)$ is infinite. This is exactly the failure mode that
Assumption~\ref{assum:concentrability_fqi} rules out: it enforces that whenever a
policy can place mass on $(s,a)$, the dataset distribution must also place mass
there (with a bounded density ratio).

\section{Optional: Agnostic policy search and the horizon barrier}\label{sec:agnostic_horizon}

The previous sections focused on \emph{discounted} offline learning with
concentrability.  Here we briefly switch to a \emph{finite-horizon} lens to make
a complementary point: without additional structure, even the most basic goal
of selecting a near-best policy from a class can require sample sizes that are
exponential in the horizon. This ``horizon barrier'' is the finite-horizon
analogue of what goes wrong in offline RL when coverage deteriorates.

Consider a horizon-$H$ MDP with initial distribution $\mu$ and finite action set
$\Acal$. Let $\Pi$ be a finite set of deterministic policies. The \emph{agnostic objective}
is
\[
\max_{\pi\in\Pi} V^\pi_0(\mu),
\]
i.e.\ compete with the best policy in the class without assuming realizability
or Bellman closure.

\paragraph{Importance sampling gives an unbiased estimator, with $|\Acal|^H$ variance.}
Collect $N$ trajectories under the behavior policy that acts uniformly at random
at each step, denoted $\mathrm{Unif}_{\Acal}$. For any deterministic $\pi$, the
standard importance-sampling (IS) estimator is
\[
\widehat V^\pi_0(\mu)
:=
\frac{1}{N}\sum_{n=1}^N
w(\tau^{(n)};\pi)\,\sum_{t=0}^{H-1} r_t^{(n)},
\qquad
w(\tau;\pi)=|\Acal|^H\,\ind\!\big(\pi(s_t)=a_t\ \forall t\big),
\]
which is unbiased: $\E[\widehat V^\pi_0(\mu)]=V^\pi_0(\mu)$. The difficulty is
that $w(\tau;\pi)$ is nonzero only when the behavior trajectory happens to match
$\pi$'s entire length-$H$ action sequence, an event of probability $|\Acal|^{-H}$.
Thus the estimator has variance that scales like $|\Acal|^H$ (up to polynomial
factors in $H$), so concentration requires exponentially many samples.

A simple union-bound argument yields the following representative statement:
with probability at least $1-\delta$, simultaneously for all $\pi\in\Pi$,
\[
\Big|\widehat V^\pi_0(\mu) - V^\pi_0(\mu)\Big|
\;\lesssim\;
H|\Acal|^H\sqrt{\frac{\log(|\Pi|/\delta)}{N}}.
\]
In particular, selecting a near-best policy in $\Pi$ from this shared data
requires $N$ that is exponential in $H$ in the worst case.

\paragraph{A matching lower bound (needle in a haystack).}
This exponential dependence is not an artifact of IS. There exist families of
horizon-$H$ MDPs (e.g.\ a balanced $|\Acal|$-ary tree in which only one leaf is
rewarding) for which any algorithm---even with access to a generative model---must
use $\Omega(|\Acal|^H)$ samples to obtain even a constant additive optimality
guarantee with nontrivial probability. Intuitively, unless the algorithm samples
the rewarding path, it receives no information about where reward lies.

\smallskip
Taken together, these facts motivate why offline RL analyses must assume some
form of overlap/coverage: without it, the effective importance weights needed to
reason about candidate policies can be exponentially large, mirroring the
$|\Acal|^H$ barrier in agnostic horizon-$H$ problems.

\section{Bibliographic Remarks and Further Readings}\label{bib:api}

The notion of \emph{concentrability} (or coverage via occupancy-measure density
ratios) was introduced by \citet{munos2003error,munos2005error} to obtain sharp
\emph{average-case} approximation bounds for approximate dynamic programming,
under the assumption that the relevant concentrability coefficients are bounded.
This line of work also supports sample-based fitted methods, with explicit
finite-sample bounds, provided that the data-collection policy induces adequate
coverage; see, e.g., \citet{munos2005error,szepesvari2005finite,antos2008learning,lazaric2016analysis}.
For further discussion of concentrability-type quantities and their role in
offline RL, see \citet{chen2019information}.

The reduction of reinforcement learning to supervised learning via reusing data
across policies was developed early in \citet{NIPS1999_1664}. 
 That work analyzed
a related evaluation/selection scheme based on a ``trajectory tree'' view of
policy behavior, and explicitly connected learnability to capacity measures of
the policy class (including VC-type dimensions).
Our optional agnostic-learning discussion in Section~\ref{sec:agnostic_horizon}
is a stripped-down version of this theme, using importance sampling to make the
exponential-in-$H$ barrier transparent.
The fundamental sample
complexity tradeoff --- avoiding explicit dependence on the size of the state space
at the cost of exponential dependence on the horizon --- is explored further in 
in~\citet{kakade2003sample}.

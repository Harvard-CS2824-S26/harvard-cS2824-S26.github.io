\newcommand{\vmax}{V_{\max}}
\chapter{Fitted Dynamic Programming Methods}
\label{chap:api}

Let us again consider the question of learning in large MDPs, when the
underlying MDPs is unknown. In the previous chapter, we relied on the
linear Bellman completeness assumption to provide strong guarantees
with only polynomial dependence on the dimension of the feature
space and the horizon $H$. This chapter considers the approach of
using \emph{function approximation} methods with iterative dynamic programming
approaches.

In particular, we now consider approaches
which rely on (average-case) supervised learning methods (namely regression), where we use
regression to approximate the target functions in both the value
iteration and policy iteration algorithms. We refer to these
algorithms as \emph{fitted $Q$-iteration} (FQI) and \emph{fitted
  policy iteration} (FPI). The FQI algorithm can be implemented in
an offline RL sampling model while the FPI algorithm requires the
sampling access to an episodic model (see 
Chapter~\ref{chap:prelims} for review of these sampling models).

\iffalse
Here, we will consider the episodic setting, where we do not have a
generative model and instead can only reset the state under our
initial state distribution. This chapter will consider a simple
approach where we learn an approximate Q function and then update our
policy greedily with respect to the estimated Q function. 

We consider infinite discounted MDPs in this chapter. Here the MDP might have large or even continuous state space.  We assume action space is discrete and we denote $A = |\Acal|$ as the number of actions. 
We are given a policy class $\Pi = \{\pi: \Scal\mapsto \Acal\} \subset \Scal\mapsto \Acal$.  Note that the policy class is a restricted policy class which is a subset of the class of all mappings from $\Scal$ to $\Acal$. 

%\sk{this doesn't matter, right?}
We denote the best policy in policy class as $\pi^\star$, which is the policy that maximizes the expected total reward with $\mu$ as the initial state distribution:
\begin{align*}
\pi^\star \in \argmax_{\pi\in \Pi} \EE\left[ \sum_{h=0}^{\infty} \gamma^h r(s_h,a_h) | a_h = \pi(s_h) \right].
\end{align*} 
Note that $\pi^\star$ is the best policy in policy class that maximizes the objective function and it is not necessarily true that $\pi^\star$ will be the optimal policy of the MDP $M$ which maximizes total reward starting from any state simultaneously (i.e., the policy class might not be rich enough to contain the optimal policy of $M$).
\fi


This chapter focuses on obtaining of \emph{average case function
  approximation error} bounds, provided we have a somewhat stringent
condition on how the underlying MDP behaves, quantified by the
\emph{concentrability coefficient}. This notion was introduced
in~\cite{munos2003error,munos2005error}. While the notion is somewhat
stringent, we will see that it is not
avoidable without further assumptions. The next chapter more
explicitly considers lower bounds, while Chapters~\ref{chap:pg_approx} and~\ref{chap:cpi} seek to
relax the concentrability notion.


\section{Fitted $Q$-Iteration (FQI) and Offline RL}

%\sk{maybe give the guarantee with $\eps_{\textrm{stat}}$-stat and
%  $\eps_{\textrm{approx}}$? for $\eps_{\textrm{stat}}$, this will just
%be the inherent $\nu$-Bellman error or the inherent $\nu$-policy
%error, where the error are average case under $\nu$. or something like
%that.}

We consider infinite horizon discounted MDP $\mathcal{M} = \{ \Scal,\Acal, \gamma, P, r,  \mu\}$ where $\mu$ is the initial state distribution. We assume reward is bounded, i.e., $\sup_{s,a} r(s,a) \in [0,1]$. For notation simplicity, we denote $\vmax : = 1/(1-\gamma)$. 

Given any $f: \Scal\times\Acal\mapsto \mathbb{R}$, we denote the Bellman operator $\Tcal f : \Scal\times\Acal\mapsto \mathbb{R}$ as follows. For all $s,a\in \Scal\times\Acal$,
\begin{align*}
\Tcal f(s,a)  := r(s,a) + \gamma \mathbb{E}_{s'\sim P(\cdot | s,a)} \max_{a'\in \Acal} f(s',a').
\end{align*}  

We assume that we have a distribution $\nu\in\Delta(\Scal\times\Acal)$. We collect a  dataset $\mathcal{D} := \{(s_i,a_i,r_i,s_i')\}_{i=1}^n$ where $s_i,a_i \sim \nu, r_i = r(s_i,a_i), s_i' \sim P(\cdot |s_i,a_i)$. Given $\Dcal$, our
goal is to output a near optimal policy for the MDP, that is we would
like our algorithm to produce a policy $\hat{\pi}$ such that, with
probability at least $1-\delta$, $V(\hat{\pi}) \geq V^\star -
\epsilon$, for some $(\epsilon,\delta)$ pair. As usual, the number of
samples $n$ will depend on the accuracy parameters $(\epsilon,\delta)$
and we would like $n$ to scale favorably with these. Given any distribution $\nu \in \Scal\times\Acal$, and any function $f:\Scal\times\Acal\mapsto\mathbb{R}$, we write $\|f \|_{2,\nu}^2 := \EE_{s,a\sim \nu} f^2(s,a)$

Denote a function class $\mathcal{F} = \{ f: \Scal\times\Acal \mapsto [0, \vmax]\}$. %We assume $\mathcal{F}$ is discrete and also contains $Q^\star$. 
%\begin{assumption}[Realizability]
%We assume $\mathcal{F}$ is rich enough such that $Q^\star\in\mathcal{F}$.
%\end{assumption}
%We require the sample complexity of the learning algorithm scales polynomially with respect to $\ln\left(\lvert\mathcal{F} \rvert\right)$.

We require the data distribution $\nu$ is exploratory enough. 
%following assumption on the data distribution $\nu$. 
\begin{assumption}[Concentrability]
\label{assum:concentrability}
There exists a constant $C$ such that for any policy $\pi$ (including non-stationary policies), we have:
\begin{align*}
\forall \pi, s,a: \frac{d^\pi(s,a)}{\nu(s,a)} \leq C.
\end{align*}\label{assum:concentrability_fqi}
\end{assumption}
Note that concentrability does not require that the state space is
finite, but it does place some constraints on the system dynamics.  Note that the above assumption requires that $\nu$ to cover all possible policies's state-action distribution, even including non-stationary policies.  %Recall the concentrability assumptions in Approximate Policy Iteration (Chapter~\ref{chap:api}) and Conservative Policy Iteration (Chapter~\ref{chap:cpi}). The concentrability assumption here is the strongest as it requires $\mu$ to cover even non-stationary policies' state-action distributions. 

In additional to the above two assumptions, we need an assumption on the representational condition of class $\mathcal{F}$. 
%\begin{assumption}[Bellman Completion]\label{assump:complete}
%We assume that for any $f\in\Fcal$, $\Tcal f \in \Fcal$.
%\end{assumption}
\begin{assumption}[Inherent Bellman Error] We assume the following
  error bound holds:
\begin{align*}
\epsilon_{approx,\nu} := \max_{f\in \Fcal} \min_{f'\in\Fcal} \|  f' - \Tcal f   \|^2_{2,\nu}.
\end{align*}
We refer to $\epsilon_{approx,\nu}$ as the \emph{inherent Bellman error}
with respect to the distribution $\nu$.
\end{assumption} 
%The assumption depends both on the representation of $\Fcal$ and also the transition $P$. Recall the linear MDP model, there we have that for any linear function $f(s,a) := \theta\cdot \phi(s,a)$, we always have $\Tcal f(s,a) = (\theta')^{\top} \phi(s,a)$ for some $\theta'$. Such %assumption does not always hold. For instance, if $\Fcal = \{ \theta\cdot  \}$
%Note that this implies that $Q^\star \in \Fcal$ (as $Q^\star$ is the convergence point of Value Iteration), which is the weaker
%assumption we would hope is sufficient.  However, as we discuss in Section~\ref{bib:offline}, the Bellman completion assumption is necessary in order to learn in polynomial sample complexity. 

\subsection{The FQI Algorithm}
Fitted Q Iteration (FQI) simply performs the following iteration. Start with some $f_0 \in \Fcal$, FQI iterates:
\begin{align}
\label{eq:FQI}
\text{FQI: } \quad f_{t} \in \argmin_{f\in\Fcal} \sum_{i=1}^n \left( f(s'_i,a_i) - r_i - \gamma \max_{a'\in\Acal} f_{t-1}(s_i,a_i)  \right)^2.
\end{align}
After $k$ many iterations, we output a policy $\pi^k(s) := \argmax_{a} f_k(s,a),\forall s$.

To get an intuition why this  approach can work, let us assume the inherent Bellman error $\epsilon_{approx} = 0$ which is saying that for any $f\in\Fcal$, we have $\Tcal f\in\Fcal$ (i.e., Bellman completion). Note that the Bayes optimal solution is $\Tcal f_{t-1}$. Due to the Bellman completion assumption, the Bayes optimal solution $\Tcal f_{t-1} \in \Fcal$. Thus, we should expect that $f_t$ is close to the Bayes optimal $\Tcal f_{t-1}$ under the distribution $\nu$, i.e., with a uniform convergence argument, for the generalization bound, we should expect that:
\begin{align*}
\EE_{s,a\sim \nu} \left( f_t(s,a) - \Tcal f_{t-1}(s,a) \right)^2 \approx  \sqrt{1/ n}. 
\end{align*} 
%Indeed, as we demonstrate in Lemma~\ref{lem:generalization_least_square}, for square loss, under the realizability assumption, i.e., the Bayes optimal belongs to $\Fcal$ ($\Tcal f_{t-1} \in \Fcal$), we can have a sharper generalization error scaling in the order of $1/ n$.  
Thus in high level, $f_t \approx \Tcal f_{t-1}$ as our data distribution $\nu$ is exploratory, and we know that based on value iteration, $\Tcal f_{k-1}$ is a better approximation of $Q^\star$ than $f_k$, i.e., $\| \Tcal f_{t-1} - Q^\star \|_{\infty} \leq \gamma \| f_{t-1} - Q^\star \|_{\infty}$, we can expect FQI to converge to the optimal solution when $n\to \infty, t\to\infty$. We formalize the above intuition below. 

\subsection{Performance Guarantees of FQI}

We first state the performance guarantee of FQI. 
\begin{theorem}[FQI guarantee]\label{thm:fqi_main}
Fix $K \in \mathbb{N}^+$. Fitted $Q$ Iteration guarantees that with probability $1-\delta$,
\begin{align*}
V^\star - V^{\pi^K} \leq \frac{1}{(1-\gamma)^2} \left(   \sqrt{ \frac{22 C \vmax^2 \ln(|\Fcal|^2 K / \delta)}{n}} + \sqrt{ 20  C\epsilon_{approx,\nu}  }   \right)  + \frac{\gamma^K \vmax}{(1-\gamma) }
%\mathcal{O}\left(\frac{1}{(1-\gamma)^3}\sqrt{\frac{C\log(|\Fcal|/\delta)}{n}}\right) + \frac{2\gamma^K }{(1-\gamma)^2}.
\end{align*}
\end{theorem}


We start from the following lemma which shows that if for all $t$, $\| f_{t+1} - \Tcal f_t \|^2_{2, \nu} \leq \varepsilon$, then the greedy policy with respect to $f_t$ is approaching to $\pi^\star$.
\begin{lemma}\label{lem:approximate_be_fqi}Assume that for all $t$, we have $\|f_{t+1} - \Tcal f_{t}\|_{2,\nu} \leq \varepsilon$, then for any $k$, we have:
\begin{align*}
V^{\pi^k} - V^\star \leq  \frac{\sqrt{C}\varepsilon}{(1-\gamma)^2} + \frac{ \gamma^k \vmax}{1-\gamma}.
\end{align*}
\end{lemma}
\begin{proof}
We start from the Performance Difference Lemma:
\begin{align*}
(1-\gamma) \left( V^\star - V^{\pi_k}\right)   & =  \EE_{s \sim d^{\pi_k}} \left[ - A^{\star}(s, \pi_k(s)) \right]\\
& = \EE_{s \sim d^{\pi_k}} \left[  Q^\star(s, \pi^\star(s)) - Q^\star(s, \pi_k(s))\right]\\
&\leq \EE_{s \sim d^{\pi_k}} \left[  Q^\star(s, \pi^\star(s)) - f_k(s, \pi^\star(s)) + f_k(s, \pi_k(s)) -  Q^\star(s, \pi_k(s))\right]\\
& \leq  \|Q^\star - f_k\|_{1, d^{\pi_k} \circ \pi^\star} + \|Q^\star - f_k\|_{1, d^{\pi_k} \circ \pi_k}       \\
& \leq   \|Q^\star - f_k\|_{2, d^{\pi_k} \circ \pi^\star} + \|Q^\star - f_k\|_{2, d^{\pi_k} \circ \pi_k}      ,
\end{align*} where the first inequality comes from the fact that $\pi_k$ is a greedy policy of $f_k$, i.e., $f_k(s, \pi_k(s)) \geq f_k(s, a)$ for any other $a$ including $\pi^\star(s)$.
Now we bound each term on the RHS of the above inequality. We do this by consider a state-action distribution $\beta$ that is induced by some policy. We have:
\begin{align*}
\| Q^\star - f_k \|_{2, \beta} &\leq \| Q^\star - \Tcal f_{k-1} \|_{2, \beta} + \| f_k - \Tcal f_{k-1} \|_{2, \beta}\\
& \leq \gamma \sqrt{ \EE_{s,a\sim \beta} \left[ \left( \EE_{s'\sim P(\cdot | s,a)} \max_{a} Q^\star(s',a) - \max_{a} f_{k-1}(s',a)   \right)^2  \right]  } + \| f_k - \Tcal f_{k-1} \|_{2,\beta} \\
&  \leq \gamma \sqrt{ \EE_{s,a\sim \beta, s'\sim P(\cdot | s,a)}  \max_{a} \left( Q^\star(s',a) - f_{k-1}(s',a) \right)^2 } + \sqrt{C} \| f_k - \Tcal f_{k-1} \|_{2, \nu},
\end{align*} where in the last inequality, we use the fact that $(\EE[x])^2 \leq \EE[x^2]$,  $(\max_{x} f(x) - \max_{x} g(x))^2 \leq \max_{x} (f(x) - g(x))^2$ for any two functions $f$ and $g$,  and assumption~\ref{assum:concentrability_fqi}.

Denote $\beta'(s',a') = \sum_{s,a} \beta(s,a) P(s'|s,a) \mathbf{1}\{a' = \argmax_{a}\left( Q^\star(s',a) - f_{k-1}(s',a) \right)^2  \}  $, the above inequality becomes:
\begin{align*}
\| Q^\star - f_k \|_{2, \beta} &\leq \gamma \| Q^\star - f_{k-1} \|_{2, \beta'} +  \sqrt{C} \| f_k - \Tcal f_{k-1} \|_{2, \nu}.
\end{align*} We can recursively repeat the same process for $\| Q^\star - f_{k-1} \|_{2, \nu'}$ till step $k = 0$:
\begin{align*}
\| Q^\star - f_k \|_{2, \beta}  \leq  \sqrt{C} \sum_{t=0}^{k-1} \gamma^t \| f_{k-t} - \Tcal f_{k-t-1}  \|_{2,\nu} + \gamma^k \| Q^\star - f_0 \|_{2,\widetilde\nu},
\end{align*} where $\widetilde\nu$ is some valid state-action distribution.  

Note that for the first term on the RHS of the above inequality, we can bound it as follow:
\begin{align*}
\sqrt{C} \sum_{t=0}^{k-1} \gamma^t \| f_{k-t} - \Tcal f_{k-t-1}  \|_{2,\mu}  \leq \sqrt{C} \sum_{t=0}^{k-1} \gamma^k \varepsilon  \leq  \sqrt{C} \varepsilon \frac{1}{1-\gamma}. 
% \mathcal{O}\left( \frac{\vmax \sqrt{C \ln (|\Fcal|/\delta)}}{(1-\gamma) \sqrt{n}} \right)
\end{align*}
For the second term, we have:
\begin{align*}
\gamma \| Q^\star - f_0 \|_{2,\widetilde\nu} \leq \gamma^k \vmax. 
\end{align*}
Thus, we have:
\begin{align*}
\| Q^\star - f_k \|_{2, \beta}  = \frac{\sqrt{C}\varepsilon}{1-\gamma} + \gamma^k \vmax,
\end{align*} for all $\beta$, including $\beta = d^{\pi_k}\circ \pi^\star$, and $\beta = d^{\pi_k}\circ \pi_k$.  This concludes the proof.
\end{proof}

What left is to show that least squares control the error $\varepsilon$. We will use the generalization bound for least squares shown in Lemma~\ref{lem:least_square_gen}.
%Below we first state a general result for least square regression. 

%\begin{lemma}[Least Squares Generalization Bound] \label{lem:least_square_gen}Given a dataset $\Dcal = \{x_i, y_i\}_{i=1}^n$ where $x_i\in\Xcal$ and $x_i,y_i \sim \nu$, $y_i = f^\star(x_i) + \epsilon_i$, where $|y_i| \leq Y, \max_x |f^\star(x) |\leq Y$ and $ |\epsilon_i | \leq \sigma$ for all $i$, and $\{\epsilon_i\}$ are independent from each other. Given a function class $\Fcal: \Xcal\mapsto [0, Y]$, we assume approximate realizable, i.e., $\min_{f\in\Fcal} \EE_{x\sim \nu} \lvert f^\star(x)- f(x) \rvert^2 \leq \epsilon_{approx}$. Denote $\hat{f}$ as the least square solution, i.e., $\hat{f} = \argmin_{f\in\Fcal} \sum_{i=1}^n \left( f(x_i) - y_i \right)^2$. With probability at least $1-\delta$, we have:
%\begin{align*}
%\EE_{x\sim \nu}\left( \hat{f}(x) - f^\star(x) \right)^2 \leq  \frac{144 Y^2 \ln(|\Fcal |/\delta)}{ n } + 8 \epsilon_{approx}.
%\end{align*}
%\end{lemma}
%We defer the proof of the above lemma to the end of this section. Below we first show that by leveraging this lemma, $\varepsilon$ from Lemma~\ref{lem:approximate_be_fqi} scales in the order of $\sqrt{\epsilon_{approx,\nu} + 1/n}$.


%\begin{lemma}[Least Squares Generalization Error]
\begin{lemma}[Bellman error]
\label{lem:generalization_least_square}
With probability at least $1-\delta$, for all $t = 0, \dots, K$, we have:
\begin{align*}
\| f_{t+1} - \Tcal f_{t}  \|_{2,\nu}^2 \leq \frac{22 \vmax^2 \ln(|\Fcal|^2 K / \delta)}{n} + 20 \epsilon_{approx,\nu}.
\end{align*}
%Given $g \in \Fcal$, denote $\hat{f}_{g} := \argmin_{f\in \Fcal} \sum_{i=1}^n (f(s_i,a_i) - r_i - \gamma \max_{a'} f(s_i', a'))^2$. With probability at least $1-\delta$, for all $g\in \Fcal$, we have:
%\begin{align*}
%\EE_{s,a\sim \nu}  \left( \hat{f}_{g}(s,a) - \Tcal g(s,a)    \right)^2  =  \frac{144 \vmax^2 \ln(|\Fcal|^2 / \delta)}{n} + 8 \epsilon_{approx}.
%\mathcal{O}\left( \frac{ \vmax^2 \ln\left( \frac{ |\Fcal| }{\delta} \right) }{n} \right).
%\end{align*}
\end{lemma}
%\paragraph{Remark} The above lemma together with a union bound over all $t = 0, 1,2, \dots, K$, we can show that for all $t \in \mathbb{N}$, we have $\| f_{t+1} - \Tcal f_{t}  \|_{2,\nu}^2 \leq \frac{144 \vmax^2 \ln(|\Fcal|^2 K / \delta)}{n} + 8 \epsilon_{approx}$, with probability at least $1-\delta$.


\begin{proof}

Let us fix a function $g\in \Fcal$, and consider the regression problem on dataset $\{ \phi(s,a), r(s,a) + \gamma \max_{a'}  g(s',a') \}$. Denote $\hat{f}_g  = \argmin_{f\in\Fcal} \sum_{i=1}^n (  f(s_i,a_i) - r_i - \gamma \max_{a'} g(s',a')   )$, as the least square solution. 

Note that in this regression problem, we have $ | r_i + \gamma \max_{a'} g(s',a') | \leq 1 + \gamma \vmax \leq 2 \vmax$, and for our Bayes optimal solution, we have $|\Tcal g(s,a)| =  | r(s,a) + \gamma \EE_{s'\sim P(\cdot | s,a)}\max_{a'} g(s',a') | \leq 1 + \gamma \vmax \leq 2\vmax$. Also note that our inherent Bellman error condition implies that $\min_{f\in\Fcal}\EE_{s,a\sim \nu}( f(s,a) - \Tcal g(s,a) )^2 \leq \epsilon_{approx,\nu}$.  Thus, we can apply Lemma~\ref{lem:least_square_gen} directly here. With probability at least $1-\delta$, we get:
\begin{align*}
\| \hat{f}_g - \Tcal g \|_{2,\nu}^2 \leq \frac{22 \vmax^2 \ln(|\Fcal|/\delta)}{n} + 20 \epsilon_{approx,\nu}.
\end{align*}
Note that the above inequality holds for the fixed $g$. We apply a union bound over all possible $g\in\Fcal$, we get that with probability at least $1-\delta$:
\begin{align}
\label{eq:uniform_bound_regression_fqi}
\forall g\in\Fcal: \| \hat{f}_g - \Tcal g \|_{2,\nu}^2 \leq \frac{22 \vmax^2 \ln(|\Fcal|^2/\delta)}{n} + 20 \epsilon_{approx,\nu}.
\end{align}

\iffalse
Let us consider a fixed iteration $t$. In time step, we perform regression on the dataset $\{ \phi(s,a), r(s,a) + \gamma \max_{a'}  f_{t+1}(s') \}$.



Let us consider a fixed function $g \in \Fcal$ first, and denote $\hat{f} = \argmin_{{f}\in\Fcal} \sum_{i=1}^n ( {f}(s_i,a_i) - r_i - \gamma \max_{a'\in\Acal} g(s'_i, a')  )^2$ as the corresponding least squares solution. Note that $\hat{f}$ is fully determined by $g$. At the end of this proof, we will apply a union bound over all $g\in\Fcal$. 

For any fixed $f\in \Fcal$, let us denote the random variable $z^f_i$:
\begin{align*}
z^f_i := \left( {f}(s_i,a_i) - r_i - \gamma \max_{a'\in\Acal} g(s'_i, a') \right)^2 - \left( \Tcal {g}(s_i,a_i) - r_i - \gamma \max_{a'\in\Acal} g(s_i', a') \right)^2.
\end{align*}  
First note that $z^f_i$ is bounded:
\begin{align*}
| z^f_i | \leq  \vmax^2,
\end{align*} which comes from the assumption that $f(s,a) \in [ 0, \vmax]$ for all $s,a, f$.

Now we compute $\EE[ z_i^f ]$ and the upper bound of the second moment $\EE[ (z_i^f)^2]$, where the expectation is taken with respect to $s_i,a_i, s'_{i}$. First we show that $z_i^f$ is an unbiased estimate of $\| f - \Tcal g \|^2_{2,\nu}$:
\begin{align*}
& \EE_{s_i,a_i \sim \nu, s_i' \sim P(\cdot | s_i,a_i)}\left [  z^f_i  \right] \\
& = \EE_{s_i,a_i \sim \nu, s_i' \sim P(\cdot | s_i,a_i)} \left( f(s_i,a_i) - \Tcal g(s_i,a_i) \right) \left( f(s_i,a_i) + \Tcal g(s_i,a_i) - 2 (r_i + \gamma \max_{a'\in \Acal} g(s_i', a') )  \right)\\
& =  \EE_{s_i,a_i \sim \nu} ( f(s_i,a_i) - \Tcal g(s_i,a_i) ) \left( f(s_i,a_i) + \Tcal g(s_i,a_i) - 2\left( r_i + \EE_{ s_i' \sim P(\cdot | s_i,a_i)}\max_{a'} g(s_i', a') \right)\right) \\
& = \EE_{s_i,a_i\sim \nu} \left( f(s_i,a_i) - \Tcal g(s_i,a_i) \right)^2  =  \EE_{s,a\sim \nu} \left( f(s,a) - \Tcal g(s,a) \right)^2 = \|f-\Tcal g\|_{2,\nu}^2,
\end{align*} where the third equality uses the definition of Bellman operator, i.e., that $r_i + \EE_{ s_i' \sim P(\cdot | s_i,a_i)}\max_{a'} g(s_i', a')  = \Tcal g(s_i,a_i)$.

Now we calculate the second moment of $z^f_i$.  We show that $\EE[ (z_i^f)^2 ] \leq 4\vmax^2 \| f - \Tcal g \|_{2,\nu}^2$:
\begin{align*}
& \EE_{s_i,a_i \sim \nu, s_i' \sim P(\cdot | s_i,a_i)}\left [  (z^f_i)^2  \right] \\
 & =  \EE_{s_i,a_i \sim \nu, s_i' \sim P(\cdot | s_i,a_i)} \left[ (f(s_i,a_i) - \Tcal g(s_i,a_i))^2 \left( f(s_i,a_i) + \Tcal g(s_i,a_i) - 2(r_i + \gamma \max_{a'} g(s'_i, a') )  \right)^2   \right]\\
 &  \leq   4 \vmax^2 \EE_{s_i,a_i \sim \nu} \left[ (f(s_i,a_i) - \Tcal g(s_i,a_i))^2 \right] \\
 & =  4 \vmax^2 \EE_{s,a \sim \nu} \left[ (f(s,a) - \Tcal g(s,a))^2 \right]  = 4 \vmax^2 \| f - \Tcal g  \|^2_{2,\nu}
\end{align*} where in the inequality, we again use the assumption that $f(s,a) \in [0, \vmax]$ for all $s,a,f$.

Now we can use Bernstein's inequality to bound the deviation between the empirical mean $\frac{1}{n} \sum_{i=1}^n z^f_i $ and the expectation $\| f - \Tcal g(s,a) \|_{2,\nu}^2$. Together with a union bound over all $f\in \Fcal$, we have that with probability at least $1-\delta$, for any $f\in \Fcal$:
\begin{align}
\| f - \Tcal g  \|^2_{2,\nu} - \frac{1}{n} \sum_{i=1}^n z^f_i \leq \sqrt{ \frac{ 8\vmax^2  \| f - \Tcal g  \|^2_{2,\nu}   \ln(|\Fcal|/\delta) }{n}  } + \frac{ 4  \vmax^2 \ln(|\Fcal|/\delta) }{3n}.\label{eq:error_for_all_f_fqi}
\end{align}

Recall $\hat{f} = \argmin_{f\in\Fcal}z_i^f$. Below we show that $\sum_{i=1}^n z_i^{\hat{f}} / n$ is upper bound by $\frac{\epsilon_{approx}}{2} + \frac{10\vmax^2 \ln(1/\delta)}{3n}$.

Let us denote $g'  = \argmin_{f\in \Fcal} \|f - \Tcal g\|_{2,\nu}^2$. Note that for $z_i^{g'}$, our previous calculation implies that we have $\EE[ z_i^{g'}] = \|  g' - \Tcal g \|^2_{2,\nu}$, and $\EE[(z_i^g)^2] \leq 4V_{\max}^2 \| g' - \Tcal g \|_{2,\nu}^2$.

Thus via a Bernstein's inequality, with probability at least $1-\delta$, we have:
\begin{align*}
\sum_{i=1}^n z_i^{g'} / n - \EE[z_i^{g'}] \leq  \sqrt{\frac{8 \vmax^2 \EE[z_i^{g'}] \ln(1/\delta)}{n}} + \frac{4\vmax^2\ln(1/\delta)}{3n}.
\end{align*}
When $\sum_{i=1}^n z_i^{g'} / n \geq 4\vmax^2 \ln(1/\delta)/n$, we can show that the above inequality implies that:
\begin{align*}
\sum_{i=1}^n z_i^{g'} / n  \leq \frac{\EE[z_i^{g'}]}{2} + \frac{10\vmax^2 \ln(1/\delta)}{3n}. 
\end{align*} Since $\EE[z_i^{g'}] \leq \epsilon_{approx}$ by the definition of inherent Bellman error, we can conclude that:
\begin{align*}
\sum_{i=1}^n z_i^{g'} / n \leq \frac{\epsilon_{approx}}{2} + \frac{10\vmax^2 \ln(1/\delta)}{3n}. 
\end{align*}  

Note that $\hat{f}$ is the least square minimizer, i.e., $\hat{f} = \argmin_{f\in\Fcal}\sum_{i} z_i^{f}$, so, we have $$\sum_{i=1}^n z_i^{\hat{f}} / n \leq \sum_{i=1}^n z_i^{g'} / n \leq \frac{\epsilon_{approx}}{2} + \frac{10\vmax^2 \ln(1/\delta)}{3n}.$$ This concludes the upper bound on $\sum_{i=1}^n z_i^{\hat{f}} / n$.

Going back to Eq.~\ref{eq:error_for_all_f_fqi}, set $f$ there to be $\hat{f}$, and use the upper bound on $\sum_{i=1}^n z_i^{\hat{f}}/n$ from the above inequality, we have:
\begin{align*}
&\| \hat f - \Tcal g  \|^2_{2,\nu}  \leq \sqrt{ \frac{ 8\vmax^2  \| \hat f - \Tcal g  \|^2_{2,\nu}   \ln(|\Fcal|/\delta) }{n}  } + \frac{ 14  \vmax^2 \ln(|\Fcal|/\delta) }{3n} + \epsilon_{approx}.
\end{align*}

%Set $f = \hat{f}$, and use the fact that $\hat{f}$ is the minimizer of the least square, i.e.,  $\frac{1}{n}\sum_{i=1}^n z_i^{\hat{f}} \leq \frac{1}{n} \sum_{i=1}^n z_i^{\Tcal f'} = 0$, we have:
%\begin{align*}
%\EE_{s,a\sim \mu} \left( \hat{f}(s,a) - \Tcal f'(s,a) \right)^2 \leq \sqrt{ \frac{ 8\vmax^2 \EE_{s,a \sim \mu} \left[ (\hat{f}(s,a) - \Tcal f'(s,a))^2 \right]   \ln(|\Fcal|/\delta) }{n}  } + \frac{ 4  \vmax^2 \ln(|\Fcal|/\delta) }{3n}.
%\end{align*}
Solve for $\| \hat{f} - \Tcal g  \|^2_{2,\nu}$ from the above inequality, we get:
\begin{align*}
\| \hat f - \Tcal g  \|^2_{2,\nu}\leq  \frac{144 \vmax^2 \ln(|\Fcal| / \delta)}{n} + 8 \epsilon_{approx}.
%\left( \sqrt{2} + \sqrt{10/3} \right)^2 \frac{\vmax^2 \ln (|\Fcal| / \delta)}{n}.
\end{align*}

Note that the above result holds for a fixed $g \in \Fcal$. With a union bound over all $g\in \Fcal$, we have that with probability at least $1-\delta$:
\begin{align}
\label{eq:uniform_bound_regression_fqi}
\forall g\in \Fcal: \quad \| \hat{f} - \Tcal g  \|^2_{2,\nu} \leq  \frac{144 \vmax^2 \ln(|\Fcal|^2 / \delta)}{n} + 8 \epsilon_{approx},
%\left( \sqrt{2} + \sqrt{10/3} \right)^2 \frac{\vmax^2 \ln (|\Fcal| / \delta)}{n}. 
\end{align} where $\hat{f}:= \argmin_{f\in\Fcal} \sum_{i=1}^n \left( f(s_i,a_i) - r_i - \max_{a'} g(s'_i, a') \right)^2$.
\fi

Consider iteration $t$, set $g = f_{t+1}$ in Eq.~\ref{eq:uniform_bound_regression_fqi}, and by the definition of $f_t$, we have $\hat{f}_{g} = f_t$ in this case, which means that above inequality in Eq.~\ref{eq:uniform_bound_regression_fqi} holds for the pair $(\hat{f}_g, g) = (f_t, f_{t+1})$. Now apply a union bound over all $t = 0, \dots, K$, we conclude the proof. 
\end{proof}



Now we are ready to prove the main theorem. 

\begin{proof}[Proof of Theorem~\ref{thm:fqi_main}]

From Lemma~\ref{lem:generalization_least_square}, we see that with probability at least $1-\delta$, we have for all $t \leq K$: $$\| f_{t+1} - \Tcal f_t  \|_{2,\nu} \leq \sqrt{ \frac{22 \vmax^2 \ln(|\Fcal|^2 K / \delta)}{n}} + \sqrt{ 20 \epsilon_{approx,\nu}  }.$$ Set $\varepsilon$ in Lemma~\ref{lem:approximate_be_fqi} to be $ \sqrt{ \frac{22 \vmax^2 \ln(|\Fcal|^2 K / \delta)}{n}} + \sqrt{ 20 \epsilon_{approx,\nu}  }$, we conclude the proof. 
\end{proof}

%\subsubsection{Proof of Lemma~\ref{lem:least_square_gen} (Least Squares Generalization Bound)}
%Finally, for completeness, we prove the least square generalization bound presented in Lemma~\ref{lem:least_square_gen}. 




\section{Fitted Policy-Iteration (FPI)}

to be added..

\iffalse
In this section, we consider fitted policy iteration. We start with a function class $\Fcal = \{f: \Scal\times\Acal\mapsto [0,1/(1-\gamma)]\}$. We assume the following inherent policy value fitting error is small:
\begin{assumption}[Inherent Policy Value Fitting Error]
$\max_{\pi} \min_{f\in\Fcal} \left\|  f - Q^{\pi} \right\|^2_{2,\nu} \leq \epsilon_{approx, \nu}$.
\end{assumption}

\subsection{The FPI Algorithm}

FPI iteratively updates the policy as follows. Starting with $\pi^0$, we iterate:
\begin{itemize}
\item Sample $s,a\sim \nu$, rollout policy $\pi^n$ to get an unbiased estimate of $Q^{\pi^n}(s,a)$, denoted as $y$. Repeat $N$ times, and denote the N triples as dataset $\Dcal$ 
\item $f_{n} = \argmin_{f \in\Fcal} \sum_{s,a,y \in \Dcal} \left( f(s_i,a_i) - y_i  \right)^2$
\item set $\pi^{n+1}(s) = \argmax_{a} f_n(s,a), \forall s$.
\end{itemize}
\fi




\section{Failure Cases Without Assumption~\ref{assum:concentrability}}
%\sk{of just FPI?}

\section{FQI for Policy Evaluation}

%\sk{We should point out that the same ideas hold for policy
%  evaluation. Perhaps we should even give a policy eval statement
%  without proof? we can add that later.}


\iffalse
\newpage
\sk{old stuff}

\section{Approximate Policy Iteration (API)}

Given a policy $\pi$,  the policy iteration algorithm (in a known tabular MDP), will update this policy to be the greedy policy $\pi'$, where:
\[
\pi'(s)= \argmax_a Q^\pi(s,a)
\]
With only sampling access (and when the state space is large), we are
not be able to estimate $Q^{\pi}(s,a)$ at every state-action pair.
Instead, let us consider an approximate update rule, based on an
average case cost function:
\begin{align*}
\widehat\pi \in \argmax_{\pi\in \Pi} \mathbb{E}_{s\sim \mu} \left[ Q^{\pi}(s, \pi(s)) \right].
\end{align*}
Note that this is equivalent to:
\[
\widehat\pi \in \argmax_{\pi\in \Pi} \mathbb{E}_{s\sim \mu} \left[ A^{\pi}(s, \pi(s)) \right].
\]
by the definition of $A^\pi(s,a)$.

%\sk{you had this before. can we do the above instead of what you hade?
%  this is because the sampling is much easier. see the commented out text.}
%\begin{align*}
%\widehat\pi \in \argmax_{\pi\in \Pi} \mathbb{E}_{s\sim d_{\mu}^{\pi^0}} \left[ A^{\pi^0}(s, \pi(s)) \right].
%\end{align*}

%\iffalse
%Given a policy $\pi^0$, one intuitive approach we attempt to do is to act greedily with respect $\pi^0$ at every state (recall Policy Iteration in a known tabular MDP). However due to the unknown MDP and large state space, we will not be able to have $A^{\pi^0}(s,a)$ at every state-action pair. Instead, we can act greedily in the average sense:
%\begin{align*}
%\widehat\pi \in \argmax_{\pi\in \Pi} \mathbb{E}_{s\sim d_{\mu}^{\pi^0}} \left[ A^{\pi^0}(s, \pi(s)) \right].
%\end{align*}
%\fi

We will refer to the above as an \emph{greedy policy selector}
procedure. We aim to pick a policy that acts greedily with respect to
$\pi$ under the state distribution $\mu$.  Implementing the exact
greedy policy selector is not possible due to the fact that we do not
know the exact $A^{\pi}$. Instead, it is not difficult to see how can
achieve an $\varepsilon$-approximate greedy policy selector, using
samples in the episodic setting. Let us define an
$\varepsilon$-\emph{greedy policy selector} as follows:

\begin{definition}[$\varepsilon$-approximate Greedy Policy Selector ]
Given a policy $\pi$, we denote  $\mathcal{G}_{\varepsilon}(\pi, \Pi, \mu)$ as the oracle that returns a policy $\widehat{\pi}\in \Pi$, such that:
\begin{align*}
\EE_{s\sim \mu} A^{\pi}(s, \widehat\pi(s)) \geq \max_{\pi'\in\Pi} 
\EE_{s\sim \mu} A^{\pi}(s,\pi'(s)) - \varepsilon. 
\end{align*}\label{def:approximate_gps}
%\sk{I changed the above.}
\end{definition}
Below we study two approaches to implement the above selector: one is
via a reduction to classification with the policy class $\Pi$ and the
other one is via a reduction to regression using value function
approximation. 
%\sk{see my comments on regression}

\subsection{Approximating the Greedy Policy Selector using (Weighted) Classification}

Below we explain that we can implement such approximate Greedy Policy Selector via reduction to a standard supervised learning oracle---weighted multi-class classification. 

Given a policy $\pi$, since we will not be able to compute $Q^{\pi}(s,a)$ exactly, we will need to estimate it via samples. Below we first describe an unbiased estimator of $Q^{\pi}(s,a)$ constructed from a simple rollout trajectory from $s,a$. 

Given $(s,a)$ and $\pi$,  the following procedure returns an unbiased estimate of $Q^{\pi}(s,a)$:
\begin{enumerate}
\item we reset the system to $(s,a)$, i.e., $(s_0,a_0) = (s,a)$, 
\item At time step $h$, after observe reward $r_h$, we terminate with probability $1-\gamma$, 
\item If terminated, we return $\widehat{Q}^{\pi}(s,a) : = \sum_{\tau = 0}^h r_\tau$
\item else, we proceed to $h+1$ by sampling $s_{h+1} \sim P(\cdot | s_h,a_h)$, repeat step 2.
\end{enumerate}

It is not hard to verify that the returned number $\widehat{Q}^{\pi}(s,a)$ is an unbiased estimate of $Q^{\pi}(s,a)$.  We create a training dataset $\Dcal = \{s_i,a_i, \widehat{Q}^{\pi}(s_i,a_i)\}_{i=1}^N$ with $s_i\sim \mu, a_i\sim U(\Acal)$, and $\widehat{Q}_i^{\pi}(s_i,a_i)$ computed from the above procedure. 

\iffalse
We first define a weighted classification oracle as follows. 
\begin{definition}[Weighted Classification Oracle] Given a dataset $\mathcal{D} = \{s_i, c_i\}_{i=1}^N$ where $c_i \in \mathbb{R}^{A}$, and a policy class $\Pi$, the weight classification oracle returns the best classifier:
\begin{align*}
\text{CO}( \mathcal{D}, \Pi) = \argmax_{\pi\in \Pi} \sum_{i=1}^N c_i[\pi(s_i)], 
\end{align*} where $c[a]$ denotes the value in the entry in $c$ that corresponds to action $a$.
\end{definition} 
Weighted classification oracle is a standard oracle in supervised learning setting, and weighted classification oracle can be further reduced to a regular classification oracle or a regression oracle. We will assume the existence of $\text{CO}$. %The basic idea here is the \emph{Reduction to Supervised Learning}: we will reduce Reinforcement Learning into a sequence of supervised learning problems each of which can be solved via the \text{CO} oracle.  The idea of reduction is very powerful: any advancement we can make in supervised classification setting (e.g., through better neural network architectures, or better optimization methods), immediately leads to an advancement in RL. 

\sk{I don't think we necessarily need to formally define weighted multi-class
  classification, though I am ok with the above if you like it.
  Regardless, I think we are better off just giving the cost function
directly even if we define CO as in the above.}

%We will use the \text{CO} oracle to greedily select a policy. Consider an arbitrary policy $\pi$. We are interested in computing the following greedy policy:
%\begin{align*}
%\argmax_{\widetilde{\pi}\in \Pi} \mathbb{E}_{s\sim d_{\mu}^{\pi}} A^{\pi}(s,\widetilde{\pi}(s)). 
%\end{align*}
%We call the above procedure as \emph{Greedy Policy Selector}. 
%Of course implement such exact greedy policy selector is not possible (e.g., we cannot evaluate the exact expectation). 
Now we can implement an approximate greedy policy selector via the \text{CO} oracle  using data from $d^{\pi}_\mu$ up to statistical error. We draw a dataset $\Dcal = \{s_i, a_i, \widetilde{A}_i\}$, where $s_i \sim d_{\mu}^{\pi}$, $a_i\sim U(\Acal)$ (where we denote $U(\Acal)$ as the uniform distribution over action space $\Acal$), and $\widetilde{A}_i$ is an unbiased estimate of $A^{\pi}(s_i,a_i)$ computed from a single rollout. We can perform the policy selection procedure using the \text{CO} oracle as follows:
\begin{align}
\label{eq:approx_greedy_selector}
\widehat\pi := \argmax_{\pi\in \Pi}  \sum_{i=1}^N  \widetilde{c}_i [\pi(s_i)],  
\end{align} where $\widetilde{c}_i\in\mathbb{R}^A$ is a one-hot vector with zeros everywhere, except the entry that corresponds to $a_i$ contains $\frac{  \widetilde{A}_i } {1/ A} $. Essentially we are performing importance weighting here so that an $\widetilde{c}_i$ is indeed an unbiased estimate of the vector $[ A^{\pi}(s_i, a)  ]^{\top}_{a \in \Acal} \in \mathbb{R}^A$, given $s_i$. To see that, note that for any $a\in \Acal$, we have $\EE[ \widetilde{c}_i[a] | s_i] = \EE\left[ \sum_{a_i} \frac{1}{A} \mathbf{1}\{a = a_i\}  \frac{\widetilde{A}_i(s_i,a_i)}{1/A}   \right] = \EE\left[ \frac{1}{A} \frac{\widetilde{A}_i(s_i,a)}{1/A} \right] =  A^{\pi}(s_i,a)$. 

\sk{we really should spell out the sampling procedure in the episodic
  model (maybe bullet point it?). also, using $\mu$ rather than $d^{\pi}_\mu$ makes life easier here.}
\fi

%\sk{how about we write.} 
With $\Dcal$, we approximate the greedy policy selector via classification:
\begin{align}
\label{eq:approx_greedy_selector}
\widehat\pi := \argmax_{\pi'\in \Pi}  \sum_{i=1}^N \ind(a_i=\pi(s_i))\widehat{Q}^\pi_i (s_i, a_i),  
\end{align}
The following theorem shows that $\widehat\pi$ is an approximate greedy policy. 


\begin{theorem}[Approximate Greedy Policy Selector]\label{thm:gps_api}
%Given a dataset $\Dcal = \{s_i, a_i, \widetilde{A}_i\}$, where $s_i \sim d_{\mu}^{\pi}$, $a_i\sim U(\Acal)$, and $\widetilde{A}_i$ is an unbiased estimate of $A^{\pi}(s_i,a_i)$ computed from a single rollout, 
For $\widehat\pi$ computed based on Eq.~\ref{eq:approx_greedy_selector}, we have that with probability at least $1-\delta$:
\begin{align*}
\EE_{s\sim \mu} A^{\pi}(s, \widehat\pi(s)) \geq \max_{\widetilde{\pi}\in \Pi} \EE_{s\sim \mu} A^{\pi}(s, \widetilde\pi(s)) - \frac{4A}{1-\gamma} \sqrt{ \frac{\ln( | \Pi| / \delta)}{ N} }.
\end{align*}\label{thm:stats_error_gps}
\end{theorem} 

%\sk{the proof should be short if we ignore the $c_i$'s, right?}
\begin{proof}
Consider a fixed policy $\pi\in \Pi$. Denote random variables $X_i := A \mathbf{1}\{a_i = \pi(s_i)\} \widehat{Q}^{\pi}_i(s_i,a_i)$. Note that $\EE_{s_i\sim \mu, a_i\sim U(\Acal)} [ X_i ] =  \EE_{s\sim \mu}[  Q^{\pi}(s, \pi(s))]$, and $\{X_i\}$ are i.i.d, and $|X_i| \leq A / (1-\gamma)$. Thus, we can apply Hoeffding's inequality: with probability at least $1-\delta$, for $\pi$ we have:
\begin{align*}
\left\lvert \sum_{i=1}^N X_i / N -  \mathbb{E}_{s\sim \mu}Q^{\pi}(s, \pi(s)) \right\rvert \leq \frac{2A}{1-\gamma} \sqrt{\frac{\ln(1/\delta)}{N}}.
\end{align*}  With a union bound over all $\pi\in\Pi$, we have that with probability at least $1-\delta$:
\begin{align*}
\forall \pi\in \Pi: \; \left\lvert A \sum_{i=1}^N \mathbf{1}\{a_i=\pi(s_i)\}\widehat{Q}^{\pi}_i(s_i,a_i)  / N -  \mathbb{E}_{s\sim \mu}Q^{\pi}(s, \pi(s)) \right\rvert \leq \frac{2A}{1-\gamma} \sqrt{\frac{\ln(|\Pi|/\delta)}{N}} := \varepsilon_{stat}.
\end{align*}

%\begin{align*}
%\left\lvert \sum_{i=1}^N \widetilde{c}_i[ \pi'(s_i)] / N - \mathbb{E}_{s\sim d_{\mu}^{\pi}}  A^{\pi}(s, \pi'(s))  \right\rvert \leq  \frac{2A}{1-\gamma} \sqrt{ \frac{ \ln\left(| \Pi | / \delta \right) }{ N}} := \varepsilon_{stat}
%\end{align*} To see this, note that first of all, we have:
%\begin{align*}
%\mathbb{E}_{s_i}\left[ \EE\left[\widetilde{c}_i[\pi'(s_i)] \big\vert s_i\right] \right] = \mathbb{E}_{s\sim d_{\mu}^{\pi}} A^{\pi}(s, \pi'(s)),
%\end{align*} as $\EE[ \widetilde{c}_i | s_i ] = [ A^{\pi}(s_i,a)]_{a\in\Acal}^{\top}$, due to the importance weighting in $\widetilde{c}_i$. Second, note that we have:
%\begin{align*}
%\left\lvert \widetilde{c}_i[a]  \right\rvert \leq \frac{A}{1-\gamma}. 
%\end{align*}
With the uniform convergence result, we can conclude that:
\begin{align*}
\EE_{s\sim \mu} Q^{\pi}\left( s, \widehat\pi(s) \right) & \geq A \sum_{i=1}^N \mathbf{1}\{a_i=\pi(s_i)\}\widehat{Q}^{\pi}_i(s_i,a_i)  / N  - \varepsilon_{stat} \\
& \geq A \sum_{i=1}^N \mathbf{1}\{a_i=\pi'(s_i)\}\widehat{Q}^{\pi}_i(s_i,a_i)  / N  - \varepsilon_{stat} \geq  \EE_{s\sim \mu} Q^{\pi}(s, \pi'(s)) - 2\varepsilon_{stat}.
%\EE_{s\sim \mu} Q^{\pi}\left( s, \widehat\pi(s) \right) \geq \frac{1}{N}\sum_{i=1}^N \widetilde{c}_i[\widehat\pi(s_i)] - \varepsilon_{stat} \geq  \frac{1}{N}\sum_{i=1}^N \widetilde{c}_i[\pi'(s_i)] - \varepsilon_{stat} \geq \EE_{s\sim d_{\mu}^{\pi}} A^{\pi}(s, \pi'(s)) - 2\varepsilon_{stat},
\end{align*} for any $\pi'\in\Pi$ including $\argmax_{\widetilde{\pi}\in\Pi} \EE_{s\sim \mu} Q^{\pi}(s,\widetilde\pi(s))$. Finally, we add $\EE_{s\sim \mu} \left[ - V^{\pi}(s)\right]$ on both sizes of the above inequality, we conclude the proof. 
\end{proof}


Note that the above analysis shows that we can approximately optimize $\max_{\widetilde\pi\in \Pi} \EE_{s\sim \mu} A^{\pi}(s, \widetilde\pi(s))$ up to statistical error $ O( A \sqrt{ \frac{\ln(|\Pi| / \delta)}{N}}/(1-\gamma)) $.  In the rest of this chapter, we denote $\varepsilon: = 4 A \sqrt{ \frac{\ln(|\Pi| / \delta)}{N}}/(1-\gamma)$.
%We can set $\frac{A}{1-\gamma} \sqrt{\ln(|\Pi|/\delta) / N} = \varepsilon$ and solve for $N$ which is the total number of i.i.d samples we need to draw in order to get an $\varepsilon$-approximate policy selector with probability at least $1-\delta$.

%For notation simplicity, we are not going to carry the statistical error around. We simply assume that we have a $\varepsilon$-approximate greedy policy selector:
%\subsection{Implementing Approximate Greedy Policy Selector using Regression}
\iffalse
Here we present an implementation based on value function approximation.
Specifically, instead of starting directly from a restrict policy class $\Pi$ and a reduction to classification, we start from a restricted value function class $\Fcal = \{f: \Scal\times\Acal \mapsto [1,1/(1-\gamma)]\}$. In this case, one can think about the policies class $\Pi$ consisting of all greedy policies with respect to $f\in \Fcal$, i.e., $\Pi = \{\pi(s) = \argmax_{a} f(s,a): f\in \Fcal\}$.

We perform a reduction to regression. Consider the following least square regression problem. Given the dataset $\{s_i,a_i, \widetilde{A}_i\}$ with $s_i\sim d^{\pi}_\mu, a_i\sim U(\Acal)$ and $\widetilde{A}_i$ is an unbiased estimate of $A^{\pi}(s_i,a_i)$,  we perform the following regression:
\begin{align*}
\widehat{f} \in \argmax_{f\in \Fcal} \sum_{i=1}^N \left( f(s_i,a_i) - \widetilde{A}^i \right)^2 .
\end{align*} With $\widehat{f}$, the approximate greedy policy is set as:
\begin{align*}
\widehat\pi(s) = \argmax_{a\in \Acal} \widehat{f}(s,a), \forall s.
\end{align*}
Using a similar uniform convergence argument as in the proof of Theorem~\ref{thm:gps_api},  it is not hard to get a similar generalization bound as in Theorem~\ref{thm:gps_api}.

\sk{the section is a little weird. the $\eps$ we would get here does't
  really correspond to the guarantees in our lemmas, right? if not,
  let's cut this section.}
  \fi

\subsection{Performance Guarantees for API} 
With the above $\varepsilon$-approximate greedy policy selector, now we introduce
the Approximate Policy Iteration (API) algorithm, which is described
in the following iteration:  
\begin{align}
\label{eq:api}
\pi^{t+1} := \Gcal_{\varepsilon}( \pi^t, \Pi, \mu), 
\end{align} where we terminate when $\EE_{s\sim \mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s)) \right] \leq \epsilon$ where $\epsilon$ is the pre-defined termination threshold, i.e., we terminate when there is not that much advantage one can make against the current policy $\pi^t$. Note that $\EE_{s\sim \mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s)) \right]$ can be easily estimated by finite number of samples using states sampled  from $\mu$ and unbiased estimate of $A^{\pi^t}(s,a)$.

Note that API does not guarantee policy improvement nor convergence without additional assumption. We will give an example in the next section where we show that even with the exact approximate greedy policy selection, i.e., $\varepsilon = 0$, API cannot make any policy improvement and could oscillate  between two suboptimal policies forever. 

To have meaningful guarantees of policy improvement and convergence for API, we introduce the following concentrability assumption on the initial distribution $\mu$:
\begin{assumption}[Bounded Concentrability Coefficient]\label{ass:concentrability_API}
We assume that 
\[
C := \max_{\pi\in \Pi} \sup_{s \in \Scal} \frac{ d^{\pi}_\mu(s)  }{
  \mu(s)}  < \infty.
\] 
\end{assumption} Note that $C \geq 1$. The above assumption says that the initial state distribution $\mu$ is exploratory such that it covers all policies' state distribution. 

%\sk{we should give the algo with a termination condition. it stops in
%  $O(1/eps)$ steps? we also should give the final guarantee}

%\sk{I am not sure if the below is true. I copied it from the CPI
%  theorem with some modifications. Something like this should be true.}

\wen{add $\epsilon_{approx}$ back..}

We now give the final eee of API. We further assume policy completeness, i.e., for any $\pi\in \Pi$, we have $\argmax_{a} A^{\pi}(s,a) \in \Pi$.
\begin{theorem}[Optimality of API]Assume policy completeness, i.e.,  for any $\pi\in \Pi$, we have $\argmax_{a} A^{\pi}(s,a) \in \Pi$.
 Set $\epsilon,\delta \in (0, 1)$. 
In each iteration, set N (the size of the classification dataset) to be $N := \Theta\left(  \frac{  A^2 C^2 \ln(|\Pi| (1-\gamma )^3\epsilon / \delta )  }{ \epsilon^2(1-\gamma)^4}  \right)$.
With probability at least $1-\delta$, API will terminate in at most $T:= 2 / ((1-\gamma)^3 \epsilon)$ steps, returning a policy
$\widehat{\pi}$ such that:%Define:
%\[
%\epsilon_{\textrm{approx}} := \EE_{s\sim\mu} \left[\max_{a\in\Acal } A^{\pi}(s,
%a)\right]  - \max_{\pi\in \Pi }\EE_{s\sim \mu} \left[A^{\widehat \pi}(s, \pi(s))\right].
%\]
%(We can view $\epsilon_{\textrm{approx}}$ as the approximation error
%of the algorithm when it halts). 
%We have that:
\begin{align*}
  V^\star - V^{\widehat\pi} \leq \frac{ C \epsilon }{ 1-\gamma}.
\end{align*} .
\label{thm:api-opt}
\end{theorem}
\begin{proof}
Based on our setting of $N$, we can show that across $T$ many iterations, we must have $\varepsilon \leq (1-\gamma) \epsilon / (2C)$ with probability at least $1-\delta$. Below we condition on this event being true. 

 If we do not terminate at iteration $t$, we have $\EE_{s\sim\mu} A^{\pi^t}(s, \pi^{t+1}(s))\geq \epsilon$, which means that:
 \begin{align*}
 \max_{\pi\in\Pi} \EE_{s\sim \mu}A^{\pi^t}(s, \pi(s))  = \EE_{s\sim \mu} \max_{a} A^{\pi^t}(s,a) \geq \EE_{s\sim\mu} A^{\pi^t}(s, \pi^{t+1}(s)) \geq \epsilon.
 \end{align*}
We start with Performance Difference Lemma.
\begin{align*}
(1-\gamma)\left( V^{\pi^{t+1}} - V^{\pi^t} \right) & =  \EE_{s\sim d^{\pi^{t+1}}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right] \\
& =  \EE_{s\sim d^{\pi^{t+1}}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right] - \max_{\pi\in\Pi} \EE_{s\sim d^{\pi^{t+1}}_\mu} \left[ A^{\pi^t}(s,\pi(s))\right] + \max_{\pi\in\Pi}\EE_{s\sim d^{\pi^{t+1}}_\mu} \left[  A^{\pi^t}(s,\pi(s))\right] \\
& = \EE_{s\sim \mu} \frac{ d^{\pi^{t+1}}_\mu(s)}{\mu(s)} \left[A^{\pi^t}(s, \pi^{t+1}(s)) - \max_a A^{\pi^t}(s,a)  \right] + \EE_{s\sim d^{\pi^{t+1}}_\mu} \left[ \max_a A^{\pi^t}(s,a)\right]  \\
& \geq C \EE_{s\sim \mu}  \left[A^{\pi^t}(s, \pi^{t+1}(s)) - \max_a A^{\pi^t}(s,a)  \right] + (1-\gamma) \EE_{s\sim \mu} \left[ \max_a A^{\pi^t}(s,a)\right] \\
& \geq - C \varepsilon + (1-\gamma) \epsilon.
%&  =  \EE_{s\sim \mu} \frac{ d^{\pi^{t+1}}_\mu(s) }{ \mu }\left[ A^{\pi^t}(s, \pi^{t+1}(s))\right]  \geq \EE_{s\sim d^{\pi^t}_\mu} \frac{ (1-\gamma) \mu(s) }{ d^{\pi^t}_\mu(s) }\left[ A^{\pi^t}(s, \pi^{t+1}(s))\right] \\
%& \geq (1-\gamma) \EE_{s\sim d^{\pi^t}_\mu} \inf_{s} \frac{ \mu(s) }{ d^{\pi^t}_\mu(s) } \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right]  \geq \frac{1-\gamma}{C} \EE_{s\sim d^{\pi^t}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right],
\end{align*} where the first inequality inequality uses the definition of $C$ in Assumption~\ref{ass:concentrability_API}, and the last inequality uses the $\varepsilon$-greedy policy selector's guarantee. Note that based on our set of $N$, we have $\varepsilon \leq \frac{(1-\gamma) \epsilon}{2 C }$, which implies that:
\begin{align*}
(1-\gamma) \left( V^{\pi^{t+1}} - V^{\pi^t} \right) \geq (1-\gamma) \epsilon - \frac{(1-\gamma)\epsilon}{2 } = (1-\gamma) \epsilon / 2.
\end{align*} Since we know that $V^{\pi^\star} \leq 1/(1-\gamma)$, the total number of iterations of API before termination must be upper bounded by:
\begin{align*}
T \leq  \frac{2}{ (1-\gamma)^3 \epsilon  }.
\end{align*}

To show its final performance, recall that we terminate when $\EE_{s\sim \mu} \max_{a}A^{\pi^t}(s,a) \leq \epsilon$. 
Again, start from performance difference lemma:
\begin{align*}
(1-\gamma) \left( V^{\star} - V^{\pi^t} \right) &= \EE_{s\sim d_\mu^{\pi^\star}} A^{\pi^t}(s, \pi^\star(s)) \leq \EE_{s\sim d_\mu^{\pi^\star}} \max_{a} A^{\pi^t}(s,a) = \EE_{s\sim \mu} \frac{d_\mu^{\pi^\star}(s)}{\mu(s)} \max_{a} A^{\pi^t}(s,a) \\
& \leq C \EE_{s\sim \mu} \max_{a} A^{\pi^t}(s,a) \leq  C \left( \EE_{s\sim \mu} A^{\pi^t}(s, \pi^{t+1}) + \varepsilon  \right) \leq C \left( \epsilon + \varepsilon \right) \leq C( \epsilon + (1-\gamma) \epsilon / C ) \leq 2 C \epsilon.
\end{align*}This concludes the proof.

\iffalse
From the greedy policy selector, we know that:
\begin{align*}
\EE_{s\sim \mu} \max_{a\in\Acal} A^{\pi^t}(s,a)  - \epsilon_{approx} = \max_{\pi\in\Pi }\EE_{s\sim \mu} A^{\pi^t}(s, \pi(s)) \leq \EE_{s\sim \mu} A^{\pi}(s, \pi^{t+1}) + \varepsilon.
\end{align*} This implies that:
\begin{align*}
\EE_{s\sim \mu} \max_{a\in\Acal} A^{\pi^t}(s,a) \leq \EE_{s\sim \mu} A^{\pi}(s, \pi^{t+1}) + \varepsilon + \epsilon_{approx}.
\end{align*}


We start with Performance Difference Lemma.
\begin{align*}
(1-\gamma)\left( V^{\pi^{t+1}} - V^{\pi^t} \right) & =  \EE_{s\sim d^{\pi^{t+1}}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right] \\
&  =  \EE_{s\sim \mu} \frac{ d^{\pi^{t+1}}_\mu(s) }{ \mu }\left[ A^{\pi^t}(s, \pi^{t+1}(s))\right]  \geq \EE_{s\sim d^{\pi^t}_\mu} \frac{ (1-\gamma) \mu(s) }{ d^{\pi^t}_\mu(s) }\left[ A^{\pi^t}(s, \pi^{t+1}(s))\right] \\
& \geq (1-\gamma) \EE_{s\sim d^{\pi^t}_\mu} \inf_{s} \frac{ \mu(s) }{ d^{\pi^t}_\mu(s) } \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right]  \geq \frac{1-\gamma}{C} \EE_{s\sim d^{\pi^t}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right],
\end{align*} where the last inequality uses the definition of $C$ in Assumption~\ref{ass:concentrability_API}.
\fi
\end{proof}


\iffalse
\sk{The below should be part of the proof.}

With this assumption, we can show that API has monotonic improvement as long as there is local improvement, i.e., $\max_{\pi\in \Pi} \EE_{s\sim d^{\pi^t}_\mu} A^{\pi^t}(s, \pi(s))$ is reasonably big.
\begin{theorem}[Monotonic Policy Improvement]
For any $t$, we have:
\begin{align*}
V^{\pi^{t+1}} - V^{\pi^t} \geq \frac{1}{C} \left[  \max_{\pi\in\Pi} \EE_{s\sim d^{\pi^t}_\mu} \left[ A^{\pi^t}(s, \pi(s))\right]  - \varepsilon  \right].
\end{align*}
\end{theorem}
\begin{proof}
We start with Performance Difference Lemma.
\begin{align*}
(1-\gamma)\left( V^{\pi^{t+1}} - V^{\pi^t} \right) & =  \EE_{s\sim d^{\pi^{t+1}}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right] \\
&  =  \EE_{s\sim d^{\pi^t}_\mu} \frac{ d^{\pi^{t+1}}_\mu(s) }{ d^{\pi^t}_\mu(s) }\left[ A^{\pi^t}(s, \pi^{t+1}(s))\right]  \geq \EE_{s\sim d^{\pi^t}_\mu} \frac{ (1-\gamma) \mu(s) }{ d^{\pi^t}_\mu(s) }\left[ A^{\pi^t}(s, \pi^{t+1}(s))\right] \\
& \geq (1-\gamma) \EE_{s\sim d^{\pi^t}_\mu} \inf_{s} \frac{ \mu(s) }{ d^{\pi^t}_\mu(s) } \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right]  \geq \frac{1-\gamma}{C} \EE_{s\sim d^{\pi^t}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right],
\end{align*} where the last inequality uses the definition of $C$ in Assumption~\ref{ass:concentrability_API}.

This implies that:
\begin{align*}
V^{\pi^{t+1}} - V^{\pi^t} \geq \frac{1}{C} \EE_{s\sim d^{\pi^t}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right] \geq \frac{1}{C} \max_{\pi\in\Pi} \EE_{s\sim d^{\pi^t}_\mu} \left[ A^{\pi^t}(s, \pi(s))\right] - \frac{\varepsilon}{C}.
\end{align*}
\end{proof}


The above theorem implies that when $\max_{\pi\in \Pi} \EE_{s\sim d^{\pi^t}_\mu} A^{\pi^t}(s, \pi(s)) > \varepsilon$ and $C < \infty$, then we make monotonic improvement every iteration. 

\fi
\section{Failure Case of API Without Assumption~\ref{ass:concentrability_API}}
\begin{figure}[t]
\includegraphics[width=6cm]{Figures/api_example}
\centering
\caption{The example MDP. The MDP has deterministic transition and $\mu$ has probability mass on $s_1$. We have reward zero every where except $r(s_2, a_1) = r(s_3,a_1) = 1$.}
\label{fig:api_example}
\end{figure}

In this section, we show that API indeed will fail to provide policy improvement if $C = \infty$. To illustrate this phenomena, we simply consider the exact greedy policy selector, i.e., we assume that for $\Gcal_{\varepsilon}(\pi, \Pi, \mu)$, we have $\varepsilon = 0$. 

\begin{claim}
There exists a policy class $\Pi$, an MDP, a $\mu$ restart distribution where $C = \infty$, and two policies $\pi'$ and $\pi''$, such that if one start API with $\pi^0 \in \{\pi', \pi''\}$,  $\pi^t$ and $\pi^{t+1}$ will oscillate  between $\pi'$ and $\pi''$ which are both $\gamma$ away from the optimal policy. Namely API with $\pi^{t+1} = \Gcal_0(\pi^t, \Pi, \mu)$ will not be able to make any policy improvement nor will it converge.
\end{claim}
\begin{proof}
The MDP is shown in Fig.~\ref{fig:api_example} where the transition is deterministic and $\mu(s_1) = 1$. We consider $\Pi$ that contains all stationary policies.   We consider the two policies $\pi'$ and $\pi''$ as follows:
\begin{align*}
\pi'(s_1) = a_1, \pi'(s_2) = a_2, \pi'(s_3) = a_1; \quad  \pi''(s_1) = a_2, \pi''(s_2) = a_1, \pi''(s_3) = a_2.
\end{align*}
Hence for $\pi'$, $d^{\pi'}_{\mu}(s_3) = 0$ and $d^{\pi'}_\mu(s) >0$ for $s\neq s_3$. Similarly for $\pi''$, we have $d^{\pi''}_\mu(s_2) =0$ and $d^{\pi''}_\mu(s) > 0$ for $s\neq s_2$.

Consider the greedy policy selection under $\pi'$:
\begin{align*}
\pi \in \argmax_{\pi\in \Pi} \EE_{s\sim d^{\pi'}_\mu} A^{\pi'}(s, \pi(s)).
\end{align*}
We claim that $\pi''$ is one of the maximizers of the above procedure.  This is because $d_\mu^{\pi'}(s_3) =0$ and thus $\pi(s_3)$ does not affect the objective function at all.   For $s_1$, note that $Q^{\pi'}(s_1, a_1) = 0$ while $Q^{\pi'}(s_1, a_2) > 0$.  Thus a greedy policy will pick $a_2$ which is consistent to the choice of $\pi''$. For $s_2$, we have $Q^{\pi'}(s_2, a_1) > 0$ while $Q^{\pi'}(s_2, a_2) = 0$. Thus a greedy policy will pick $a_1$ at $s_2$ which again is consistent with the choice of $\pi''$ at $s_2$. This concludes that $\pi''$ is one of the greedy policies under $\pi'$. 

Similarly, one can argue that $\pi' \in \argmax_{\pi \in \Pi} \EE_{s \sim d^{\pi''}_h}A^{\pi''}(s,\pi(s))$, i.e., at $\pi''$, API will switch back to $\pi'$ in the next iteration. 

Thus, we have proven that when running API with either $\pi'$ or $\pi''$ as initialization, API can oscillate between $\pi'$ and $\pi''$ forever. Note that $\pi'$ and $\pi''$ have the same value and are $\gamma$ away from the optimal policy's value. 
\end{proof}

The above phenomena really comes from the fact that API is making abrupt policy update, i.e., there is no way we can guarantee $\pi^{t+1}$ is close to $\pi^t$, for instance, in terms of their resulting state distribution. Thus, for states $s$ that have really small probability under $d^{\pi^t}_\mu$, $\pi^{t+1}(\cdot | s)$ and $\pi^t(\cdot | s)$ could be different. Intuitively, if we look at the Performance Difference between $\pi^{t+1}$ and $\pi^t$, we see that:
\begin{align*}
 V^{\pi^{t+1}} - V^{\pi^t}  & =  \frac{1}{1-\gamma} \EE_{s\sim d^{\pi^{t+1}}_\mu} \left[ A^{\pi^t}(s, \pi^{t+1}(s))\right], 
\end{align*} which says that in order to make policy improvement, we need the new policy $\pi^{t+1}$ to be greedy with respect to $\pi^t$ under $d^{\pi^{t+1}}_\mu$---the state distribution of the new policy. However, the greedy policy selector only selects a policy that is greedy with respect to $\pi^t$ under $d^{\pi^t}_\mu$.  Hence, unless $d^{\pi^t}_\mu$ and $d^{\pi^{t+1}}_\mu$ are close, we will not be able to directly transfer the potential local one-step improvement $\EE_{s\sim d^{\pi^t}} A^{\pi^t}(s, \pi^{t+1}(s))$ to policy improvement $V^{\pi^{t+1}} - V^{\pi^t}$. 

\section{Can we relax the concentrability notion?}

There is another family of policy optimization algorithm which use incremental policy updates, i.e., when we perform policy update, we ensure that $d^{\pi^{t+1}}_\mu$ is not all that different from $d^{\pi^t}_\mu$. Incremental policy update will the core of Part~\ref{part:opt}.  
For instance, the algorithm Conservative Policy Iteration, which we study in Chapter~\ref{chap:cpi}, uses a conservative policy update  $\pi^{t+1}(\cdot | s) := (1-\alpha) \pi^t(\cdot | s) + \alpha \Gcal_{\varepsilon}(\pi^t, \Pi, \mu)$ for all $s$, which, for small $\alpha$, ensures that $d^{\pi^{t+1}}_\mu$ and $d^{\pi^t}_\mu$ are not too far apart.  Properly setting the step size $\alpha$, we can show that CPI makes monotonic policy improvement and converges to a local optimal policy with a more relaxed condition over Assumption~\ref{ass:concentrability_API}. We will explain the benefit of incremental policy updating in more detail in Part III. %and Chapter~\ref{chp:cpi}.

\fi

\section{Bibliographic Remarks and Further Readings}\label{bib:api}


The notion of concentrability was developed in
\cite{munos2003error,munos2005error} in order to permitting sharper
bounds in terms of average case function approximation error, provided
that the concentrability coefficient is bounded.  These methods also
permit sample based fitting methods, with sample size and error
bounds, provided there is a data collection policy that induces a
bounded concentrability coefficient~\cite{munos2005error,
  szepesvari2005finite,antos2008learning,lazaric2016analysis}. \citet{chen2019information} provide a
more detailed discussion on this quantity. 

% SK and this was the start of the previous lower bound chapter. the "linear part" of the lower bounds is now in the previous chapter.

\chapter{Information Theoretic Lower Bounds}
%\chapter{Generalization in RL \\ \& Reductions to Supervised Learning}
\label{chap:generalization_stat_limits}

In reinforcement learning, we seek to have learnability results which
are applicable to cases where number of states is large (or, possibly,
countably or uncountably infinite). This is a question of
generalization, which, more broadly, is one of the central
challenges in machine learning. 

%More generally, these are questions of
%generalization, which is one of the central
%challenges in machine learning.

The previous two chapters largely focussed on \emph{sufficient}
conditions under which we can obtain sample complexity results which
do not explicitly depend on the size of the state (or action) space.  This chapter 
focusses on what are \emph{necessary} conditions for
generalization. Here, we can frame our questions by examining 
the extent to which generalization in RL is similar to (or different
from) that in supervised learning. Two most basic settings in
supervised learning are: (i) agnostic learning (i.e. finding the best
classifier or hypothesis in some class) and (ii) learning with linear
models (i.e. learning the best linear regressor or the best linear
classifier).  This chapter will focus on \emph{lower bounds} with
regards to the analogues of these two questions for reinforcement learning:
\begin{itemize}
\item (Agnostic learning) Given some hypothesis class (of policies,
  value functions, or models), what is the sample complexity of
  finding (nearly) the best hypothesis in this class?
\item (Linearly realizable values or policies) Suppose we are given
  some $d$-dimensional feature mapping where we are guaranteed that
  either the optimal value function is linear in these given features
  or that the optimal policy has a linear parameterization.  Are we
  able to obtain sample complexity guarantees that are polynomial in
  $d$, with little to no explicit dependence on the size of the state
  or action spaces? We will consider this question in both the offline
  setting (for the purposes of policy evaluation, as in
  Chapter~\ref{chap:Bellman_complete}) and for in online setting where
  our goal is to learn a near optimal optimal policy.
\end{itemize}
Observe that supervised learning can be viewed as horizon one, $H=1$, RL
problem (where the learner only receives feedback for the ``label'',
i.e. the action, chosen). We can view the second question above as the
analogue of linear regression or classification with halfspaces. In
supervised learning, both of these settings have postive answers, and
they are fundamental in our understanding of generalization.  Perhaps
surprisingly, we will see negative answers to these questions in the
RL setting.  The
significance of this provides insights as to why our study of
generalization in reinforcement learning is substantially more subtle than in
supervised learning. Importantly, the insights we develop here will also help us to
motivate the more refined assumptions and settings that we consider in
subsequent chapters (see Section~\ref{sec:how_to_study_RL} for discussion).


This chapter will work with finite horizon MDPs, where we consider
both the episodic setting and the generative model setting. 
With regards to the first question on agnostic learning, this chapter
follows the ideas first introduced
in~\cite{NIPS1999_1664}.  With regards to the second
question on linear realizability, this chapter follows the results
in~\cite{du2019good,wang2020statistical,WeiszAS21,Wang_linear_lower}. 


\section{Agnostic Learning}

Suppose we have a hypothesis class $\Hcal$ (either finite or
infinite), where for each $f \in \Hcal$ we have an associated policy
$\pi_f:\Scal \rightarrow \Acal$, which, for simplicity, we assume is deterministic.
Here, we could have that:
\begin{itemize}
\item $\Hcal$ itself is a class of policies.
\item $\Hcal$ is a set of state-action values, where for $f \in \Hcal$,
  we associate it with the greedy policy $\pi_f(s,h) = \argmax_a
  f_h(s,a)$, where $s\in\Scal$ and $h\in[H]$.
\item $\Hcal$ could be a class of models (i.e. each $f \in \Hcal$ is an
  MDP itself). Here, for each $f \in \Hcal$, we can let $\pi_f$ be the
  optimal policy in the MDP $f$. 
\end{itemize}
We let $\Pi$ denote the induced set of policies from our hypothesis
class $\Hcal$, i.e. $\Pi= \{\pi_f | f \in \Hcal\}$.


The goal of agnostic learning can be formulated by the following optimization problem:
\begin{equation}\label{eq:ag_learn_objective}
\max_{\pi\in\Pi} \E_{s_0\sim \mu} V^{\pi}(s_0),
\end{equation}
where we are interested in the number of samples required to 
approximately solve this optimization problem. We will work both in
the episodic setting and the generative model setting, and, for
simplicity, we will restrict ourselves to finite hypothesis classes.

\iffalse
As before, we only hope to perform favorably against the best policy
in $\Pi$. Recall that in our aforementioned sampling model we have
the ability to obtain trajectories from $s_0\sim \mu$ under policies
of our choosing. As we have seen, agnostic learning is possible in
the supervised learning setting, with regret bounds that have no dependence
on the size of the domain --- the size of domain is analogous to the size
the state space $|\Scal|$. 
\fi

\paragraph{Binary classification as an $H=1$ RL problem:} 
Observe that the problem of binary classification can be viewed as
learning in an MDP with a horizon of one. In particular, take $H=1$;
take $|\Acal|=2$; let the distribution over starting states $s_0\sim\mu$
correspond to the input distribution; and take the reward function as
$r(s,a) = \ind(\textrm{label}(s) =a)$. In other words, we equate our
action with the prediction of the binary class membership, and the reward
function is determined by if our prediction is correct or not.

\subsection{Review: Binary Classification}

One of the most important concepts for learning binary classifiers is
that it is possible to \emph{generalize} even when the state space is
infinite. Here note that the domain of our classifiers, often denoted
by $\mathcal{X}$, is analogous to the state space $\Scal$. We now
briefly review some basics of supervised learning before we turn to
the question of generalization in reinforcement learning.

Consider the problem of binary classification with $N$ labeled
examples of the form $(x_i, y_i)_{i=1}^N$, with $x_i \in
\Xcal$ and $y_i \in \{0,1\}$. Suppose we have a (finite or infinte) set $\Hcal$ of
binary classifiers where each $h \in \Hcal$ is a mapping of the form
$h :\Xcal \to \{0,1\}$. Let $\ind(h(x) \ne y)$ be an indicator which
takes the value $0$ if $h(x) = y$ and $1$ otherwise. We assume that our samples
are drawn i.i.d. according to a fixed joint distribution $D$ over $(x,y)$.

Define the empirical error and the true error as:
\[
\widehat{\err}(h) = 
\frac{1}{N} \sum_{i=1}^N \ind(h(x_i) \ne y_i) , \quad
\err(h) = \E_{(X,Y)\sim D} \ind(h(X) \ne Y).
\]
For a given $h \in \Hcal$, Hoeffding's inequality implies that with
probability at least $1-\delta$: 
\[\left|\textrm{err}(h) - \widehat{\textrm{err}}(h)
\right| \leq \sqrt{\frac{1}{2N}\log\frac{2}{\delta}}.\]
This and the union bound give rise to what is often referred to as the
``Occam's razor'' bound:

\begin{proposition}\label{proposition:occam_sl}
(The ``Occam's razor'' bound)
Suppose $\Hcal$ is finite.
Let $\widehat{h} = \arg\min_{h \in \Hcal}\widehat{\textrm{err}}(h)$.
%and $h^\star = \arg\min_{h \in  \Hcal} \err(h)$. 
With probability at least $1-\delta$:
\[
\err(\widehat{h})  \leq \min_{h \in  \Hcal}\err(h) +
\sqrt{\frac{2}{N}\log\frac{2|\Hcal|}{\delta}}.
\]
\end{proposition}

Hence, provided that
\[
N \geq \frac{2 \log\frac{2|\Hcal|}{\delta}}{\eps^2},
\]
then with probability at least
$1-\delta$, we have that:
\[
\err(\widehat{h})  \leq \min_{h \in  \Hcal}\err(h)+\eps.
\]
A key observation here is that the
our regret --- the regret is the left hand side of the above
inequality --- has \emph{no dependence} on the size of
$\mathcal{X}$ (i.e. $\Scal$) which may be infinite and is only
logarithmic in the number of hypothesis in our class.



\subsection{Importance Sampling and a Reduction to Supervised Learning}

Now let us return to the agnostic learning question in
Equation~\ref{eq:ag_learn_objective}. We will see that, provided we
are willing to be exponential in the horizon $H$, then agnostic
learning is possible. Furthermore, it is a straightforward argument to
see that we are not able to do better.


\subsubsection*{An Occam's Razor Bound for RL}

We now provide a reduction of RL to the supervised learning problem,
given only sampling access in the episodic setting. The key issue is
how to efficiently reuse data. The idea is that we will simply collect $N$
trajectories by executing a policy which chooses actions uniformly at
random; let $\textrm{Unif}_{\Acal}$ denote this policy. 

The following shows how we can obtain an unbiased estimate of
the value of any policy $\pi$ using this uniform policy $\textrm{Unif}_{\Acal}$:
\begin{lemma}
(Unbiased estimation of $V^\pi_0(\mu)$) Let $\pi$ be any
deterministic policy. We have that:
\[
V^\pi_0(\mu) = 
|\Acal|^H \cdot \E_{\tau\sim \Pr_{\textrm{Unif}_{\Acal}}}
\left[\ind\Big(\pi(s_0)=a_0,\ \ldots ,\pi(s_{H-1})=a_{H-1}\Big)
\sum_{h=0}^{H-1} r(s_h,a_h)\right]
\]
where $\Pr_{\textrm{Unif}_{\Acal}}$ specifies the distribution over 
trajectories $\tau=(s_0,a_0,r_0, \ldots s_{H-1}, a_{H-1},r_{H-1})$
under the policy $\textrm{Unif}_{\Acal}$.
\end{lemma}

The proof follows from a standard importance sampling
argument (applied to the distribution over the trajectories). 
\begin{proof}
We have that:
\begin{align*}
V^\pi_0(\mu) &= 
\E_{\tau\sim \Pr_{\pi}}
\left[\sum_{h=0}^{H-1} r_h\right]\\
&=
\E_{\tau\sim \Pr_{\textrm{Unif}_{\Acal}}}
\left[
\frac{\Pr_{\pi}(\tau)}{\Pr_{\textrm{Unif}_{\Acal}}(\tau)}
%\ind\Big(\pi(s_0)=a_0,\ \ldots ,\pi(s_{H-1})=a_{H-1}\Big)
\sum_{h=0}^{H-1} r_h\right]\\
&=|\Acal|^H \cdot \E_{\tau\sim \Pr_{\textrm{Unif}_{\Acal}}}
\left[\ind\Big(\pi(s_0)=a_0,\ \ldots ,\pi(s_{H-1})=a_{H-1}\Big)
\sum_{h=0}^{H-1} r_h\right]
\end{align*}
where the last step follows due to that the probability ratio is only
nonzero when $\textrm{Unif}_{\Acal}$ choose actions identical to that
of $\pi$.
\end{proof}

Crudely, the factor of $|\Acal|^H$ is due to that the estimated reward of $\pi$ on a
trajectory is nonzero only when $\pi$ takes exactly identical
actions to those taken by $\textrm{Unif}_{\Acal}$ on the trajectory,
which occurs with probability $1/|\Acal|^H$.

% in which case the
%estimated value of $\pi$ is $|\Acal|^H$ times that of $\textrm{Unif}_{\Acal}$.


Now, given sampling access in the episodic model, we can use
$\textrm{Unif}_{\Acal}$ to get any estimate of any other policy
$\pi\in\Pi$.
Note that the factor of $|\Acal|^H$ in the previous lemma will lead to this approach being a high
variance estimator. Suppose we draw $N$ trajectories under $\textrm{Unif}_{\Acal}$.
Denote the $n$-th sampled trajectory by $(s_0^n, a_0^n, r_1^n, s_1^n,
\ldots,s_{H-1}^n, a_{H-1}^n, r_{H-1}^n)$. We can then use following to estimate the
finite horizon reward of any given policy $\pi$:
\[
\widehat V^\pi_0(\mu) = \frac{|\Acal|^H}{N}\sum_{n=1}^N 
\ind\Big(\pi(s_0^n)=a_0^n, \ldots \pi(s_{H-1}^n)=a_{H-1}^n\Big)
\sum_{t=0}^{H-1} r(s_t^n,a_t^n).
\]

\begin{proposition}\label{proposition:occam_RL}
(An ``Occam's razor bound'' for RL)
Let $\delta\geq 0$. Suppose $\Pi$ is a finite and suppose we use the aforementioned estimator, $\widehat
V^{\pi}_0(\mu)$, to estimate the value of every $\pi\in\Pi$.  Let
$\widehat{\pi} = \arg\max_{\pi\in\Pi}\widehat V^{\pi}_0(\mu)$. 
%and  $\pi^\star = \arg\max_{\pi \in  \Pi} V^\pi(s_0)$.  
We have that with probability at least
$1-\delta$,
\[
V_0^{\widehat{\pi}}(\mu) \geq \max_{\pi \in  \Pi} V_0^{\pi}(\mu) -
H |\Acal|^H\sqrt{\frac{2}{N}\log\frac{2|\Pi|}{\delta}}.
\]
\end{proposition}

\begin{proof}
Note that, fixing $\pi$, our estimator is sum of i.i.d. random variables,
where each independent estimator,  $ |\Acal|^H
\ind\Big(\pi(s_0^n)=a_0^n, \ldots \pi(s_{H-1}^n)=a_{H-1}^n\Big)
\sum_{t=0}^{H-1} r(s_t^n,a_t^n)$, is bounded by $H |\Acal|^H$. The
remainder of the argument is identical to that used in Proposition~\ref{proposition:occam_sl}.
\end{proof}

Hence, provided that 
\[
N \geq  H |\Acal|^H \ \frac{2\log(2|\Pi|/\delta)}{\eps^2},
\]
then with probability at least
$1-\delta$, we have that:
\[
V_0^{\widehat \pi}(s_0) \geq \max_{\pi \in  \Pi} V_0^\pi(s_0)  -\eps.
\]
This is the analogue of the Occam's razor bound for RL.

Importantly, the above shows that we can avoid dependence on the size
of the state space, though this comes at the price of an exponential dependence
on the horizon. As we see in the next section, this dependence is
unavoidable (without making further assumptions).

\paragraph{Infinite Policy Classes.}
In the supervised learning setting, a crucial observation is that even
though a hypothesis set $\Hcal$ of binary classifiers may be
infinite, we may still be able to obtain non-trivial generalization
bounds. A crucial observation here is that even
though the set $\Hcal$ may be infinite, the number of
possible behaviors of on a finite set of states is not necessarily
exhaustive. 
The
\emph{VapnikChervonenkis} (VC) dimension of $\Hcal$, $VC(\Hcal)$, is formal way to
characterize this intuition, and, using this concept, we are able to
obtain generalization bounds in terms of the $VC(\Hcal)$.

With regards to infinite hypothesis classes of policies (say for the
case where $|\Acal|=2$), extending our Occam's razor bound can be done
with precisely the same approach. In particular, when $|\Acal|=2$,
each $\pi\in\Pi$ can be viewed as Boolean function, and this gives
rise to the VC dimension $\textrm{VC}(\Pi)$ of our policy class. The
bound in Proposition~\ref{proposition:occam_RL} can be replaced with a
VC-dimension bound that is analogous to that of binary
classification (see Section~\ref{ch3:bib}). 


\subsubsection*{Lower Bounds}

Clearly, the drawback of this agnostic learning approach is that we
would require a number of samples that is exponential in
the problem horizon. We now see that if we desire a
sample complexity that scales with $O(\log|\Pi|)$, then an
exponential dependence on the effective horizon is unavoidable,
without making further assumptions.

Here, our lower bound permits a (possibly randomized) algorithm to utilize a generative
model (which is a more powerful sampling model than the episodic one).
% and
%the algorithm may return a policy $\pi$ not necessarily corresponding to
%any policy in $\Pi$.

\begin{proposition}
(Lower Bound with a Generative Model)
Suppose algorithm $\Acal$ has access to a generative model.
There exists a policy class $\Pi$, where $|\Pi| =|\Acal|^H$  such that if algorithm $\Acal$
returns any policy $\pi$ (not necessarily in $\Pi$) such that
\[
V_0^{\pi}(\mu) \geq \max_{\pi\in\Pi} V_0^{\pi}(\mu) - 0.5.
\]
with probability greater than $1/2$, then $\Acal$ must make a number
of number calls $N$ to the generative model where:
\[
N \geq c |\Acal|^H
\]
(where $c$ is an absolute constant).
\end{proposition}


\begin{proof}
The proof is simple. Consider a $|\Acal|$-ary balanced tree, with
$|\Acal|^H$ states and $|\Acal|$ actions, where states correspond
nodes and actions correspond to edges; actions always move the agent
from the root towards a leaf node. We make only one leaf node
rewarding, which is unknown to the algorithm. We consider the policy class  to be
all $|\Acal|^H$ policies. The theorem now immediately follows since
the algorithm gains no knowledge of the rewarding leaf node unless it
queries that node.
\end{proof}

Note this immediately rules out the possibility that any algorithm which can obtain a $\log
|\Pi|$ dependence without paying a factor of $|\Acal|^H$ in the
sample size due to that $\log |\Pi| = H \log |\Acal|$ in the example above.
